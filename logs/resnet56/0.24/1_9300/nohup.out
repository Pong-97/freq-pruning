Files already downloaded and verified
=> loading teacher model checkpoint '/home2/pengyifan/pyf/hypergraph_cluster/log/pretrained_model/cifar_resnet56/model_best.pth.tar'
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 36, 36]
=> loading checkpoint '/home2/pengyifan/pyf/freq/pretrained_model/cifar_resnet56/model_best.pth.tar'
=> loaded checkpoint '/home2/pengyifan/pyf/freq/pretrained_model/cifar_resnet56/model_best.pth.tar'  
layer index: 2 	 total channel: 16 	 remaining channel: 16
layer index:  1
num_parameters:  853018
[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3283, Accuracy: 9041/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.2899, Accuracy: 9117/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3264, Accuracy: 9078/10000 (90.8%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3065, Accuracy: 9086/10000 (90.9%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3763, Accuracy: 8928/10000 (89.3%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3640, Accuracy: 8941/10000 (89.4%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3236, Accuracy: 9065/10000 (90.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.2984, Accuracy: 9096/10000 (91.0%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3172, Accuracy: 9080/10000 (90.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3290, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3369, Accuracy: 9011/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3610, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3404, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3258, Accuracy: 9081/10000 (90.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3128, Accuracy: 9105/10000 (91.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.2923, Accuracy: 9161/10000 (91.6%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3642, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3133, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3308, Accuracy: 9038/10000 (90.4%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3161, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3294, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3277, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3564, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3940, Accuracy: 8918/10000 (89.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3167, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3101, Accuracy: 9109/10000 (91.1%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3519, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3975, Accuracy: 8895/10000 (88.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3571, Accuracy: 9023/10000 (90.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3727, Accuracy: 8936/10000 (89.4%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2580, Accuracy: 9265/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2565, Accuracy: 9290/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2547, Accuracy: 9294/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2601, Accuracy: 9306/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2534, Accuracy: 9320/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2516, Accuracy: 9329/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2552, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2564, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2577, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2597, Accuracy: 9332/10000 (93.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2584, Accuracy: 9352/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2616, Accuracy: 9325/10000 (93.2%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2584, Accuracy: 9345/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2573, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2640, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2598, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2606, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2608, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2585, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2648, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2603, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2623, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2613, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2623, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2606, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2621, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2612, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2609, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2628, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2598, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.9372 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 57) Prec1: 0.937200
layer index: 7 	 total channel: 16 	 remaining channel: 9
layer index:  2
num_parameters:  850988
[16, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3463, Accuracy: 8976/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8881/10000 (88.8%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3601, Accuracy: 8985/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.2990, Accuracy: 9101/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3449, Accuracy: 9037/10000 (90.4%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3891, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3291, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3656, Accuracy: 8949/10000 (89.5%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3471, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3563, Accuracy: 9014/10000 (90.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3428, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3577, Accuracy: 9011/10000 (90.1%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4801, Accuracy: 8742/10000 (87.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3537, Accuracy: 9012/10000 (90.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3216, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3385, Accuracy: 9066/10000 (90.7%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4139, Accuracy: 8903/10000 (89.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3786, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3345, Accuracy: 9048/10000 (90.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3117, Accuracy: 9101/10000 (91.0%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3321, Accuracy: 9074/10000 (90.7%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3760, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4224, Accuracy: 8892/10000 (88.9%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3306, Accuracy: 9049/10000 (90.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3597, Accuracy: 9037/10000 (90.4%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3199, Accuracy: 9106/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3854, Accuracy: 8934/10000 (89.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3438, Accuracy: 9035/10000 (90.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3287, Accuracy: 9113/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3346, Accuracy: 9054/10000 (90.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2489, Accuracy: 9305/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2490, Accuracy: 9325/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2485, Accuracy: 9334/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2483, Accuracy: 9366/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2486, Accuracy: 9355/10000 (93.6%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2473, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2513, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2536, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2560, Accuracy: 9357/10000 (93.6%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2557, Accuracy: 9368/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2563, Accuracy: 9375/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2557, Accuracy: 9368/10000 (93.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2547, Accuracy: 9381/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9366/10000 (93.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2604, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2596, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2612, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2610, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2580, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2601, Accuracy: 9386/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2574, Accuracy: 9383/10000 (93.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2606, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2598, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2612, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2595, Accuracy: 9382/10000 (93.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2587, Accuracy: 9391/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2592, Accuracy: 9393/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2579, Accuracy: 9397/10000 (94.0%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2591, Accuracy: 9390/10000 (93.9%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2568, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001
Best accuracy: 0.9397 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 59) Prec1: 0.939700
layer index: 10 	 total channel: 16 	 remaining channel: 9
layer index:  3
num_parameters:  849399
[16, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4329, Accuracy: 8779/10000 (87.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3946, Accuracy: 8899/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3552, Accuracy: 9000/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3508, Accuracy: 8957/10000 (89.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4437, Accuracy: 8786/10000 (87.9%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3723, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3611, Accuracy: 8967/10000 (89.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3209, Accuracy: 9091/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3301, Accuracy: 9059/10000 (90.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3902, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3617, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3814, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3583, Accuracy: 8978/10000 (89.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3552, Accuracy: 9025/10000 (90.2%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4017, Accuracy: 8929/10000 (89.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3302, Accuracy: 9099/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3576, Accuracy: 9026/10000 (90.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3875, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3581, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3521, Accuracy: 9003/10000 (90.0%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3766, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3303, Accuracy: 9057/10000 (90.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3697, Accuracy: 8990/10000 (89.9%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3200, Accuracy: 9074/10000 (90.7%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4119, Accuracy: 8896/10000 (89.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3072, Accuracy: 9157/10000 (91.6%), learning rate: 0.01
is best and save!

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3891, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3593, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3552, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3471, Accuracy: 9099/10000 (91.0%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2564, Accuracy: 9309/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2511, Accuracy: 9321/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2504, Accuracy: 9329/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2552, Accuracy: 9349/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2553, Accuracy: 9352/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2519, Accuracy: 9361/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2525, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2513, Accuracy: 9357/10000 (93.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2553, Accuracy: 9372/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2559, Accuracy: 9375/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2533, Accuracy: 9374/10000 (93.7%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2533, Accuracy: 9377/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2534, Accuracy: 9378/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2583, Accuracy: 9384/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2598, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2572, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2569, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2564, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2531, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2587, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2565, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2574, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2561, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2581, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2559, Accuracy: 9382/10000 (93.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2567, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2567, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2562, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2580, Accuracy: 9382/10000 (93.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2547, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001
Best accuracy: 0.9384 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 45) Prec1: 0.938400
layer index: 15 	 total channel: 16 	 remaining channel: 9
layer index:  4
num_parameters:  847810
[16, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3766, Accuracy: 8959/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3477, Accuracy: 8987/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3702, Accuracy: 8954/10000 (89.5%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3568, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3675, Accuracy: 8948/10000 (89.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8951/10000 (89.5%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4583, Accuracy: 8767/10000 (87.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3531, Accuracy: 9017/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4051, Accuracy: 8892/10000 (88.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3747, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3465, Accuracy: 9058/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4097, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3865, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3432, Accuracy: 9065/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3134, Accuracy: 9110/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3477, Accuracy: 9072/10000 (90.7%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3701, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3651, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3974, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3367, Accuracy: 9064/10000 (90.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3677, Accuracy: 9021/10000 (90.2%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3225, Accuracy: 9104/10000 (91.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3589, Accuracy: 9041/10000 (90.4%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3232, Accuracy: 9117/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3444, Accuracy: 9097/10000 (91.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3729, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3389, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3608, Accuracy: 9052/10000 (90.5%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3789, Accuracy: 9009/10000 (90.1%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4198, Accuracy: 8906/10000 (89.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2516, Accuracy: 9306/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2483, Accuracy: 9320/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2487, Accuracy: 9326/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2490, Accuracy: 9341/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2540, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2530, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2560, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2568, Accuracy: 9352/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2578, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2556, Accuracy: 9367/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2565, Accuracy: 9351/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2590, Accuracy: 9360/10000 (93.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2541, Accuracy: 9357/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2612, Accuracy: 9371/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2602, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2575, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2597, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2587, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2561, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2600, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2574, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2593, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2585, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2588, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2592, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2590, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2583, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2574, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2590, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2568, Accuracy: 9381/10000 (93.8%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9381 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.938100
layer index: 18 	 total channel: 16 	 remaining channel: 9
layer index:  5
num_parameters:  846221
[16, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3464, Accuracy: 9027/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3980, Accuracy: 8895/10000 (88.9%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3728, Accuracy: 8912/10000 (89.1%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3955, Accuracy: 8891/10000 (88.9%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3606, Accuracy: 9025/10000 (90.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3936, Accuracy: 8933/10000 (89.3%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3729, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3738, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3945, Accuracy: 8966/10000 (89.7%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3980, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3629, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3501, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3919, Accuracy: 8949/10000 (89.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3650, Accuracy: 8988/10000 (89.9%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3778, Accuracy: 8965/10000 (89.7%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3245, Accuracy: 9110/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3627, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3711, Accuracy: 8984/10000 (89.8%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3464, Accuracy: 9056/10000 (90.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3269, Accuracy: 9124/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3773, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3717, Accuracy: 9009/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.5040, Accuracy: 8738/10000 (87.4%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3892, Accuracy: 8984/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3382, Accuracy: 9069/10000 (90.7%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3505, Accuracy: 8993/10000 (89.9%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3497, Accuracy: 9071/10000 (90.7%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3829, Accuracy: 9025/10000 (90.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3953, Accuracy: 9028/10000 (90.3%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2536, Accuracy: 9306/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2549, Accuracy: 9331/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2567, Accuracy: 9327/10000 (93.3%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2550, Accuracy: 9330/10000 (93.3%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2549, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2538, Accuracy: 9348/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2570, Accuracy: 9330/10000 (93.3%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2593, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2590, Accuracy: 9350/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9366/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2588, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2591, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2562, Accuracy: 9364/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2623, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2637, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2603, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2630, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2617, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2600, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2644, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2612, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2642, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2616, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2641, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2633, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2618, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2614, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2609, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2621, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2603, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.9366 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 41) Prec1: 0.936600
layer index: 23 	 total channel: 16 	 remaining channel: 9
layer index:  6
num_parameters:  844632
[16, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3553, Accuracy: 9003/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3699, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4799, Accuracy: 8743/10000 (87.4%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3099, Accuracy: 9098/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3901, Accuracy: 8928/10000 (89.3%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3915, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3639, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3463, Accuracy: 9035/10000 (90.3%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4148, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3981, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3735, Accuracy: 9031/10000 (90.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3444, Accuracy: 9079/10000 (90.8%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3848, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3299, Accuracy: 9102/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3132, Accuracy: 9140/10000 (91.4%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3703, Accuracy: 8981/10000 (89.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3493, Accuracy: 9063/10000 (90.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3704, Accuracy: 8957/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3232, Accuracy: 9103/10000 (91.0%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4125, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3583, Accuracy: 9045/10000 (90.4%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3470, Accuracy: 9024/10000 (90.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3268, Accuracy: 9125/10000 (91.2%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3484, Accuracy: 9059/10000 (90.6%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4020, Accuracy: 8965/10000 (89.7%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3409, Accuracy: 9081/10000 (90.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3439, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3676, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3531, Accuracy: 9057/10000 (90.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4404, Accuracy: 8858/10000 (88.6%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2708, Accuracy: 9259/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2662, Accuracy: 9281/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2639, Accuracy: 9291/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2636, Accuracy: 9309/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2652, Accuracy: 9322/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2605, Accuracy: 9324/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2641, Accuracy: 9344/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2638, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2628, Accuracy: 9333/10000 (93.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2665, Accuracy: 9338/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2657, Accuracy: 9345/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2623, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2680, Accuracy: 9352/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2681, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2664, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2671, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2677, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2644, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2675, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2662, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2690, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2662, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2676, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2668, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2668, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2668, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2654, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2674, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2648, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.9359 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.935900
layer index: 26 	 total channel: 16 	 remaining channel: 9
layer index:  7
num_parameters:  843043
[16, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3526, Accuracy: 9016/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3913, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3071, Accuracy: 9111/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8951/10000 (89.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3752, Accuracy: 8971/10000 (89.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4006, Accuracy: 8951/10000 (89.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3536, Accuracy: 9015/10000 (90.2%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3977, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 8964/10000 (89.6%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3527, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.5248, Accuracy: 8689/10000 (86.9%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3529, Accuracy: 9055/10000 (90.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3816, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3588, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3791, Accuracy: 8990/10000 (89.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3561, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3963, Accuracy: 8948/10000 (89.5%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3624, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3070, Accuracy: 9131/10000 (91.3%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4173, Accuracy: 8927/10000 (89.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3507, Accuracy: 9079/10000 (90.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3409, Accuracy: 9056/10000 (90.6%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3842, Accuracy: 9028/10000 (90.3%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3600, Accuracy: 9057/10000 (90.6%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3866, Accuracy: 9003/10000 (90.0%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3397, Accuracy: 9074/10000 (90.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3735, Accuracy: 8990/10000 (89.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3814, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3580, Accuracy: 9047/10000 (90.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2491, Accuracy: 9306/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2501, Accuracy: 9312/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2540, Accuracy: 9312/10000 (93.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2495, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2507, Accuracy: 9345/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2494, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2518, Accuracy: 9356/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2534, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2546, Accuracy: 9359/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2548, Accuracy: 9361/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2561, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2590, Accuracy: 9340/10000 (93.4%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2525, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9364/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2568, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2535, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2550, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2558, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2525, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2557, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2539, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2564, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2548, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2552, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2548, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2542, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2541, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2534, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2564, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2532, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.937 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 60) Prec1: 0.937000
layer index: 31 	 total channel: 16 	 remaining channel: 9
layer index:  8
num_parameters:  841454
[16, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3709, Accuracy: 8978/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3577, Accuracy: 9001/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3707, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3800, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4213, Accuracy: 8851/10000 (88.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3606, Accuracy: 8992/10000 (89.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3285, Accuracy: 9082/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3819, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4190, Accuracy: 8884/10000 (88.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3354, Accuracy: 9049/10000 (90.5%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3282, Accuracy: 9098/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3530, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3305, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3075, Accuracy: 9130/10000 (91.3%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3455, Accuracy: 9059/10000 (90.6%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3340, Accuracy: 9088/10000 (90.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3707, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3353, Accuracy: 9085/10000 (90.8%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4028, Accuracy: 8921/10000 (89.2%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3360, Accuracy: 9068/10000 (90.7%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3968, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3581, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3574, Accuracy: 9034/10000 (90.3%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3934, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3868, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3551, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3510, Accuracy: 9068/10000 (90.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3519, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3425, Accuracy: 9098/10000 (91.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4030, Accuracy: 8954/10000 (89.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2614, Accuracy: 9300/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2572, Accuracy: 9320/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2589, Accuracy: 9330/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2579, Accuracy: 9341/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2557, Accuracy: 9337/10000 (93.4%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2560, Accuracy: 9347/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2591, Accuracy: 9356/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2607, Accuracy: 9365/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2600, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2588, Accuracy: 9361/10000 (93.6%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2619, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2634, Accuracy: 9366/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2605, Accuracy: 9370/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2663, Accuracy: 9369/10000 (93.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2624, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2612, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2640, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2634, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2609, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2625, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2623, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2656, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2619, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2627, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2630, Accuracy: 9382/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2635, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2636, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2631, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2656, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2618, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001
Best accuracy: 0.9384 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.938400
layer index: 34 	 total channel: 16 	 remaining channel: 9
layer index:  9
num_parameters:  839865
[16, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3979, Accuracy: 8899/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3877, Accuracy: 8930/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3621, Accuracy: 8969/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3695, Accuracy: 8988/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4089, Accuracy: 8868/10000 (88.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3920, Accuracy: 8953/10000 (89.5%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3568, Accuracy: 9007/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4595, Accuracy: 8767/10000 (87.7%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3527, Accuracy: 9034/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3558, Accuracy: 9041/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3785, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3701, Accuracy: 8954/10000 (89.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3158, Accuracy: 9140/10000 (91.4%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3164, Accuracy: 9099/10000 (91.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3115, Accuracy: 9106/10000 (91.1%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3559, Accuracy: 9026/10000 (90.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3999, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3565, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3282, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4551, Accuracy: 8887/10000 (88.9%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4234, Accuracy: 8905/10000 (89.1%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4338, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3834, Accuracy: 9012/10000 (90.1%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3468, Accuracy: 9041/10000 (90.4%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3529, Accuracy: 9085/10000 (90.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3689, Accuracy: 9031/10000 (90.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3654, Accuracy: 9028/10000 (90.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3653, Accuracy: 9045/10000 (90.4%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4792, Accuracy: 8799/10000 (88.0%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2555, Accuracy: 9317/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2570, Accuracy: 9324/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2544, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2547, Accuracy: 9359/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2545, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2579, Accuracy: 9342/10000 (93.4%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2577, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2596, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2613, Accuracy: 9363/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2614, Accuracy: 9350/10000 (93.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2653, Accuracy: 9366/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2644, Accuracy: 9363/10000 (93.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2625, Accuracy: 9367/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2650, Accuracy: 9372/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2657, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2651, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2664, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2660, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2637, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2664, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2657, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2679, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2660, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2663, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2664, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2661, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2650, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2644, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2684, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2642, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001
Best accuracy: 0.9377 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 52) Prec1: 0.937700
layer index: 39 	 total channel: 16 	 remaining channel: 9
layer index:  10
num_parameters:  838276
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3639, Accuracy: 8964/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3684, Accuracy: 9027/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4702, Accuracy: 8783/10000 (87.8%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3552, Accuracy: 9018/10000 (90.2%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4133, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3876, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3526, Accuracy: 9006/10000 (90.1%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3934, Accuracy: 8933/10000 (89.3%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3669, Accuracy: 9031/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3935, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3634, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3716, Accuracy: 8988/10000 (89.9%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3494, Accuracy: 9078/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3341, Accuracy: 9123/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4009, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3551, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3365, Accuracy: 9103/10000 (91.0%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4382, Accuracy: 8815/10000 (88.2%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3073, Accuracy: 9160/10000 (91.6%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4869, Accuracy: 8763/10000 (87.6%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3619, Accuracy: 9034/10000 (90.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3484, Accuracy: 9065/10000 (90.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3771, Accuracy: 9026/10000 (90.3%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3577, Accuracy: 9031/10000 (90.3%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3222, Accuracy: 9087/10000 (90.9%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3757, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4040, Accuracy: 8935/10000 (89.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3633, Accuracy: 9003/10000 (90.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3589, Accuracy: 9034/10000 (90.3%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2516, Accuracy: 9318/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2481, Accuracy: 9350/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2458, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2471, Accuracy: 9363/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2446, Accuracy: 9372/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2445, Accuracy: 9376/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2435, Accuracy: 9372/10000 (93.7%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2486, Accuracy: 9372/10000 (93.7%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2488, Accuracy: 9377/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2481, Accuracy: 9373/10000 (93.7%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2516, Accuracy: 9366/10000 (93.7%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2517, Accuracy: 9366/10000 (93.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2514, Accuracy: 9379/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2505, Accuracy: 9385/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2522, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2508, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2526, Accuracy: 9388/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2513, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2490, Accuracy: 9393/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2505, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2511, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2519, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2511, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2506, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2510, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2503, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2507, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2512, Accuracy: 9380/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2531, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2503, Accuracy: 9390/10000 (93.9%), learning rate: 0.0001
Best accuracy: 0.9393 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 50) Prec1: 0.939300
layer index: 42 	 total channel: 16 	 remaining channel: 9
layer index:  11
num_parameters:  836687
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4333, Accuracy: 8756/10000 (87.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3374, Accuracy: 9021/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4023, Accuracy: 8929/10000 (89.3%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3828, Accuracy: 8932/10000 (89.3%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3811, Accuracy: 8978/10000 (89.8%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3311, Accuracy: 9087/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3308, Accuracy: 9060/10000 (90.6%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3136, Accuracy: 9095/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3839, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4173, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3436, Accuracy: 9062/10000 (90.6%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3261, Accuracy: 9116/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3784, Accuracy: 9009/10000 (90.1%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3601, Accuracy: 9016/10000 (90.2%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3579, Accuracy: 9027/10000 (90.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3368, Accuracy: 9065/10000 (90.7%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4011, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3357, Accuracy: 9071/10000 (90.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3298, Accuracy: 9130/10000 (91.3%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3420, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3360, Accuracy: 9100/10000 (91.0%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3384, Accuracy: 9082/10000 (90.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3635, Accuracy: 9037/10000 (90.4%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3789, Accuracy: 9002/10000 (90.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3556, Accuracy: 9044/10000 (90.4%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4538, Accuracy: 8866/10000 (88.7%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4072, Accuracy: 8889/10000 (88.9%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3880, Accuracy: 8965/10000 (89.7%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3881, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4471, Accuracy: 8863/10000 (88.6%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2570, Accuracy: 9321/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2561, Accuracy: 9340/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2583, Accuracy: 9311/10000 (93.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9327/10000 (93.3%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2577, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2599, Accuracy: 9334/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2631, Accuracy: 9319/10000 (93.2%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2609, Accuracy: 9340/10000 (93.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2605, Accuracy: 9337/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2610, Accuracy: 9343/10000 (93.4%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2670, Accuracy: 9331/10000 (93.3%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2663, Accuracy: 9349/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2652, Accuracy: 9363/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2671, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2656, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2643, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2678, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2664, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2632, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2649, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2653, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2687, Accuracy: 9353/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2671, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2670, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2662, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2662, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2658, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2646, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2677, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2643, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001
Best accuracy: 0.9365 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 54) Prec1: 0.936500
layer index: 47 	 total channel: 16 	 remaining channel: 9
layer index:  12
num_parameters:  835098
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3246, Accuracy: 9068/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3470, Accuracy: 9006/10000 (90.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4070, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3572, Accuracy: 9012/10000 (90.1%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3958, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4753, Accuracy: 8762/10000 (87.6%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3372, Accuracy: 9078/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3835, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3448, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3565, Accuracy: 9040/10000 (90.4%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3412, Accuracy: 9033/10000 (90.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4061, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4588, Accuracy: 8839/10000 (88.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3556, Accuracy: 9044/10000 (90.4%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3444, Accuracy: 9065/10000 (90.7%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3796, Accuracy: 8948/10000 (89.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3551, Accuracy: 9070/10000 (90.7%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3384, Accuracy: 9102/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3323, Accuracy: 9105/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3595, Accuracy: 9006/10000 (90.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3730, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3628, Accuracy: 9012/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4021, Accuracy: 8930/10000 (89.3%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3694, Accuracy: 9024/10000 (90.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3869, Accuracy: 8984/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3358, Accuracy: 9064/10000 (90.6%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3360, Accuracy: 9096/10000 (91.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3780, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3780, Accuracy: 9002/10000 (90.0%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2500, Accuracy: 9322/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2469, Accuracy: 9356/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2470, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2485, Accuracy: 9351/10000 (93.5%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2463, Accuracy: 9375/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2491, Accuracy: 9361/10000 (93.6%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2502, Accuracy: 9366/10000 (93.7%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2509, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2523, Accuracy: 9382/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2524, Accuracy: 9382/10000 (93.8%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2543, Accuracy: 9375/10000 (93.8%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2538, Accuracy: 9370/10000 (93.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2517, Accuracy: 9380/10000 (93.8%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2544, Accuracy: 9375/10000 (93.8%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2512, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2504, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2531, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2517, Accuracy: 9373/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2492, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2524, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2507, Accuracy: 9383/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2533, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2516, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2517, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2506, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2519, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2517, Accuracy: 9389/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2509, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2531, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2512, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001
Best accuracy: 0.9389 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.938900
layer index: 50 	 total channel: 16 	 remaining channel: 9
layer index:  13
num_parameters:  833509
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4343, Accuracy: 8760/10000 (87.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4305, Accuracy: 8849/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4301, Accuracy: 8854/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3694, Accuracy: 8989/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3906, Accuracy: 8921/10000 (89.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4217, Accuracy: 8921/10000 (89.2%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3794, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3704, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3672, Accuracy: 8992/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3672, Accuracy: 9010/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3738, Accuracy: 9014/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4022, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3698, Accuracy: 9030/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3488, Accuracy: 9045/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3846, Accuracy: 8974/10000 (89.7%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4469, Accuracy: 8857/10000 (88.6%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3739, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3758, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3359, Accuracy: 9079/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3529, Accuracy: 9066/10000 (90.7%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3231, Accuracy: 9116/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3298, Accuracy: 9091/10000 (90.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3730, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3625, Accuracy: 9006/10000 (90.1%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3975, Accuracy: 8984/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3769, Accuracy: 9017/10000 (90.2%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3604, Accuracy: 9032/10000 (90.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3860, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3815, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4056, Accuracy: 8925/10000 (89.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2466, Accuracy: 9316/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2449, Accuracy: 9334/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2492, Accuracy: 9353/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2475, Accuracy: 9369/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2486, Accuracy: 9371/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2470, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2459, Accuracy: 9363/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2493, Accuracy: 9374/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2522, Accuracy: 9375/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2519, Accuracy: 9373/10000 (93.7%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2540, Accuracy: 9353/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2529, Accuracy: 9385/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2506, Accuracy: 9378/10000 (93.8%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2545, Accuracy: 9386/10000 (93.9%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2510, Accuracy: 9393/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2499, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2523, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2512, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2483, Accuracy: 9399/10000 (94.0%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2521, Accuracy: 9386/10000 (93.9%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2520, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2534, Accuracy: 9386/10000 (93.9%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2511, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2522, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2508, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2522, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2524, Accuracy: 9390/10000 (93.9%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2531, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2543, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2508, Accuracy: 9392/10000 (93.9%), learning rate: 0.0001
Best accuracy: 0.9399 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 50) Prec1: 0.939900
layer index: 55 	 total channel: 16 	 remaining channel: 9
layer index:  14
num_parameters:  831920
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3578, Accuracy: 8990/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4276, Accuracy: 8906/10000 (89.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3790, Accuracy: 8931/10000 (89.3%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3704, Accuracy: 8988/10000 (89.9%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3335, Accuracy: 9057/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3580, Accuracy: 8974/10000 (89.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3599, Accuracy: 9062/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3593, Accuracy: 8971/10000 (89.7%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3469, Accuracy: 9061/10000 (90.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3693, Accuracy: 9016/10000 (90.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3878, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3452, Accuracy: 9067/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3599, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3397, Accuracy: 9081/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3431, Accuracy: 9086/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3090, Accuracy: 9125/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3768, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3510, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3686, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3343, Accuracy: 9078/10000 (90.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4479, Accuracy: 8875/10000 (88.8%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3531, Accuracy: 8953/10000 (89.5%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3349, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3576, Accuracy: 9025/10000 (90.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3537, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3835, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3751, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3729, Accuracy: 9041/10000 (90.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3537, Accuracy: 9063/10000 (90.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3836, Accuracy: 9010/10000 (90.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2444, Accuracy: 9332/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2440, Accuracy: 9354/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2442, Accuracy: 9347/10000 (93.5%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2474, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2466, Accuracy: 9369/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2461, Accuracy: 9374/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2472, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2479, Accuracy: 9381/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2490, Accuracy: 9385/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2477, Accuracy: 9395/10000 (93.9%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2499, Accuracy: 9386/10000 (93.9%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2494, Accuracy: 9383/10000 (93.8%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2494, Accuracy: 9401/10000 (94.0%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2506, Accuracy: 9388/10000 (93.9%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2491, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2459, Accuracy: 9408/10000 (94.1%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2488, Accuracy: 9405/10000 (94.1%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2485, Accuracy: 9407/10000 (94.1%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2449, Accuracy: 9400/10000 (94.0%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2476, Accuracy: 9394/10000 (93.9%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2491, Accuracy: 9381/10000 (93.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2499, Accuracy: 9395/10000 (93.9%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2480, Accuracy: 9397/10000 (94.0%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2491, Accuracy: 9391/10000 (93.9%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2485, Accuracy: 9399/10000 (94.0%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2479, Accuracy: 9409/10000 (94.1%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2475, Accuracy: 9396/10000 (94.0%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2478, Accuracy: 9399/10000 (94.0%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2502, Accuracy: 9405/10000 (94.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2471, Accuracy: 9408/10000 (94.1%), learning rate: 0.0001
Best accuracy: 0.9409 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 57) Prec1: 0.940900
layer index: 58 	 total channel: 16 	 remaining channel: 9
layer index:  15
num_parameters:  830331
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3979, Accuracy: 8901/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4457, Accuracy: 8793/10000 (87.9%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3660, Accuracy: 9005/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3283, Accuracy: 9088/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3657, Accuracy: 8977/10000 (89.8%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3657, Accuracy: 8966/10000 (89.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3501, Accuracy: 9064/10000 (90.6%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3368, Accuracy: 9080/10000 (90.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3351, Accuracy: 9080/10000 (90.8%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3708, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3249, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4747, Accuracy: 8776/10000 (87.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3353, Accuracy: 9106/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4372, Accuracy: 8887/10000 (88.9%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3482, Accuracy: 9049/10000 (90.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3813, Accuracy: 9009/10000 (90.1%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3658, Accuracy: 9015/10000 (90.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3593, Accuracy: 9023/10000 (90.2%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3285, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3218, Accuracy: 9089/10000 (90.9%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3337, Accuracy: 9033/10000 (90.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3742, Accuracy: 9052/10000 (90.5%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3445, Accuracy: 9097/10000 (91.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3212, Accuracy: 9136/10000 (91.4%), learning rate: 0.01
is best and save!

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4381, Accuracy: 8843/10000 (88.4%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4261, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3494, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4423, Accuracy: 8810/10000 (88.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2425, Accuracy: 9298/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2409, Accuracy: 9314/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2459, Accuracy: 9311/10000 (93.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2414, Accuracy: 9341/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2418, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2414, Accuracy: 9355/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2441, Accuracy: 9359/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2466, Accuracy: 9350/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2487, Accuracy: 9345/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2459, Accuracy: 9365/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2500, Accuracy: 9353/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2485, Accuracy: 9354/10000 (93.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2480, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2508, Accuracy: 9366/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2533, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2493, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2516, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2497, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2474, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2494, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2495, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2508, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2486, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2498, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2492, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2493, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2483, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2479, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2507, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2483, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001
Best accuracy: 0.9375 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 56) Prec1: 0.937500
layer index: 63 	 total channel: 16 	 remaining channel: 9
layer index:  16
num_parameters:  828742
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3783, Accuracy: 8968/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3442, Accuracy: 8999/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3708, Accuracy: 9022/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3798, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8945/10000 (89.4%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3387, Accuracy: 9054/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3552, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3279, Accuracy: 9068/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3787, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3520, Accuracy: 9036/10000 (90.4%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3782, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3905, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4148, Accuracy: 8908/10000 (89.1%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3532, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3288, Accuracy: 9071/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3551, Accuracy: 9047/10000 (90.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4483, Accuracy: 8886/10000 (88.9%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 8920/10000 (89.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3460, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3599, Accuracy: 9019/10000 (90.2%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3468, Accuracy: 9080/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3708, Accuracy: 9028/10000 (90.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3968, Accuracy: 8974/10000 (89.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3385, Accuracy: 9084/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3554, Accuracy: 9063/10000 (90.6%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3734, Accuracy: 9024/10000 (90.2%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3584, Accuracy: 9036/10000 (90.4%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3829, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3658, Accuracy: 8988/10000 (89.9%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3656, Accuracy: 8998/10000 (90.0%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2496, Accuracy: 9313/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2474, Accuracy: 9319/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2439, Accuracy: 9354/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2455, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2450, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2451, Accuracy: 9362/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2474, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2483, Accuracy: 9357/10000 (93.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2486, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2474, Accuracy: 9368/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2501, Accuracy: 9373/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2517, Accuracy: 9373/10000 (93.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2481, Accuracy: 9397/10000 (94.0%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2521, Accuracy: 9370/10000 (93.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2497, Accuracy: 9380/10000 (93.8%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2494, Accuracy: 9383/10000 (93.8%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2522, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2505, Accuracy: 9384/10000 (93.8%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2468, Accuracy: 9397/10000 (94.0%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2494, Accuracy: 9385/10000 (93.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2489, Accuracy: 9391/10000 (93.9%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2514, Accuracy: 9382/10000 (93.8%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2499, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2503, Accuracy: 9395/10000 (93.9%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2484, Accuracy: 9396/10000 (94.0%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2471, Accuracy: 9402/10000 (94.0%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2478, Accuracy: 9400/10000 (94.0%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2468, Accuracy: 9396/10000 (94.0%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2506, Accuracy: 9387/10000 (93.9%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2467, Accuracy: 9392/10000 (93.9%), learning rate: 0.0001
Best accuracy: 0.9402 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 57) Prec1: 0.940200
layer index: 66 	 total channel: 16 	 remaining channel: 9
layer index:  17
num_parameters:  827153
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3640, Accuracy: 8938/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3832, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3385, Accuracy: 9061/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3217, Accuracy: 9039/10000 (90.4%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4455, Accuracy: 8788/10000 (87.9%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3651, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8918/10000 (89.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4341, Accuracy: 8918/10000 (89.2%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3806, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3809, Accuracy: 8952/10000 (89.5%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4242, Accuracy: 8931/10000 (89.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3316, Accuracy: 9109/10000 (91.1%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4738, Accuracy: 8780/10000 (87.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4346, Accuracy: 8876/10000 (88.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3737, Accuracy: 8972/10000 (89.7%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3559, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3665, Accuracy: 9038/10000 (90.4%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3623, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3619, Accuracy: 9011/10000 (90.1%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3667, Accuracy: 8991/10000 (89.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3072, Accuracy: 9167/10000 (91.7%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3553, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3613, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3474, Accuracy: 9052/10000 (90.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3946, Accuracy: 8993/10000 (89.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3675, Accuracy: 9061/10000 (90.6%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3721, Accuracy: 8995/10000 (89.9%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4952, Accuracy: 8745/10000 (87.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3247, Accuracy: 9104/10000 (91.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4175, Accuracy: 8926/10000 (89.3%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2633, Accuracy: 9291/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2623, Accuracy: 9294/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2630, Accuracy: 9309/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2617, Accuracy: 9325/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2587, Accuracy: 9334/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2598, Accuracy: 9325/10000 (93.2%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2607, Accuracy: 9319/10000 (93.2%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2608, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2629, Accuracy: 9338/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2607, Accuracy: 9338/10000 (93.4%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2628, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2664, Accuracy: 9328/10000 (93.3%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2637, Accuracy: 9343/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2664, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2614, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2608, Accuracy: 9348/10000 (93.5%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2622, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2605, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2576, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2610, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2594, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2624, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2608, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2614, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2606, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2602, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2613, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2594, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2625, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2598, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001
Best accuracy: 0.9361 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.936100
layer index: 71 	 total channel: 16 	 remaining channel: 9
layer index:  18
num_parameters:  825564
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3769, Accuracy: 8910/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3814, Accuracy: 8947/10000 (89.5%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3953, Accuracy: 8933/10000 (89.3%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3836, Accuracy: 8923/10000 (89.2%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3760, Accuracy: 9010/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3349, Accuracy: 9088/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4079, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3869, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3550, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3833, Accuracy: 8972/10000 (89.7%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3700, Accuracy: 8992/10000 (89.9%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4105, Accuracy: 8944/10000 (89.4%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3844, Accuracy: 8974/10000 (89.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3359, Accuracy: 9075/10000 (90.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4038, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3467, Accuracy: 9038/10000 (90.4%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4139, Accuracy: 8915/10000 (89.2%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3645, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3484, Accuracy: 9093/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3436, Accuracy: 9032/10000 (90.3%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4194, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3777, Accuracy: 8977/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3681, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4318, Accuracy: 8861/10000 (88.6%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3576, Accuracy: 9044/10000 (90.4%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3560, Accuracy: 8989/10000 (89.9%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3889, Accuracy: 9010/10000 (90.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3907, Accuracy: 9011/10000 (90.1%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3708, Accuracy: 9024/10000 (90.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2561, Accuracy: 9299/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2532, Accuracy: 9321/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2545, Accuracy: 9322/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2594, Accuracy: 9325/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2570, Accuracy: 9338/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2554, Accuracy: 9334/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2546, Accuracy: 9348/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2537, Accuracy: 9359/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2608, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2605, Accuracy: 9351/10000 (93.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2662, Accuracy: 9346/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2605, Accuracy: 9371/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2638, Accuracy: 9365/10000 (93.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2608, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2596, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2610, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2601, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2578, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2608, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2595, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2610, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2596, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2608, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2613, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2598, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2607, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2600, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2618, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2594, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.9371 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 43) Prec1: 0.937100
layer index: 74 	 total channel: 16 	 remaining channel: 9
layer index:  19
num_parameters:  822967
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4624, Accuracy: 8683/10000 (86.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3588, Accuracy: 8967/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4720, Accuracy: 8716/10000 (87.2%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3774, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3766, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4077, Accuracy: 8933/10000 (89.3%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3977, Accuracy: 8904/10000 (89.0%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3557, Accuracy: 9022/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3565, Accuracy: 9029/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3671, Accuracy: 8990/10000 (89.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3567, Accuracy: 9026/10000 (90.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3704, Accuracy: 8981/10000 (89.8%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3715, Accuracy: 8991/10000 (89.9%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4021, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4063, Accuracy: 8892/10000 (88.9%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3657, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4629, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3481, Accuracy: 9036/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3507, Accuracy: 9053/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3726, Accuracy: 9015/10000 (90.2%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3841, Accuracy: 9015/10000 (90.2%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3460, Accuracy: 9070/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3835, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3896, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3638, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3518, Accuracy: 9051/10000 (90.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.5003, Accuracy: 8748/10000 (87.5%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3650, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3594, Accuracy: 9021/10000 (90.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2718, Accuracy: 9265/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2681, Accuracy: 9300/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2638, Accuracy: 9330/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2687, Accuracy: 9323/10000 (93.2%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2654, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2671, Accuracy: 9335/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2698, Accuracy: 9348/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2732, Accuracy: 9345/10000 (93.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2717, Accuracy: 9339/10000 (93.4%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2766, Accuracy: 9343/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2727, Accuracy: 9337/10000 (93.4%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2718, Accuracy: 9338/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2760, Accuracy: 9353/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2775, Accuracy: 9325/10000 (93.2%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2764, Accuracy: 9335/10000 (93.3%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2757, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2751, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2738, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2755, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2743, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2763, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2763, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2761, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2752, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2746, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2762, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2745, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2766, Accuracy: 9342/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2753, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001
Best accuracy: 0.9353 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 45) Prec1: 0.935300
layer index: 80 	 total channel: 32 	 remaining channel: 15
layer index:  20
num_parameters:  816660
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3343, Accuracy: 9016/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3738, Accuracy: 8964/10000 (89.6%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3972, Accuracy: 8919/10000 (89.2%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3384, Accuracy: 9033/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4419, Accuracy: 8826/10000 (88.3%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4228, Accuracy: 8914/10000 (89.1%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3801, Accuracy: 8967/10000 (89.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4840, Accuracy: 8726/10000 (87.3%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3416, Accuracy: 9037/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3298, Accuracy: 9078/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3387, Accuracy: 9080/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3786, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4576, Accuracy: 8838/10000 (88.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3688, Accuracy: 8990/10000 (89.9%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3678, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3488, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3915, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4327, Accuracy: 8911/10000 (89.1%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8952/10000 (89.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3442, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3850, Accuracy: 8973/10000 (89.7%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3766, Accuracy: 9017/10000 (90.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3878, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4016, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3753, Accuracy: 8987/10000 (89.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3556, Accuracy: 9091/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4264, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4760, Accuracy: 8800/10000 (88.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3549, Accuracy: 9058/10000 (90.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4427, Accuracy: 8884/10000 (88.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2884, Accuracy: 9244/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2837, Accuracy: 9266/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2783, Accuracy: 9268/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2784, Accuracy: 9284/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2729, Accuracy: 9303/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2749, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2779, Accuracy: 9282/10000 (92.8%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2791, Accuracy: 9294/10000 (92.9%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2814, Accuracy: 9305/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2797, Accuracy: 9314/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2809, Accuracy: 9322/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2794, Accuracy: 9318/10000 (93.2%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2825, Accuracy: 9309/10000 (93.1%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2889, Accuracy: 9299/10000 (93.0%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2886, Accuracy: 9294/10000 (92.9%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2872, Accuracy: 9307/10000 (93.1%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2871, Accuracy: 9309/10000 (93.1%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2850, Accuracy: 9302/10000 (93.0%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2822, Accuracy: 9308/10000 (93.1%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2850, Accuracy: 9302/10000 (93.0%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2844, Accuracy: 9304/10000 (93.0%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2860, Accuracy: 9303/10000 (93.0%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2838, Accuracy: 9311/10000 (93.1%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2835, Accuracy: 9306/10000 (93.1%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9305/10000 (93.1%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2837, Accuracy: 9305/10000 (93.1%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2845, Accuracy: 9312/10000 (93.1%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2829, Accuracy: 9307/10000 (93.1%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2857, Accuracy: 9305/10000 (93.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2824, Accuracy: 9314/10000 (93.1%), learning rate: 0.0001
Best accuracy: 0.9322 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 42) Prec1: 0.932200
layer index: 83 	 total channel: 32 	 remaining channel: 15
layer index:  21
num_parameters:  809435
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3864, Accuracy: 8882/10000 (88.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3425, Accuracy: 9022/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4357, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3891, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3759, Accuracy: 8962/10000 (89.6%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4204, Accuracy: 8886/10000 (88.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4173, Accuracy: 8919/10000 (89.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3598, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3701, Accuracy: 8995/10000 (89.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4148, Accuracy: 8895/10000 (88.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3844, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4021, Accuracy: 8944/10000 (89.4%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4520, Accuracy: 8845/10000 (88.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3809, Accuracy: 9006/10000 (90.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3914, Accuracy: 8978/10000 (89.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4001, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3977, Accuracy: 8959/10000 (89.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3799, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3735, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3474, Accuracy: 9063/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4307, Accuracy: 8879/10000 (88.8%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3718, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3909, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3751, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8987/10000 (89.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3723, Accuracy: 9033/10000 (90.3%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3650, Accuracy: 9019/10000 (90.2%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4323, Accuracy: 8874/10000 (88.7%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3246, Accuracy: 9103/10000 (91.0%), learning rate: 0.01
is best and save!

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3533, Accuracy: 9078/10000 (90.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2652, Accuracy: 9286/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2631, Accuracy: 9296/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2630, Accuracy: 9318/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2655, Accuracy: 9321/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2645, Accuracy: 9333/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2664, Accuracy: 9331/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2684, Accuracy: 9332/10000 (93.3%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2654, Accuracy: 9345/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2680, Accuracy: 9329/10000 (93.3%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2710, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2702, Accuracy: 9343/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2717, Accuracy: 9347/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2707, Accuracy: 9354/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2739, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2702, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2733, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2717, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2687, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2717, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2715, Accuracy: 9348/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2723, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2713, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2700, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2708, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2704, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2710, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2712, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2724, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2717, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001
Best accuracy: 0.9366 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 57) Prec1: 0.936600
layer index: 88 	 total channel: 32 	 remaining channel: 15
layer index:  22
num_parameters:  802210
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3204, Accuracy: 9116/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3632, Accuracy: 8953/10000 (89.5%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4534, Accuracy: 8756/10000 (87.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4201, Accuracy: 8907/10000 (89.1%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3479, Accuracy: 9069/10000 (90.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3671, Accuracy: 8981/10000 (89.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3409, Accuracy: 9046/10000 (90.5%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3619, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3684, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4202, Accuracy: 8883/10000 (88.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3923, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3567, Accuracy: 9021/10000 (90.2%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3704, Accuracy: 8989/10000 (89.9%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3749, Accuracy: 8995/10000 (89.9%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4049, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4407, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3829, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3632, Accuracy: 9014/10000 (90.1%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3643, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4319, Accuracy: 8917/10000 (89.2%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4957, Accuracy: 8791/10000 (87.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3790, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3599, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3870, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3636, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3575, Accuracy: 9051/10000 (90.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4107, Accuracy: 8945/10000 (89.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3965, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3877, Accuracy: 9009/10000 (90.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9308/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9309/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2561, Accuracy: 9342/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2571, Accuracy: 9345/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2528, Accuracy: 9364/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2506, Accuracy: 9362/10000 (93.6%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2507, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2519, Accuracy: 9365/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2555, Accuracy: 9375/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2563, Accuracy: 9353/10000 (93.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2588, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2586, Accuracy: 9363/10000 (93.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2606, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2628, Accuracy: 9354/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2621, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2603, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2630, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2619, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2583, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2603, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2589, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2601, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2604, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2583, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2597, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2589, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2600, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2581, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2596, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2590, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001
Best accuracy: 0.9375 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 40) Prec1: 0.937500
layer index: 91 	 total channel: 32 	 remaining channel: 15
layer index:  23
num_parameters:  794985
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8910/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3529, Accuracy: 9028/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4202, Accuracy: 8840/10000 (88.4%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3691, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3970, Accuracy: 8954/10000 (89.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4911, Accuracy: 8763/10000 (87.6%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3696, Accuracy: 9030/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4197, Accuracy: 8865/10000 (88.7%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4164, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3814, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3615, Accuracy: 9051/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3459, Accuracy: 9056/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3992, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3615, Accuracy: 9014/10000 (90.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4125, Accuracy: 8914/10000 (89.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3457, Accuracy: 9048/10000 (90.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.5147, Accuracy: 8752/10000 (87.5%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3775, Accuracy: 8973/10000 (89.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3196, Accuracy: 9119/10000 (91.2%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3284, Accuracy: 9110/10000 (91.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3909, Accuracy: 8904/10000 (89.0%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3582, Accuracy: 9012/10000 (90.1%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3650, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4184, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3857, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4039, Accuracy: 8982/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3575, Accuracy: 9054/10000 (90.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4782, Accuracy: 8792/10000 (87.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3810, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3960, Accuracy: 8951/10000 (89.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2565, Accuracy: 9307/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2558, Accuracy: 9318/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2596, Accuracy: 9333/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2617, Accuracy: 9314/10000 (93.1%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2608, Accuracy: 9338/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2645, Accuracy: 9320/10000 (93.2%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2648, Accuracy: 9337/10000 (93.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2641, Accuracy: 9330/10000 (93.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2685, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2689, Accuracy: 9330/10000 (93.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2718, Accuracy: 9353/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2723, Accuracy: 9346/10000 (93.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2736, Accuracy: 9337/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2777, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2714, Accuracy: 9348/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2718, Accuracy: 9355/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2729, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2723, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2708, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2741, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2718, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2731, Accuracy: 9348/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2729, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2717, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2720, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2719, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2722, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2719, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2725, Accuracy: 9342/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2722, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001
Best accuracy: 0.936 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 50) Prec1: 0.936000
layer index: 96 	 total channel: 32 	 remaining channel: 15
layer index:  24
num_parameters:  787760
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3524, Accuracy: 9059/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3725, Accuracy: 9027/10000 (90.3%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4600, Accuracy: 8791/10000 (87.9%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3654, Accuracy: 9011/10000 (90.1%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3813, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4492, Accuracy: 8798/10000 (88.0%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3539, Accuracy: 9023/10000 (90.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3760, Accuracy: 8982/10000 (89.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 8925/10000 (89.2%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3625, Accuracy: 9018/10000 (90.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3836, Accuracy: 8982/10000 (89.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3763, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3553, Accuracy: 9014/10000 (90.1%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3553, Accuracy: 9045/10000 (90.4%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3536, Accuracy: 9029/10000 (90.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3455, Accuracy: 9077/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3939, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3485, Accuracy: 9035/10000 (90.3%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3621, Accuracy: 9043/10000 (90.4%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4210, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4676, Accuracy: 8808/10000 (88.1%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3445, Accuracy: 9053/10000 (90.5%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3737, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3850, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3731, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3422, Accuracy: 9060/10000 (90.6%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3797, Accuracy: 8996/10000 (90.0%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3894, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4149, Accuracy: 8897/10000 (89.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4261, Accuracy: 8838/10000 (88.4%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2741, Accuracy: 9271/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2678, Accuracy: 9297/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2673, Accuracy: 9315/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2696, Accuracy: 9320/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2697, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2681, Accuracy: 9326/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2710, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2679, Accuracy: 9346/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2691, Accuracy: 9355/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2680, Accuracy: 9333/10000 (93.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2729, Accuracy: 9344/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2716, Accuracy: 9360/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2703, Accuracy: 9350/10000 (93.5%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2734, Accuracy: 9351/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2708, Accuracy: 9358/10000 (93.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2694, Accuracy: 9359/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2724, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2699, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2687, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2706, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2699, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2715, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2701, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2692, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2698, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2688, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2699, Accuracy: 9365/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2689, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2709, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2697, Accuracy: 9374/10000 (93.7%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9374 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.937400
layer index: 99 	 total channel: 32 	 remaining channel: 15
layer index:  25
num_parameters:  780535
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3782, Accuracy: 8926/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3730, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4261, Accuracy: 8860/10000 (88.6%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3289, Accuracy: 9060/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3748, Accuracy: 8969/10000 (89.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3541, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3674, Accuracy: 8959/10000 (89.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4134, Accuracy: 8893/10000 (88.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 8914/10000 (89.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3831, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8930/10000 (89.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3802, Accuracy: 8951/10000 (89.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3763, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3353, Accuracy: 9036/10000 (90.4%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4091, Accuracy: 8927/10000 (89.3%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4422, Accuracy: 8857/10000 (88.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3442, Accuracy: 9057/10000 (90.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3457, Accuracy: 9073/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 9034/10000 (90.3%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3658, Accuracy: 9045/10000 (90.4%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3699, Accuracy: 9016/10000 (90.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3533, Accuracy: 9063/10000 (90.6%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3644, Accuracy: 8992/10000 (89.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3681, Accuracy: 9046/10000 (90.5%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3997, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3528, Accuracy: 9046/10000 (90.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3918, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3684, Accuracy: 9072/10000 (90.7%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4033, Accuracy: 8979/10000 (89.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2709, Accuracy: 9270/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2686, Accuracy: 9281/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2688, Accuracy: 9293/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2685, Accuracy: 9320/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2697, Accuracy: 9328/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2720, Accuracy: 9321/10000 (93.2%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2745, Accuracy: 9309/10000 (93.1%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2751, Accuracy: 9330/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2759, Accuracy: 9322/10000 (93.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2769, Accuracy: 9335/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2759, Accuracy: 9311/10000 (93.1%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2760, Accuracy: 9328/10000 (93.3%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2740, Accuracy: 9339/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2784, Accuracy: 9338/10000 (93.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2794, Accuracy: 9327/10000 (93.3%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2798, Accuracy: 9322/10000 (93.2%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2805, Accuracy: 9326/10000 (93.3%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2785, Accuracy: 9324/10000 (93.2%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2770, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2789, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2783, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2803, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2777, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2775, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001
is best and save!

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2776, Accuracy: 9332/10000 (93.3%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2783, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2801, Accuracy: 9333/10000 (93.3%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2793, Accuracy: 9339/10000 (93.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2796, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2776, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001
Best accuracy: 0.9346 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 55) Prec1: 0.934600
layer index: 104 	 total channel: 32 	 remaining channel: 15
layer index:  26
num_parameters:  773310
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3990, Accuracy: 8930/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3939, Accuracy: 8941/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3536, Accuracy: 9003/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4202, Accuracy: 8875/10000 (88.8%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3739, Accuracy: 9021/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3757, Accuracy: 8969/10000 (89.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4390, Accuracy: 8846/10000 (88.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4232, Accuracy: 8877/10000 (88.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4012, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3752, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4036, Accuracy: 8915/10000 (89.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4402, Accuracy: 8826/10000 (88.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3977, Accuracy: 8919/10000 (89.2%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3724, Accuracy: 9002/10000 (90.0%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3900, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3451, Accuracy: 9043/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4020, Accuracy: 8955/10000 (89.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3768, Accuracy: 8969/10000 (89.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3402, Accuracy: 9063/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3578, Accuracy: 9050/10000 (90.5%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4628, Accuracy: 8825/10000 (88.2%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3847, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4495, Accuracy: 8895/10000 (88.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3874, Accuracy: 9000/10000 (90.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3492, Accuracy: 9051/10000 (90.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4142, Accuracy: 8935/10000 (89.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4083, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3922, Accuracy: 8966/10000 (89.7%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3823, Accuracy: 8976/10000 (89.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2540, Accuracy: 9296/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2552, Accuracy: 9314/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9322/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2610, Accuracy: 9334/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2611, Accuracy: 9334/10000 (93.3%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2586, Accuracy: 9350/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2620, Accuracy: 9342/10000 (93.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2603, Accuracy: 9360/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2622, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2645, Accuracy: 9350/10000 (93.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2649, Accuracy: 9351/10000 (93.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2645, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2731, Accuracy: 9362/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2686, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2697, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2706, Accuracy: 9354/10000 (93.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2681, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2646, Accuracy: 9361/10000 (93.6%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2675, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2668, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2697, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2684, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2672, Accuracy: 9357/10000 (93.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2671, Accuracy: 9356/10000 (93.6%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2661, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2669, Accuracy: 9364/10000 (93.6%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2683, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2692, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2649, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9366 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.936600
layer index: 107 	 total channel: 32 	 remaining channel: 15
layer index:  27
num_parameters:  766085
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3538, Accuracy: 8978/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4215, Accuracy: 8805/10000 (88.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4498, Accuracy: 8762/10000 (87.6%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4569, Accuracy: 8736/10000 (87.4%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3537, Accuracy: 8981/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4526, Accuracy: 8812/10000 (88.1%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3771, Accuracy: 9010/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3662, Accuracy: 9011/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3637, Accuracy: 8992/10000 (89.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3660, Accuracy: 8989/10000 (89.9%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4082, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3762, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3934, Accuracy: 8933/10000 (89.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3975, Accuracy: 8930/10000 (89.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4043, Accuracy: 8889/10000 (88.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4435, Accuracy: 8885/10000 (88.8%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4389, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4383, Accuracy: 8831/10000 (88.3%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3291, Accuracy: 9082/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.5707, Accuracy: 8631/10000 (86.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4443, Accuracy: 8857/10000 (88.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3908, Accuracy: 8924/10000 (89.2%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4189, Accuracy: 8921/10000 (89.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3359, Accuracy: 9091/10000 (90.9%), learning rate: 0.01
is best and save!

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3698, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4421, Accuracy: 8893/10000 (88.9%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3790, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4147, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3665, Accuracy: 9022/10000 (90.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2601, Accuracy: 9307/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2536, Accuracy: 9332/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2521, Accuracy: 9347/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2576, Accuracy: 9343/10000 (93.4%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2567, Accuracy: 9358/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2567, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2596, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2615, Accuracy: 9367/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2582, Accuracy: 9369/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2600, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2643, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2681, Accuracy: 9372/10000 (93.7%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2617, Accuracy: 9379/10000 (93.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2684, Accuracy: 9372/10000 (93.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2710, Accuracy: 9362/10000 (93.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2686, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2690, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2673, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2658, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2693, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2662, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2703, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2684, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2675, Accuracy: 9377/10000 (93.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2673, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2673, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2683, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2671, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2693, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2674, Accuracy: 9371/10000 (93.7%), learning rate: 0.0001
Best accuracy: 0.9379 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 44) Prec1: 0.937900
layer index: 112 	 total channel: 32 	 remaining channel: 15
layer index:  28
num_parameters:  758860
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3485, Accuracy: 9023/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4297, Accuracy: 8795/10000 (87.9%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3896, Accuracy: 8934/10000 (89.3%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4184, Accuracy: 8891/10000 (88.9%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3805, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3686, Accuracy: 8987/10000 (89.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4090, Accuracy: 8867/10000 (88.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3875, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3987, Accuracy: 8973/10000 (89.7%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3781, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3519, Accuracy: 9031/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3579, Accuracy: 9041/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.5163, Accuracy: 8605/10000 (86.1%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3721, Accuracy: 8972/10000 (89.7%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3471, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4652, Accuracy: 8765/10000 (87.7%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3971, Accuracy: 8927/10000 (89.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4163, Accuracy: 8937/10000 (89.4%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3626, Accuracy: 9056/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4159, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3837, Accuracy: 8949/10000 (89.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3884, Accuracy: 8939/10000 (89.4%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4140, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3708, Accuracy: 8997/10000 (90.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3756, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3505, Accuracy: 9054/10000 (90.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3712, Accuracy: 9035/10000 (90.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4618, Accuracy: 8855/10000 (88.6%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4110, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4198, Accuracy: 8877/10000 (88.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2560, Accuracy: 9298/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2491, Accuracy: 9338/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2523, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2520, Accuracy: 9360/10000 (93.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2543, Accuracy: 9358/10000 (93.6%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2552, Accuracy: 9335/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2560, Accuracy: 9341/10000 (93.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2585, Accuracy: 9346/10000 (93.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2603, Accuracy: 9356/10000 (93.6%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2575, Accuracy: 9349/10000 (93.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2617, Accuracy: 9340/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2613, Accuracy: 9353/10000 (93.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9359/10000 (93.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2595, Accuracy: 9352/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2577, Accuracy: 9366/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2581, Accuracy: 9360/10000 (93.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2586, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2569, Accuracy: 9372/10000 (93.7%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2556, Accuracy: 9376/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2583, Accuracy: 9379/10000 (93.8%), learning rate: 0.0001
is best and save!

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2581, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2592, Accuracy: 9369/10000 (93.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2569, Accuracy: 9363/10000 (93.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2575, Accuracy: 9367/10000 (93.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2584, Accuracy: 9368/10000 (93.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2580, Accuracy: 9378/10000 (93.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2594, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2596, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2588, Accuracy: 9370/10000 (93.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2581, Accuracy: 9375/10000 (93.8%), learning rate: 0.0001
Best accuracy: 0.9379 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 51) Prec1: 0.937900
layer index: 115 	 total channel: 32 	 remaining channel: 16
layer index:  29
num_parameters:  752060
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4089, Accuracy: 8810/10000 (88.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4311, Accuracy: 8817/10000 (88.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4646, Accuracy: 8768/10000 (87.7%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4087, Accuracy: 8835/10000 (88.3%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3624, Accuracy: 8976/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4107, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3954, Accuracy: 8952/10000 (89.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3418, Accuracy: 9048/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3634, Accuracy: 9004/10000 (90.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3701, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4148, Accuracy: 8907/10000 (89.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3464, Accuracy: 9041/10000 (90.4%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3843, Accuracy: 8957/10000 (89.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3957, Accuracy: 8970/10000 (89.7%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3673, Accuracy: 9030/10000 (90.3%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4010, Accuracy: 8923/10000 (89.2%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3553, Accuracy: 9042/10000 (90.4%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4951, Accuracy: 8709/10000 (87.1%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3784, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3385, Accuracy: 9070/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3680, Accuracy: 9035/10000 (90.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3875, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3834, Accuracy: 8965/10000 (89.7%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3616, Accuracy: 9025/10000 (90.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3715, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3858, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3671, Accuracy: 9018/10000 (90.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3507, Accuracy: 9045/10000 (90.4%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2672, Accuracy: 9300/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2636, Accuracy: 9307/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2622, Accuracy: 9330/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2663, Accuracy: 9325/10000 (93.2%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2669, Accuracy: 9327/10000 (93.3%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2665, Accuracy: 9326/10000 (93.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2690, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2677, Accuracy: 9329/10000 (93.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9346/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2703, Accuracy: 9348/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2716, Accuracy: 9334/10000 (93.3%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2710, Accuracy: 9349/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2732, Accuracy: 9354/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2733, Accuracy: 9354/10000 (93.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2711, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2710, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2722, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2717, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2700, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2739, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2720, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2733, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2720, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2713, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2733, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2709, Accuracy: 9351/10000 (93.5%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2730, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2734, Accuracy: 9349/10000 (93.5%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2744, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2712, Accuracy: 9352/10000 (93.5%), learning rate: 0.0001
Best accuracy: 0.9354 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 44) Prec1: 0.935400
layer index: 120 	 total channel: 32 	 remaining channel: 16
layer index:  30
num_parameters:  745116
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3667, Accuracy: 8973/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3941, Accuracy: 8912/10000 (89.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3980, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3959, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3850, Accuracy: 8944/10000 (89.4%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4336, Accuracy: 8828/10000 (88.3%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3943, Accuracy: 8919/10000 (89.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3692, Accuracy: 8949/10000 (89.5%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3598, Accuracy: 9005/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3634, Accuracy: 9011/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3628, Accuracy: 9001/10000 (90.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3979, Accuracy: 8908/10000 (89.1%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4339, Accuracy: 8838/10000 (88.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3948, Accuracy: 8914/10000 (89.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4261, Accuracy: 8856/10000 (88.6%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3935, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4026, Accuracy: 8930/10000 (89.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3988, Accuracy: 8940/10000 (89.4%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3924, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3617, Accuracy: 9035/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3947, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3680, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4045, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3693, Accuracy: 9020/10000 (90.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3897, Accuracy: 8978/10000 (89.8%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3932, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3651, Accuracy: 9013/10000 (90.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3528, Accuracy: 9034/10000 (90.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3941, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4110, Accuracy: 8887/10000 (88.9%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2674, Accuracy: 9277/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9288/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2725, Accuracy: 9298/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2748, Accuracy: 9315/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2748, Accuracy: 9310/10000 (93.1%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2781, Accuracy: 9308/10000 (93.1%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2760, Accuracy: 9325/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2756, Accuracy: 9328/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2767, Accuracy: 9319/10000 (93.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2781, Accuracy: 9324/10000 (93.2%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2806, Accuracy: 9332/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2811, Accuracy: 9325/10000 (93.2%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2796, Accuracy: 9339/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2842, Accuracy: 9324/10000 (93.2%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2811, Accuracy: 9328/10000 (93.3%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2815, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2827, Accuracy: 9331/10000 (93.3%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2808, Accuracy: 9330/10000 (93.3%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2787, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2821, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2825, Accuracy: 9330/10000 (93.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2832, Accuracy: 9323/10000 (93.2%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2814, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2801, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001
is best and save!

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2809, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2811, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2830, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2824, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2825, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2808, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001
is best and save!
Best accuracy: 0.935 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.935000
layer index: 123 	 total channel: 32 	 remaining channel: 16
layer index:  31
num_parameters:  738172
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3630, Accuracy: 8977/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4055, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3979, Accuracy: 8867/10000 (88.7%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3739, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3694, Accuracy: 8957/10000 (89.6%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3833, Accuracy: 8970/10000 (89.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4024, Accuracy: 8858/10000 (88.6%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3989, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3869, Accuracy: 8964/10000 (89.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3491, Accuracy: 9025/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3994, Accuracy: 8919/10000 (89.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3722, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4480, Accuracy: 8848/10000 (88.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4158, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4168, Accuracy: 8883/10000 (88.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3751, Accuracy: 9026/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3930, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4225, Accuracy: 8869/10000 (88.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3697, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3387, Accuracy: 9066/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3554, Accuracy: 9069/10000 (90.7%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4230, Accuracy: 8877/10000 (88.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3728, Accuracy: 9005/10000 (90.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4203, Accuracy: 8866/10000 (88.7%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3801, Accuracy: 8993/10000 (89.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3555, Accuracy: 9022/10000 (90.2%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3673, Accuracy: 9017/10000 (90.2%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4105, Accuracy: 8929/10000 (89.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3774, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4334, Accuracy: 8875/10000 (88.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2767, Accuracy: 9254/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2733, Accuracy: 9274/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2778, Accuracy: 9280/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2796, Accuracy: 9300/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2774, Accuracy: 9316/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2761, Accuracy: 9304/10000 (93.0%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2755, Accuracy: 9314/10000 (93.1%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2752, Accuracy: 9326/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2771, Accuracy: 9316/10000 (93.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2778, Accuracy: 9330/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2755, Accuracy: 9331/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2821, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2822, Accuracy: 9330/10000 (93.3%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2835, Accuracy: 9322/10000 (93.2%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2837, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2825, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2820, Accuracy: 9329/10000 (93.3%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2811, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2792, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2823, Accuracy: 9326/10000 (93.3%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2808, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2823, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2799, Accuracy: 9331/10000 (93.3%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2807, Accuracy: 9330/10000 (93.3%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2811, Accuracy: 9331/10000 (93.3%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2810, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2814, Accuracy: 9338/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2829, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2831, Accuracy: 9325/10000 (93.2%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2797, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001
Best accuracy: 0.9344 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 46) Prec1: 0.934400
layer index: 128 	 total channel: 32 	 remaining channel: 16
layer index:  32
num_parameters:  731228
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3995, Accuracy: 8913/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4149, Accuracy: 8872/10000 (88.7%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4034, Accuracy: 8850/10000 (88.5%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3727, Accuracy: 8955/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4220, Accuracy: 8871/10000 (88.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3569, Accuracy: 9032/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3647, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3816, Accuracy: 8936/10000 (89.4%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3765, Accuracy: 8987/10000 (89.9%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4144, Accuracy: 8908/10000 (89.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3832, Accuracy: 8966/10000 (89.7%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4013, Accuracy: 8924/10000 (89.2%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3940, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4472, Accuracy: 8847/10000 (88.5%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3656, Accuracy: 9009/10000 (90.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4797, Accuracy: 8751/10000 (87.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4013, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3994, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3992, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3826, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3937, Accuracy: 8958/10000 (89.6%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3854, Accuracy: 8963/10000 (89.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3431, Accuracy: 9080/10000 (90.8%), learning rate: 0.01
is best and save!

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3916, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4193, Accuracy: 8893/10000 (88.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3920, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4175, Accuracy: 8928/10000 (89.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3976, Accuracy: 8896/10000 (89.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4024, Accuracy: 8949/10000 (89.5%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4458, Accuracy: 8824/10000 (88.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2659, Accuracy: 9275/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2608, Accuracy: 9305/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2623, Accuracy: 9303/10000 (93.0%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2643, Accuracy: 9323/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2642, Accuracy: 9322/10000 (93.2%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2668, Accuracy: 9317/10000 (93.2%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2667, Accuracy: 9317/10000 (93.2%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2681, Accuracy: 9321/10000 (93.2%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2697, Accuracy: 9335/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2689, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2734, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2742, Accuracy: 9349/10000 (93.5%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2738, Accuracy: 9336/10000 (93.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2761, Accuracy: 9338/10000 (93.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2758, Accuracy: 9329/10000 (93.3%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2754, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2769, Accuracy: 9346/10000 (93.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2745, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2729, Accuracy: 9339/10000 (93.4%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2783, Accuracy: 9335/10000 (93.3%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2754, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2766, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2752, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2744, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2748, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2752, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2764, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2756, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2774, Accuracy: 9345/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2746, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001
Best accuracy: 0.9349 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 43) Prec1: 0.934900
layer index: 131 	 total channel: 32 	 remaining channel: 16
layer index:  33
num_parameters:  724284
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3895, Accuracy: 8869/10000 (88.7%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3994, Accuracy: 8912/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3969, Accuracy: 8838/10000 (88.4%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4622, Accuracy: 8715/10000 (87.2%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4764, Accuracy: 8677/10000 (86.8%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3975, Accuracy: 8868/10000 (88.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4041, Accuracy: 8875/10000 (88.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8937/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3633, Accuracy: 9021/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4037, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3621, Accuracy: 9036/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4242, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3880, Accuracy: 8931/10000 (89.3%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4076, Accuracy: 8934/10000 (89.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4089, Accuracy: 8925/10000 (89.2%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3803, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3574, Accuracy: 9061/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4065, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4068, Accuracy: 8973/10000 (89.7%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3636, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8942/10000 (89.4%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3715, Accuracy: 8978/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3609, Accuracy: 9021/10000 (90.2%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4116, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4146, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3434, Accuracy: 9063/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.5050, Accuracy: 8665/10000 (86.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4293, Accuracy: 8912/10000 (89.1%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4117, Accuracy: 8944/10000 (89.4%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3702, Accuracy: 9012/10000 (90.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2799, Accuracy: 9254/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2741, Accuracy: 9284/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2755, Accuracy: 9296/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2767, Accuracy: 9312/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2772, Accuracy: 9302/10000 (93.0%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2758, Accuracy: 9306/10000 (93.1%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2772, Accuracy: 9327/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2798, Accuracy: 9314/10000 (93.1%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2799, Accuracy: 9324/10000 (93.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2820, Accuracy: 9328/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2867, Accuracy: 9316/10000 (93.2%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9343/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2827, Accuracy: 9332/10000 (93.3%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2845, Accuracy: 9327/10000 (93.3%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2844, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2836, Accuracy: 9330/10000 (93.3%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2819, Accuracy: 9341/10000 (93.4%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2807, Accuracy: 9350/10000 (93.5%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2841, Accuracy: 9336/10000 (93.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2828, Accuracy: 9337/10000 (93.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2833, Accuracy: 9344/10000 (93.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2823, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2833, Accuracy: 9334/10000 (93.3%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2828, Accuracy: 9342/10000 (93.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2833, Accuracy: 9343/10000 (93.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2823, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9340/10000 (93.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2826, Accuracy: 9347/10000 (93.5%), learning rate: 0.0001
Best accuracy: 0.935 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 50) Prec1: 0.935000
layer index: 136 	 total channel: 32 	 remaining channel: 16
layer index:  34
num_parameters:  717340
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4526, Accuracy: 8802/10000 (88.0%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3911, Accuracy: 8900/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4497, Accuracy: 8765/10000 (87.7%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4235, Accuracy: 8821/10000 (88.2%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3986, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3633, Accuracy: 8975/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4008, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4168, Accuracy: 8896/10000 (89.0%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4338, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3971, Accuracy: 8925/10000 (89.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3951, Accuracy: 8931/10000 (89.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3806, Accuracy: 8976/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3449, Accuracy: 9038/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3469, Accuracy: 9051/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3635, Accuracy: 8998/10000 (90.0%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4150, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4400, Accuracy: 8809/10000 (88.1%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4167, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3777, Accuracy: 8995/10000 (89.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3916, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3668, Accuracy: 9003/10000 (90.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3719, Accuracy: 8985/10000 (89.8%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3790, Accuracy: 8986/10000 (89.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3902, Accuracy: 8952/10000 (89.5%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3897, Accuracy: 8934/10000 (89.3%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3803, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4032, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4677, Accuracy: 8777/10000 (87.8%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4041, Accuracy: 8916/10000 (89.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2844, Accuracy: 9242/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2815, Accuracy: 9270/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2841, Accuracy: 9272/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2874, Accuracy: 9289/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2852, Accuracy: 9299/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2848, Accuracy: 9314/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2854, Accuracy: 9314/10000 (93.1%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2844, Accuracy: 9325/10000 (93.2%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2860, Accuracy: 9327/10000 (93.3%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2868, Accuracy: 9317/10000 (93.2%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2901, Accuracy: 9311/10000 (93.1%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2908, Accuracy: 9312/10000 (93.1%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2901, Accuracy: 9317/10000 (93.2%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2937, Accuracy: 9292/10000 (92.9%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2940, Accuracy: 9309/10000 (93.1%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2936, Accuracy: 9310/10000 (93.1%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2940, Accuracy: 9308/10000 (93.1%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2930, Accuracy: 9308/10000 (93.1%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2906, Accuracy: 9321/10000 (93.2%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9314/10000 (93.1%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2914, Accuracy: 9326/10000 (93.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2922, Accuracy: 9315/10000 (93.2%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2901, Accuracy: 9318/10000 (93.2%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2904, Accuracy: 9323/10000 (93.2%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2908, Accuracy: 9319/10000 (93.2%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2891, Accuracy: 9333/10000 (93.3%), learning rate: 0.0001
is best and save!

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2915, Accuracy: 9330/10000 (93.3%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2905, Accuracy: 9322/10000 (93.2%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2906, Accuracy: 9314/10000 (93.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2888, Accuracy: 9339/10000 (93.4%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9339 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.933900
layer index: 139 	 total channel: 32 	 remaining channel: 16
layer index:  35
num_parameters:  710396
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4602, Accuracy: 8694/10000 (86.9%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4150, Accuracy: 8800/10000 (88.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.5534, Accuracy: 8527/10000 (85.3%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3911, Accuracy: 8897/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4786, Accuracy: 8724/10000 (87.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3892, Accuracy: 8929/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3956, Accuracy: 8891/10000 (88.9%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4145, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4102, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4296, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3776, Accuracy: 8948/10000 (89.5%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4373, Accuracy: 8831/10000 (88.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4314, Accuracy: 8798/10000 (88.0%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4311, Accuracy: 8856/10000 (88.6%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3603, Accuracy: 8984/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4239, Accuracy: 8859/10000 (88.6%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4425, Accuracy: 8866/10000 (88.7%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4841, Accuracy: 8769/10000 (87.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4066, Accuracy: 8961/10000 (89.6%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3387, Accuracy: 9061/10000 (90.6%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4043, Accuracy: 8932/10000 (89.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4038, Accuracy: 8920/10000 (89.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3807, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4076, Accuracy: 8942/10000 (89.4%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3832, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3600, Accuracy: 9038/10000 (90.4%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3784, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4211, Accuracy: 8911/10000 (89.1%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3888, Accuracy: 8964/10000 (89.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4679, Accuracy: 8792/10000 (87.9%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2788, Accuracy: 9226/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2793, Accuracy: 9259/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2791, Accuracy: 9262/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2798, Accuracy: 9287/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2775, Accuracy: 9293/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2778, Accuracy: 9294/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2846, Accuracy: 9292/10000 (92.9%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2803, Accuracy: 9301/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2880, Accuracy: 9288/10000 (92.9%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2864, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2889, Accuracy: 9295/10000 (92.9%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2821, Accuracy: 9337/10000 (93.4%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2864, Accuracy: 9313/10000 (93.1%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2885, Accuracy: 9320/10000 (93.2%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2884, Accuracy: 9320/10000 (93.2%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2888, Accuracy: 9321/10000 (93.2%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2880, Accuracy: 9321/10000 (93.2%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2858, Accuracy: 9327/10000 (93.3%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2883, Accuracy: 9316/10000 (93.2%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2877, Accuracy: 9312/10000 (93.1%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2905, Accuracy: 9319/10000 (93.2%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2881, Accuracy: 9321/10000 (93.2%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2861, Accuracy: 9320/10000 (93.2%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2881, Accuracy: 9311/10000 (93.1%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2876, Accuracy: 9321/10000 (93.2%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2881, Accuracy: 9323/10000 (93.2%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2884, Accuracy: 9317/10000 (93.2%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2892, Accuracy: 9308/10000 (93.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2862, Accuracy: 9323/10000 (93.2%), learning rate: 0.0001
Best accuracy: 0.9337 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 44) Prec1: 0.933700
layer index: 144 	 total channel: 32 	 remaining channel: 18
layer index:  36
num_parameters:  704320
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8926/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4317, Accuracy: 8804/10000 (88.0%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4750, Accuracy: 8696/10000 (87.0%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3873, Accuracy: 8883/10000 (88.8%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4116, Accuracy: 8848/10000 (88.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4000, Accuracy: 8882/10000 (88.8%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4626, Accuracy: 8778/10000 (87.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3532, Accuracy: 8990/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4023, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 8940/10000 (89.4%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4086, Accuracy: 8917/10000 (89.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3566, Accuracy: 9004/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.6247, Accuracy: 8493/10000 (84.9%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4134, Accuracy: 8935/10000 (89.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3863, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4058, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4273, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4045, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3991, Accuracy: 8962/10000 (89.6%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3551, Accuracy: 9040/10000 (90.4%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3899, Accuracy: 8967/10000 (89.7%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3746, Accuracy: 8983/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8968/10000 (89.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3990, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3979, Accuracy: 8925/10000 (89.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4411, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4334, Accuracy: 8835/10000 (88.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4322, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3893, Accuracy: 8939/10000 (89.4%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8929/10000 (89.3%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9220/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2849, Accuracy: 9235/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9246/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2875, Accuracy: 9261/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2852, Accuracy: 9271/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2843, Accuracy: 9271/10000 (92.7%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2877, Accuracy: 9292/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2893, Accuracy: 9282/10000 (92.8%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2927, Accuracy: 9283/10000 (92.8%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2948, Accuracy: 9276/10000 (92.8%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2927, Accuracy: 9283/10000 (92.8%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2953, Accuracy: 9282/10000 (92.8%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2928, Accuracy: 9289/10000 (92.9%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2986, Accuracy: 9273/10000 (92.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2969, Accuracy: 9294/10000 (92.9%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2955, Accuracy: 9298/10000 (93.0%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2980, Accuracy: 9290/10000 (92.9%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2968, Accuracy: 9288/10000 (92.9%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2941, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2983, Accuracy: 9291/10000 (92.9%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2959, Accuracy: 9299/10000 (93.0%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2983, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2958, Accuracy: 9293/10000 (92.9%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2948, Accuracy: 9297/10000 (93.0%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2959, Accuracy: 9297/10000 (93.0%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2963, Accuracy: 9291/10000 (92.9%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2969, Accuracy: 9294/10000 (92.9%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2963, Accuracy: 9300/10000 (93.0%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2984, Accuracy: 9299/10000 (93.0%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2963, Accuracy: 9287/10000 (92.9%), learning rate: 0.0001
Best accuracy: 0.93 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 59) Prec1: 0.930000
layer index: 147 	 total channel: 32 	 remaining channel: 16
layer index:  37
num_parameters:  692480
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4580, Accuracy: 8636/10000 (86.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4690, Accuracy: 8643/10000 (86.4%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4276, Accuracy: 8833/10000 (88.3%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4124, Accuracy: 8825/10000 (88.2%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4197, Accuracy: 8838/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4489, Accuracy: 8754/10000 (87.5%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4273, Accuracy: 8854/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4252, Accuracy: 8827/10000 (88.3%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4054, Accuracy: 8926/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4096, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3931, Accuracy: 8952/10000 (89.5%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3779, Accuracy: 8998/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4288, Accuracy: 8832/10000 (88.3%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4581, Accuracy: 8828/10000 (88.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4007, Accuracy: 8904/10000 (89.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3852, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4641, Accuracy: 8772/10000 (87.7%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4045, Accuracy: 8915/10000 (89.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4820, Accuracy: 8792/10000 (87.9%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3496, Accuracy: 9050/10000 (90.5%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4063, Accuracy: 8929/10000 (89.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3707, Accuracy: 9008/10000 (90.1%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4257, Accuracy: 8869/10000 (88.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4310, Accuracy: 8879/10000 (88.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4042, Accuracy: 8943/10000 (89.4%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3742, Accuracy: 9021/10000 (90.2%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4628, Accuracy: 8794/10000 (87.9%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3925, Accuracy: 8999/10000 (90.0%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4991, Accuracy: 8723/10000 (87.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4434, Accuracy: 8882/10000 (88.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2895, Accuracy: 9223/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2902, Accuracy: 9222/10000 (92.2%), learning rate: 0.001

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2882, Accuracy: 9236/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2916, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2910, Accuracy: 9265/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2942, Accuracy: 9248/10000 (92.5%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2940, Accuracy: 9267/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2982, Accuracy: 9258/10000 (92.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2983, Accuracy: 9269/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2994, Accuracy: 9269/10000 (92.7%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.3014, Accuracy: 9256/10000 (92.6%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.3006, Accuracy: 9280/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2976, Accuracy: 9289/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.3037, Accuracy: 9268/10000 (92.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3010, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.3001, Accuracy: 9291/10000 (92.9%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.3020, Accuracy: 9276/10000 (92.8%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.3005, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2993, Accuracy: 9291/10000 (92.9%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.3012, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.3004, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.3043, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.3015, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.3020, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.3014, Accuracy: 9283/10000 (92.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2999, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.3005, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.3005, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.3026, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2996, Accuracy: 9292/10000 (92.9%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9292 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.929200
layer index: 153 	 total channel: 64 	 remaining channel: 32
layer index:  38
num_parameters:  669376
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4510, Accuracy: 8677/10000 (86.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3824, Accuracy: 8896/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3975, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4176, Accuracy: 8834/10000 (88.3%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3718, Accuracy: 8950/10000 (89.5%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.5048, Accuracy: 8639/10000 (86.4%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.5045, Accuracy: 8647/10000 (86.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4013, Accuracy: 8830/10000 (88.3%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3913, Accuracy: 8917/10000 (89.2%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3866, Accuracy: 8923/10000 (89.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3911, Accuracy: 8958/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3830, Accuracy: 8953/10000 (89.5%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4093, Accuracy: 8872/10000 (88.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3670, Accuracy: 8964/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3962, Accuracy: 8924/10000 (89.2%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8991/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4407, Accuracy: 8858/10000 (88.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.5107, Accuracy: 8673/10000 (86.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3946, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3800, Accuracy: 8979/10000 (89.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4505, Accuracy: 8861/10000 (88.6%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3860, Accuracy: 8962/10000 (89.6%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3634, Accuracy: 8999/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3878, Accuracy: 8956/10000 (89.6%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4269, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3787, Accuracy: 8982/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3570, Accuracy: 9021/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3702, Accuracy: 8972/10000 (89.7%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4028, Accuracy: 8890/10000 (88.9%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4652, Accuracy: 8769/10000 (87.7%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2848, Accuracy: 9226/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2869, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2915, Accuracy: 9241/10000 (92.4%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2970, Accuracy: 9268/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2951, Accuracy: 9256/10000 (92.6%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2941, Accuracy: 9272/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2982, Accuracy: 9274/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2947, Accuracy: 9267/10000 (92.7%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2985, Accuracy: 9277/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.3011, Accuracy: 9266/10000 (92.7%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.3045, Accuracy: 9275/10000 (92.8%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.3059, Accuracy: 9270/10000 (92.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.3067, Accuracy: 9267/10000 (92.7%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.3099, Accuracy: 9268/10000 (92.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3051, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.3040, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.3060, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.3036, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.3031, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.3044, Accuracy: 9285/10000 (92.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.3041, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.3061, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.3042, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.3054, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.3051, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.3045, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.3048, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.3054, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.3060, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.3043, Accuracy: 9292/10000 (92.9%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9292 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.929200
layer index: 156 	 total channel: 64 	 remaining channel: 32
layer index:  39
num_parameters:  641664
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4514, Accuracy: 8628/10000 (86.3%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3921, Accuracy: 8794/10000 (87.9%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4721, Accuracy: 8651/10000 (86.5%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3786, Accuracy: 8840/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4276, Accuracy: 8761/10000 (87.6%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3896, Accuracy: 8879/10000 (88.8%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3836, Accuracy: 8877/10000 (88.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4100, Accuracy: 8869/10000 (88.7%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4759, Accuracy: 8696/10000 (87.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3782, Accuracy: 8941/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3852, Accuracy: 8903/10000 (89.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3572, Accuracy: 8983/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3659, Accuracy: 8951/10000 (89.5%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4593, Accuracy: 8779/10000 (87.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4663, Accuracy: 8731/10000 (87.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4139, Accuracy: 8875/10000 (88.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4581, Accuracy: 8733/10000 (87.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4159, Accuracy: 8862/10000 (88.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3840, Accuracy: 8985/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3500, Accuracy: 9035/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4182, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4040, Accuracy: 8893/10000 (88.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4244, Accuracy: 8844/10000 (88.4%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4019, Accuracy: 8882/10000 (88.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3883, Accuracy: 8920/10000 (89.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4321, Accuracy: 8869/10000 (88.7%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4044, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4430, Accuracy: 8834/10000 (88.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3651, Accuracy: 8980/10000 (89.8%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8951/10000 (89.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2843, Accuracy: 9226/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2771, Accuracy: 9256/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2750, Accuracy: 9252/10000 (92.5%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2767, Accuracy: 9287/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2757, Accuracy: 9306/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2781, Accuracy: 9291/10000 (92.9%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2807, Accuracy: 9295/10000 (92.9%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2816, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2844, Accuracy: 9287/10000 (92.9%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2877, Accuracy: 9292/10000 (92.9%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2874, Accuracy: 9289/10000 (92.9%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2847, Accuracy: 9303/10000 (93.0%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9300/10000 (93.0%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2870, Accuracy: 9312/10000 (93.1%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2866, Accuracy: 9289/10000 (92.9%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2877, Accuracy: 9290/10000 (92.9%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2882, Accuracy: 9297/10000 (93.0%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2883, Accuracy: 9298/10000 (93.0%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2854, Accuracy: 9303/10000 (93.0%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2872, Accuracy: 9305/10000 (93.1%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2874, Accuracy: 9314/10000 (93.1%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2879, Accuracy: 9315/10000 (93.2%), learning rate: 0.0001
is best and save!

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2866, Accuracy: 9303/10000 (93.0%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2866, Accuracy: 9314/10000 (93.1%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2871, Accuracy: 9313/10000 (93.1%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2863, Accuracy: 9306/10000 (93.1%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2866, Accuracy: 9311/10000 (93.1%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2870, Accuracy: 9317/10000 (93.2%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2895, Accuracy: 9303/10000 (93.0%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2873, Accuracy: 9310/10000 (93.1%), learning rate: 0.0001
Best accuracy: 0.9317 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 59) Prec1: 0.931700
layer index: 161 	 total channel: 64 	 remaining channel: 32
layer index:  40
num_parameters:  613952
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4401, Accuracy: 8714/10000 (87.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4388, Accuracy: 8783/10000 (87.8%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4242, Accuracy: 8793/10000 (87.9%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3920, Accuracy: 8876/10000 (88.8%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4580, Accuracy: 8757/10000 (87.6%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4028, Accuracy: 8856/10000 (88.6%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.5101, Accuracy: 8619/10000 (86.2%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4346, Accuracy: 8809/10000 (88.1%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3652, Accuracy: 9007/10000 (90.1%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4238, Accuracy: 8842/10000 (88.4%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4216, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3969, Accuracy: 8923/10000 (89.2%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4093, Accuracy: 8842/10000 (88.4%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4245, Accuracy: 8861/10000 (88.6%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4221, Accuracy: 8878/10000 (88.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4079, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4015, Accuracy: 8892/10000 (88.9%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4100, Accuracy: 8888/10000 (88.9%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3869, Accuracy: 8938/10000 (89.4%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3660, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3804, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4152, Accuracy: 8867/10000 (88.7%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.5056, Accuracy: 8684/10000 (86.8%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3883, Accuracy: 8904/10000 (89.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4221, Accuracy: 8871/10000 (88.7%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4180, Accuracy: 8911/10000 (89.1%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3643, Accuracy: 9020/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4073, Accuracy: 8915/10000 (89.2%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4160, Accuracy: 8833/10000 (88.3%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4095, Accuracy: 8928/10000 (89.3%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2828, Accuracy: 9203/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2810, Accuracy: 9209/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2823, Accuracy: 9253/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2854, Accuracy: 9266/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2892, Accuracy: 9248/10000 (92.5%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2875, Accuracy: 9250/10000 (92.5%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9256/10000 (92.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2907, Accuracy: 9273/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2927, Accuracy: 9271/10000 (92.7%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2974, Accuracy: 9268/10000 (92.7%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2985, Accuracy: 9270/10000 (92.7%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.3017, Accuracy: 9269/10000 (92.7%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2970, Accuracy: 9276/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.3024, Accuracy: 9272/10000 (92.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3027, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.3022, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.3032, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.3034, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.3008, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.3046, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.3027, Accuracy: 9285/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.3031, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.3011, Accuracy: 9287/10000 (92.9%), learning rate: 0.0001
is best and save!

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.3012, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.3024, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.3016, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.3017, Accuracy: 9276/10000 (92.8%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.3005, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.3027, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.3013, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
Best accuracy: 0.9287 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 54) Prec1: 0.928700
layer index: 164 	 total channel: 64 	 remaining channel: 32
layer index:  41
num_parameters:  586240
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4140, Accuracy: 8748/10000 (87.5%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4378, Accuracy: 8738/10000 (87.4%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4073, Accuracy: 8778/10000 (87.8%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4090, Accuracy: 8800/10000 (88.0%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3998, Accuracy: 8828/10000 (88.3%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3853, Accuracy: 8908/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4727, Accuracy: 8676/10000 (86.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4251, Accuracy: 8817/10000 (88.2%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 8903/10000 (89.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4073, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3693, Accuracy: 8919/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4140, Accuracy: 8879/10000 (88.8%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4559, Accuracy: 8763/10000 (87.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4338, Accuracy: 8801/10000 (88.0%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3930, Accuracy: 8920/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4430, Accuracy: 8835/10000 (88.3%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4273, Accuracy: 8832/10000 (88.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4551, Accuracy: 8741/10000 (87.4%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4453, Accuracy: 8749/10000 (87.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4493, Accuracy: 8759/10000 (87.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3873, Accuracy: 8960/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3382, Accuracy: 9034/10000 (90.3%), learning rate: 0.01
is best and save!

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3629, Accuracy: 8994/10000 (89.9%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4183, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4340, Accuracy: 8817/10000 (88.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3768, Accuracy: 8973/10000 (89.7%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4025, Accuracy: 8874/10000 (88.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4200, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4076, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4473, Accuracy: 8788/10000 (87.9%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2770, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2791, Accuracy: 9269/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2806, Accuracy: 9267/10000 (92.7%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2816, Accuracy: 9295/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2799, Accuracy: 9302/10000 (93.0%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2812, Accuracy: 9302/10000 (93.0%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2837, Accuracy: 9283/10000 (92.8%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2885, Accuracy: 9295/10000 (92.9%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2907, Accuracy: 9286/10000 (92.9%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2917, Accuracy: 9290/10000 (92.9%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2932, Accuracy: 9292/10000 (92.9%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2918, Accuracy: 9302/10000 (93.0%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2946, Accuracy: 9288/10000 (92.9%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2909, Accuracy: 9294/10000 (92.9%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2904, Accuracy: 9304/10000 (93.0%), learning rate: 0.0001
is best and save!

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2921, Accuracy: 9299/10000 (93.0%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2912, Accuracy: 9310/10000 (93.1%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2885, Accuracy: 9315/10000 (93.2%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2931, Accuracy: 9302/10000 (93.0%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2917, Accuracy: 9315/10000 (93.2%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2935, Accuracy: 9311/10000 (93.1%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2911, Accuracy: 9304/10000 (93.0%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2909, Accuracy: 9299/10000 (93.0%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2914, Accuracy: 9312/10000 (93.1%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2903, Accuracy: 9310/10000 (93.1%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2903, Accuracy: 9318/10000 (93.2%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2917, Accuracy: 9308/10000 (93.1%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2931, Accuracy: 9307/10000 (93.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2906, Accuracy: 9304/10000 (93.0%), learning rate: 0.0001
Best accuracy: 0.9318 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.931800
layer index: 169 	 total channel: 64 	 remaining channel: 34
layer index:  42
num_parameters:  560260
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3692, Accuracy: 8885/10000 (88.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4985, Accuracy: 8650/10000 (86.5%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4460, Accuracy: 8810/10000 (88.1%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4024, Accuracy: 8828/10000 (88.3%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3967, Accuracy: 8874/10000 (88.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4187, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.5021, Accuracy: 8671/10000 (86.7%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3603, Accuracy: 8967/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3727, Accuracy: 8950/10000 (89.5%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3896, Accuracy: 8929/10000 (89.3%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4468, Accuracy: 8783/10000 (87.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3787, Accuracy: 8932/10000 (89.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4705, Accuracy: 8683/10000 (86.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4230, Accuracy: 8887/10000 (88.9%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3665, Accuracy: 9003/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4021, Accuracy: 8907/10000 (89.1%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4862, Accuracy: 8708/10000 (87.1%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4154, Accuracy: 8863/10000 (88.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4084, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8891/10000 (88.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3621, Accuracy: 8987/10000 (89.9%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3659, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4027, Accuracy: 8909/10000 (89.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3900, Accuracy: 8921/10000 (89.2%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3858, Accuracy: 8947/10000 (89.5%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3643, Accuracy: 8976/10000 (89.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4419, Accuracy: 8826/10000 (88.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4114, Accuracy: 8874/10000 (88.7%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4034, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3834, Accuracy: 8982/10000 (89.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2856, Accuracy: 9232/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2802, Accuracy: 9239/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2810, Accuracy: 9251/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2845, Accuracy: 9264/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2827, Accuracy: 9288/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2833, Accuracy: 9267/10000 (92.7%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9282/10000 (92.8%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2890, Accuracy: 9276/10000 (92.8%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2859, Accuracy: 9291/10000 (92.9%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2871, Accuracy: 9284/10000 (92.8%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2889, Accuracy: 9280/10000 (92.8%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2938, Accuracy: 9286/10000 (92.9%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2944, Accuracy: 9289/10000 (92.9%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2987, Accuracy: 9282/10000 (92.8%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2972, Accuracy: 9273/10000 (92.7%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2971, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2973, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2965, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2951, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2978, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2975, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2982, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2948, Accuracy: 9280/10000 (92.8%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2970, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2958, Accuracy: 9276/10000 (92.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2955, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2969, Accuracy: 9292/10000 (92.9%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2968, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2986, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2960, Accuracy: 9288/10000 (92.9%), learning rate: 0.0001
Best accuracy: 0.9292 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 58) Prec1: 0.929200
layer index: 172 	 total channel: 64 	 remaining channel: 34
layer index:  43
num_parameters:  533740
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.5001, Accuracy: 8479/10000 (84.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3800, Accuracy: 8817/10000 (88.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4797, Accuracy: 8599/10000 (86.0%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3822, Accuracy: 8836/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4236, Accuracy: 8751/10000 (87.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3730, Accuracy: 8928/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8908/10000 (89.1%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3638, Accuracy: 8931/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4125, Accuracy: 8834/10000 (88.3%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3526, Accuracy: 8969/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4540, Accuracy: 8799/10000 (88.0%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4351, Accuracy: 8858/10000 (88.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3968, Accuracy: 8870/10000 (88.7%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4308, Accuracy: 8842/10000 (88.4%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3674, Accuracy: 8976/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4711, Accuracy: 8702/10000 (87.0%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3722, Accuracy: 8960/10000 (89.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3611, Accuracy: 8975/10000 (89.8%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4123, Accuracy: 8880/10000 (88.8%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4296, Accuracy: 8843/10000 (88.4%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3949, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3972, Accuracy: 8898/10000 (89.0%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4033, Accuracy: 8892/10000 (88.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4236, Accuracy: 8805/10000 (88.1%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4546, Accuracy: 8785/10000 (87.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4053, Accuracy: 8871/10000 (88.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4235, Accuracy: 8855/10000 (88.6%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4074, Accuracy: 8883/10000 (88.8%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4078, Accuracy: 8889/10000 (88.9%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2770, Accuracy: 9209/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2765, Accuracy: 9233/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2805, Accuracy: 9225/10000 (92.2%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2878, Accuracy: 9229/10000 (92.3%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2827, Accuracy: 9234/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2828, Accuracy: 9246/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9241/10000 (92.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2886, Accuracy: 9254/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2887, Accuracy: 9247/10000 (92.5%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2859, Accuracy: 9269/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2906, Accuracy: 9268/10000 (92.7%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2922, Accuracy: 9257/10000 (92.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2900, Accuracy: 9245/10000 (92.4%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2909, Accuracy: 9252/10000 (92.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2947, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2937, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2938, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2928, Accuracy: 9251/10000 (92.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2908, Accuracy: 9251/10000 (92.5%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2946, Accuracy: 9244/10000 (92.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2935, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2961, Accuracy: 9251/10000 (92.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9255/10000 (92.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2942, Accuracy: 9258/10000 (92.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2918, Accuracy: 9250/10000 (92.5%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2923, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2938, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2941, Accuracy: 9266/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2942, Accuracy: 9256/10000 (92.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2932, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001
Best accuracy: 0.9269 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 41) Prec1: 0.926900
layer index: 177 	 total channel: 64 	 remaining channel: 34
layer index:  44
num_parameters:  507220
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3812, Accuracy: 8898/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3769, Accuracy: 8912/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4013, Accuracy: 8846/10000 (88.5%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4304, Accuracy: 8749/10000 (87.5%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3768, Accuracy: 8897/10000 (89.0%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4331, Accuracy: 8764/10000 (87.6%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.3756, Accuracy: 8919/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3890, Accuracy: 8910/10000 (89.1%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3731, Accuracy: 8933/10000 (89.3%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4101, Accuracy: 8870/10000 (88.7%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3930, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4106, Accuracy: 8878/10000 (88.8%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4058, Accuracy: 8901/10000 (89.0%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3939, Accuracy: 8906/10000 (89.1%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4180, Accuracy: 8840/10000 (88.4%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4922, Accuracy: 8686/10000 (86.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4735, Accuracy: 8717/10000 (87.2%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4386, Accuracy: 8779/10000 (87.8%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4039, Accuracy: 8886/10000 (88.9%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3651, Accuracy: 8975/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4147, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3965, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3680, Accuracy: 8996/10000 (90.0%), learning rate: 0.01
is best and save!

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.5217, Accuracy: 8654/10000 (86.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4338, Accuracy: 8818/10000 (88.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3879, Accuracy: 8939/10000 (89.4%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3870, Accuracy: 8899/10000 (89.0%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4998, Accuracy: 8693/10000 (86.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4297, Accuracy: 8829/10000 (88.3%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4583, Accuracy: 8779/10000 (87.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2846, Accuracy: 9189/10000 (91.9%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2818, Accuracy: 9240/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2796, Accuracy: 9245/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2852, Accuracy: 9252/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2835, Accuracy: 9255/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2834, Accuracy: 9274/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9257/10000 (92.6%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2869, Accuracy: 9259/10000 (92.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2883, Accuracy: 9272/10000 (92.7%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2921, Accuracy: 9256/10000 (92.6%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2903, Accuracy: 9269/10000 (92.7%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2919, Accuracy: 9263/10000 (92.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2905, Accuracy: 9281/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2957, Accuracy: 9267/10000 (92.7%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3012, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2999, Accuracy: 9254/10000 (92.5%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2991, Accuracy: 9266/10000 (92.7%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2977, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2945, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2997, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2951, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2985, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2965, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2974, Accuracy: 9261/10000 (92.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2965, Accuracy: 9265/10000 (92.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2958, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2954, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2971, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2981, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2956, Accuracy: 9273/10000 (92.7%), learning rate: 0.0001
Best accuracy: 0.9281 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 44) Prec1: 0.928100
layer index: 180 	 total channel: 64 	 remaining channel: 34
layer index:  45
num_parameters:  480700
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4255, Accuracy: 8654/10000 (86.5%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4487, Accuracy: 8647/10000 (86.5%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4741, Accuracy: 8622/10000 (86.2%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3745, Accuracy: 8843/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3851, Accuracy: 8843/10000 (88.4%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3812, Accuracy: 8887/10000 (88.9%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4278, Accuracy: 8736/10000 (87.4%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4858, Accuracy: 8702/10000 (87.0%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3822, Accuracy: 8900/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4757, Accuracy: 8665/10000 (86.7%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8878/10000 (88.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4247, Accuracy: 8833/10000 (88.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4130, Accuracy: 8831/10000 (88.3%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4358, Accuracy: 8778/10000 (87.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3848, Accuracy: 8881/10000 (88.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3857, Accuracy: 8887/10000 (88.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4652, Accuracy: 8773/10000 (87.7%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3840, Accuracy: 8902/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3999, Accuracy: 8880/10000 (88.8%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4100, Accuracy: 8792/10000 (87.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4277, Accuracy: 8838/10000 (88.4%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3870, Accuracy: 8902/10000 (89.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4484, Accuracy: 8740/10000 (87.4%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3764, Accuracy: 8936/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4138, Accuracy: 8819/10000 (88.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4531, Accuracy: 8778/10000 (87.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3598, Accuracy: 8941/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3829, Accuracy: 8923/10000 (89.2%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3617, Accuracy: 8926/10000 (89.3%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4086, Accuracy: 8881/10000 (88.8%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2868, Accuracy: 9194/10000 (91.9%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2852, Accuracy: 9221/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2909, Accuracy: 9212/10000 (92.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2923, Accuracy: 9224/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2873, Accuracy: 9239/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2910, Accuracy: 9226/10000 (92.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2905, Accuracy: 9257/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2912, Accuracy: 9251/10000 (92.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2969, Accuracy: 9250/10000 (92.5%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2931, Accuracy: 9247/10000 (92.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2972, Accuracy: 9252/10000 (92.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2994, Accuracy: 9254/10000 (92.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2981, Accuracy: 9265/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.3056, Accuracy: 9258/10000 (92.6%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3058, Accuracy: 9247/10000 (92.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.3038, Accuracy: 9255/10000 (92.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.3047, Accuracy: 9257/10000 (92.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.3032, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.3001, Accuracy: 9267/10000 (92.7%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.3052, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.3037, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.3055, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.3026, Accuracy: 9257/10000 (92.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.3024, Accuracy: 9254/10000 (92.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.3022, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.3036, Accuracy: 9257/10000 (92.6%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.3033, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.3045, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.3050, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.3038, Accuracy: 9259/10000 (92.6%), learning rate: 0.0001
Best accuracy: 0.9272 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 52) Prec1: 0.927200
layer index: 185 	 total channel: 64 	 remaining channel: 34
layer index:  46
num_parameters:  454180
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 64, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3739, Accuracy: 8905/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4018, Accuracy: 8826/10000 (88.3%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4836, Accuracy: 8677/10000 (86.8%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4093, Accuracy: 8746/10000 (87.5%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3925, Accuracy: 8852/10000 (88.5%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4022, Accuracy: 8842/10000 (88.4%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.5086, Accuracy: 8593/10000 (85.9%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4191, Accuracy: 8772/10000 (87.7%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4370, Accuracy: 8755/10000 (87.6%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4033, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4901, Accuracy: 8651/10000 (86.5%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4600, Accuracy: 8760/10000 (87.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.5448, Accuracy: 8470/10000 (84.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4477, Accuracy: 8784/10000 (87.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4217, Accuracy: 8844/10000 (88.4%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4514, Accuracy: 8735/10000 (87.3%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4153, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3661, Accuracy: 8973/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4119, Accuracy: 8906/10000 (89.1%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4267, Accuracy: 8829/10000 (88.3%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3773, Accuracy: 8935/10000 (89.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3929, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4140, Accuracy: 8820/10000 (88.2%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8881/10000 (88.8%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4292, Accuracy: 8822/10000 (88.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4348, Accuracy: 8815/10000 (88.2%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3616, Accuracy: 8988/10000 (89.9%), learning rate: 0.01
is best and save!

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4131, Accuracy: 8889/10000 (88.9%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3671, Accuracy: 8957/10000 (89.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3888, Accuracy: 8921/10000 (89.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2895, Accuracy: 9196/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2910, Accuracy: 9211/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2904, Accuracy: 9228/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2882, Accuracy: 9236/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2896, Accuracy: 9247/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2855, Accuracy: 9256/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2922, Accuracy: 9257/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2942, Accuracy: 9244/10000 (92.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2965, Accuracy: 9259/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2973, Accuracy: 9246/10000 (92.5%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2990, Accuracy: 9275/10000 (92.8%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2988, Accuracy: 9262/10000 (92.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2963, Accuracy: 9256/10000 (92.6%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.3022, Accuracy: 9258/10000 (92.6%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.3039, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2998, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.3007, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2995, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2965, Accuracy: 9283/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.3004, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.3004, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.3005, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.3004, Accuracy: 9271/10000 (92.7%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2997, Accuracy: 9262/10000 (92.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.3001, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.3015, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.3008, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.3001, Accuracy: 9276/10000 (92.8%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.3021, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.3006, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001
Best accuracy: 0.9283 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 50) Prec1: 0.928300
layer index: 188 	 total channel: 64 	 remaining channel: 36
layer index:  47
num_parameters:  429428
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 64, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.5345, Accuracy: 8419/10000 (84.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3986, Accuracy: 8734/10000 (87.3%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4026, Accuracy: 8742/10000 (87.4%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.3868, Accuracy: 8833/10000 (88.3%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3496, Accuracy: 8890/10000 (88.9%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3736, Accuracy: 8868/10000 (88.7%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4239, Accuracy: 8738/10000 (87.4%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4161, Accuracy: 8779/10000 (87.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4228, Accuracy: 8774/10000 (87.7%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4464, Accuracy: 8699/10000 (87.0%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3633, Accuracy: 8907/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.5117, Accuracy: 8543/10000 (85.4%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.5841, Accuracy: 8385/10000 (83.8%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8877/10000 (88.8%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8879/10000 (88.8%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8883/10000 (88.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4054, Accuracy: 8835/10000 (88.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4582, Accuracy: 8724/10000 (87.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4228, Accuracy: 8873/10000 (88.7%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4252, Accuracy: 8807/10000 (88.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3672, Accuracy: 8975/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4053, Accuracy: 8830/10000 (88.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4579, Accuracy: 8705/10000 (87.1%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3664, Accuracy: 8985/10000 (89.8%), learning rate: 0.01
is best and save!

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4268, Accuracy: 8818/10000 (88.2%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3915, Accuracy: 8878/10000 (88.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8904/10000 (89.0%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3913, Accuracy: 8907/10000 (89.1%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3864, Accuracy: 8922/10000 (89.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4529, Accuracy: 8716/10000 (87.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2836, Accuracy: 9163/10000 (91.6%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2809, Accuracy: 9197/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2826, Accuracy: 9203/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2848, Accuracy: 9218/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2832, Accuracy: 9232/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2853, Accuracy: 9233/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2835, Accuracy: 9234/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2863, Accuracy: 9232/10000 (92.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2904, Accuracy: 9222/10000 (92.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2926, Accuracy: 9233/10000 (92.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2900, Accuracy: 9249/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2948, Accuracy: 9233/10000 (92.3%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2967, Accuracy: 9227/10000 (92.3%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2980, Accuracy: 9243/10000 (92.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2984, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2942, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2955, Accuracy: 9256/10000 (92.6%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2943, Accuracy: 9254/10000 (92.5%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2920, Accuracy: 9252/10000 (92.5%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2951, Accuracy: 9240/10000 (92.4%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2944, Accuracy: 9240/10000 (92.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2952, Accuracy: 9259/10000 (92.6%), learning rate: 0.0001
is best and save!

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2937, Accuracy: 9248/10000 (92.5%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2933, Accuracy: 9247/10000 (92.5%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2938, Accuracy: 9261/10000 (92.6%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2950, Accuracy: 9247/10000 (92.5%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2942, Accuracy: 9248/10000 (92.5%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2963, Accuracy: 9244/10000 (92.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2950, Accuracy: 9251/10000 (92.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2950, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001
Best accuracy: 0.9261 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 56) Prec1: 0.926100
layer index: 193 	 total channel: 64 	 remaining channel: 36
layer index:  48
num_parameters:  404172
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 64, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3815, Accuracy: 8840/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3885, Accuracy: 8853/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3615, Accuracy: 8909/10000 (89.1%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4567, Accuracy: 8640/10000 (86.4%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3695, Accuracy: 8946/10000 (89.5%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3726, Accuracy: 8900/10000 (89.0%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4137, Accuracy: 8790/10000 (87.9%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4854, Accuracy: 8692/10000 (86.9%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4028, Accuracy: 8852/10000 (88.5%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4178, Accuracy: 8814/10000 (88.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3813, Accuracy: 8896/10000 (89.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4243, Accuracy: 8826/10000 (88.3%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4314, Accuracy: 8788/10000 (87.9%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4274, Accuracy: 8803/10000 (88.0%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8914/10000 (89.1%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3777, Accuracy: 8946/10000 (89.5%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4427, Accuracy: 8764/10000 (87.6%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3915, Accuracy: 8859/10000 (88.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3696, Accuracy: 8963/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3545, Accuracy: 9017/10000 (90.2%), learning rate: 0.01
is best and save!

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3801, Accuracy: 8903/10000 (89.0%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3939, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4297, Accuracy: 8796/10000 (88.0%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3669, Accuracy: 8913/10000 (89.1%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4066, Accuracy: 8873/10000 (88.7%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4021, Accuracy: 8869/10000 (88.7%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4781, Accuracy: 8674/10000 (86.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4394, Accuracy: 8819/10000 (88.2%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4046, Accuracy: 8860/10000 (88.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3725, Accuracy: 8938/10000 (89.4%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2849, Accuracy: 9197/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2797, Accuracy: 9224/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2779, Accuracy: 9248/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2771, Accuracy: 9255/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2762, Accuracy: 9253/10000 (92.5%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2781, Accuracy: 9247/10000 (92.5%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2797, Accuracy: 9254/10000 (92.5%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2840, Accuracy: 9255/10000 (92.6%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2882, Accuracy: 9249/10000 (92.5%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2823, Accuracy: 9262/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2877, Accuracy: 9252/10000 (92.5%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2882, Accuracy: 9261/10000 (92.6%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2885, Accuracy: 9253/10000 (92.5%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2941, Accuracy: 9247/10000 (92.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2947, Accuracy: 9250/10000 (92.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2915, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2937, Accuracy: 9265/10000 (92.7%), learning rate: 0.0001
is best and save!

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2922, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2898, Accuracy: 9266/10000 (92.7%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2920, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2915, Accuracy: 9259/10000 (92.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2937, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2923, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2908, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2916, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2923, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2927, Accuracy: 9267/10000 (92.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2925, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2945, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9266/10000 (92.7%), learning rate: 0.0001
Best accuracy: 0.927 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 49) Prec1: 0.927000
layer index: 196 	 total channel: 64 	 remaining channel: 36
layer index:  49
num_parameters:  378916
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 64, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4922, Accuracy: 8425/10000 (84.2%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4499, Accuracy: 8611/10000 (86.1%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.3643, Accuracy: 8820/10000 (88.2%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4105, Accuracy: 8679/10000 (86.8%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4165, Accuracy: 8706/10000 (87.1%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3649, Accuracy: 8840/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.6093, Accuracy: 8252/10000 (82.5%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4526, Accuracy: 8653/10000 (86.5%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4383, Accuracy: 8709/10000 (87.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4268, Accuracy: 8741/10000 (87.4%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4311, Accuracy: 8732/10000 (87.3%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4057, Accuracy: 8792/10000 (87.9%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4002, Accuracy: 8788/10000 (87.9%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.3899, Accuracy: 8842/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3849, Accuracy: 8831/10000 (88.3%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4622, Accuracy: 8689/10000 (86.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4433, Accuracy: 8711/10000 (87.1%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3994, Accuracy: 8807/10000 (88.1%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4089, Accuracy: 8824/10000 (88.2%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4031, Accuracy: 8817/10000 (88.2%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3811, Accuracy: 8916/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3985, Accuracy: 8816/10000 (88.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4467, Accuracy: 8765/10000 (87.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.5545, Accuracy: 8499/10000 (85.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4184, Accuracy: 8810/10000 (88.1%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4181, Accuracy: 8795/10000 (87.9%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4366, Accuracy: 8707/10000 (87.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3770, Accuracy: 8916/10000 (89.2%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4637, Accuracy: 8672/10000 (86.7%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4583, Accuracy: 8719/10000 (87.2%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2687, Accuracy: 9196/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2636, Accuracy: 9229/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2687, Accuracy: 9235/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2683, Accuracy: 9246/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2702, Accuracy: 9255/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9252/10000 (92.5%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2720, Accuracy: 9251/10000 (92.5%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2739, Accuracy: 9251/10000 (92.5%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2784, Accuracy: 9253/10000 (92.5%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2755, Accuracy: 9245/10000 (92.4%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2800, Accuracy: 9264/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9249/10000 (92.5%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2819, Accuracy: 9265/10000 (92.7%), learning rate: 0.001
is best and save!

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2849, Accuracy: 9251/10000 (92.5%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2868, Accuracy: 9257/10000 (92.6%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2804, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2822, Accuracy: 9262/10000 (92.6%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2806, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2792, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2809, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2831, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2833, Accuracy: 9265/10000 (92.7%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2831, Accuracy: 9264/10000 (92.6%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2820, Accuracy: 9271/10000 (92.7%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2831, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2827, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2841, Accuracy: 9265/10000 (92.7%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2853, Accuracy: 9273/10000 (92.7%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2845, Accuracy: 9258/10000 (92.6%), learning rate: 0.0001
Best accuracy: 0.9281 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 56) Prec1: 0.928100
layer index: 201 	 total channel: 64 	 remaining channel: 36
layer index:  50
num_parameters:  353660
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 64, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4064, Accuracy: 8739/10000 (87.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4347, Accuracy: 8730/10000 (87.3%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4453, Accuracy: 8636/10000 (86.4%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4031, Accuracy: 8818/10000 (88.2%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4187, Accuracy: 8769/10000 (87.7%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4005, Accuracy: 8783/10000 (87.8%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4530, Accuracy: 8645/10000 (86.4%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3827, Accuracy: 8844/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4069, Accuracy: 8822/10000 (88.2%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4788, Accuracy: 8635/10000 (86.3%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4121, Accuracy: 8802/10000 (88.0%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3868, Accuracy: 8874/10000 (88.7%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.5072, Accuracy: 8556/10000 (85.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4834, Accuracy: 8640/10000 (86.4%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4043, Accuracy: 8799/10000 (88.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4717, Accuracy: 8692/10000 (86.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4125, Accuracy: 8829/10000 (88.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4234, Accuracy: 8760/10000 (87.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3656, Accuracy: 8898/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3709, Accuracy: 8893/10000 (88.9%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4318, Accuracy: 8767/10000 (87.7%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4038, Accuracy: 8843/10000 (88.4%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4274, Accuracy: 8779/10000 (87.8%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4088, Accuracy: 8844/10000 (88.4%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4723, Accuracy: 8706/10000 (87.1%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3839, Accuracy: 8878/10000 (88.8%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4038, Accuracy: 8833/10000 (88.3%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3997, Accuracy: 8900/10000 (89.0%), learning rate: 0.01
is best and save!

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3779, Accuracy: 8855/10000 (88.6%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4164, Accuracy: 8808/10000 (88.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2728, Accuracy: 9198/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2657, Accuracy: 9231/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2727, Accuracy: 9231/10000 (92.3%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2715, Accuracy: 9251/10000 (92.5%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2741, Accuracy: 9235/10000 (92.3%), learning rate: 0.001

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2747, Accuracy: 9230/10000 (92.3%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2821, Accuracy: 9236/10000 (92.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2834, Accuracy: 9241/10000 (92.4%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2813, Accuracy: 9261/10000 (92.6%), learning rate: 0.001
is best and save!

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2807, Accuracy: 9242/10000 (92.4%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2833, Accuracy: 9240/10000 (92.4%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2896, Accuracy: 9245/10000 (92.4%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2893, Accuracy: 9249/10000 (92.5%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2872, Accuracy: 9237/10000 (92.4%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2931, Accuracy: 9246/10000 (92.5%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2863, Accuracy: 9239/10000 (92.4%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2886, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2879, Accuracy: 9244/10000 (92.4%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2857, Accuracy: 9254/10000 (92.5%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2883, Accuracy: 9254/10000 (92.5%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2879, Accuracy: 9243/10000 (92.4%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2886, Accuracy: 9245/10000 (92.4%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2887, Accuracy: 9245/10000 (92.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2887, Accuracy: 9241/10000 (92.4%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2895, Accuracy: 9241/10000 (92.4%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2886, Accuracy: 9243/10000 (92.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2899, Accuracy: 9240/10000 (92.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2911, Accuracy: 9246/10000 (92.5%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2909, Accuracy: 9246/10000 (92.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2914, Accuracy: 9252/10000 (92.5%), learning rate: 0.0001
Best accuracy: 0.9261 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 40) Prec1: 0.926100
layer index: 204 	 total channel: 64 	 remaining channel: 36
layer index:  51
num_parameters:  328404
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 64, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.5224, Accuracy: 8412/10000 (84.1%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3629, Accuracy: 8836/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4313, Accuracy: 8690/10000 (86.9%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4151, Accuracy: 8700/10000 (87.0%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.5249, Accuracy: 8421/10000 (84.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.4169, Accuracy: 8718/10000 (87.2%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4930, Accuracy: 8585/10000 (85.8%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.5835, Accuracy: 8359/10000 (83.6%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.3758, Accuracy: 8843/10000 (88.4%), learning rate: 0.01
is best and save!

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4423, Accuracy: 8671/10000 (86.7%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3544, Accuracy: 8919/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4256, Accuracy: 8737/10000 (87.4%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3882, Accuracy: 8774/10000 (87.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4728, Accuracy: 8626/10000 (86.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3938, Accuracy: 8795/10000 (87.9%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4009, Accuracy: 8793/10000 (87.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3700, Accuracy: 8866/10000 (88.7%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.3794, Accuracy: 8818/10000 (88.2%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3817, Accuracy: 8847/10000 (88.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8863/10000 (88.6%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4028, Accuracy: 8810/10000 (88.1%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3903, Accuracy: 8821/10000 (88.2%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3753, Accuracy: 8870/10000 (88.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3752, Accuracy: 8891/10000 (88.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3751, Accuracy: 8894/10000 (88.9%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3976, Accuracy: 8811/10000 (88.1%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4119, Accuracy: 8814/10000 (88.1%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4135, Accuracy: 8829/10000 (88.3%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3837, Accuracy: 8846/10000 (88.5%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4704, Accuracy: 8664/10000 (86.6%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2733, Accuracy: 9179/10000 (91.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2692, Accuracy: 9208/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2704, Accuracy: 9216/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2753, Accuracy: 9215/10000 (92.2%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2736, Accuracy: 9231/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2733, Accuracy: 9236/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2756, Accuracy: 9228/10000 (92.3%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2808, Accuracy: 9231/10000 (92.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2864, Accuracy: 9223/10000 (92.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2844, Accuracy: 9226/10000 (92.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2880, Accuracy: 9217/10000 (92.2%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2938, Accuracy: 9209/10000 (92.1%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2932, Accuracy: 9232/10000 (92.3%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2936, Accuracy: 9215/10000 (92.2%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2953, Accuracy: 9221/10000 (92.2%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2912, Accuracy: 9228/10000 (92.3%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2936, Accuracy: 9223/10000 (92.2%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2927, Accuracy: 9226/10000 (92.3%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2909, Accuracy: 9240/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2940, Accuracy: 9241/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2933, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2955, Accuracy: 9232/10000 (92.3%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2938, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2952, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2958, Accuracy: 9239/10000 (92.4%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2949, Accuracy: 9234/10000 (92.3%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2948, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2957, Accuracy: 9244/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2964, Accuracy: 9237/10000 (92.4%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2967, Accuracy: 9237/10000 (92.4%), learning rate: 0.0001
Best accuracy: 0.9244 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 59) Prec1: 0.924400
layer index: 209 	 total channel: 64 	 remaining channel: 36
layer index:  52
num_parameters:  303148
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 64, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4482, Accuracy: 8663/10000 (86.6%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.3977, Accuracy: 8816/10000 (88.2%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4515, Accuracy: 8669/10000 (86.7%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4248, Accuracy: 8685/10000 (86.8%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4348, Accuracy: 8720/10000 (87.2%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3772, Accuracy: 8835/10000 (88.3%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4017, Accuracy: 8849/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.5213, Accuracy: 8545/10000 (85.4%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4732, Accuracy: 8613/10000 (86.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.3793, Accuracy: 8865/10000 (88.7%), learning rate: 0.01
is best and save!

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.3741, Accuracy: 8866/10000 (88.7%), learning rate: 0.01
is best and save!

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4074, Accuracy: 8771/10000 (87.7%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.3887, Accuracy: 8825/10000 (88.2%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4993, Accuracy: 8626/10000 (86.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.3876, Accuracy: 8798/10000 (88.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4552, Accuracy: 8679/10000 (86.8%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.3733, Accuracy: 8887/10000 (88.9%), learning rate: 0.01
is best and save!

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4134, Accuracy: 8774/10000 (87.7%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4163, Accuracy: 8770/10000 (87.7%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4121, Accuracy: 8813/10000 (88.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3916, Accuracy: 8832/10000 (88.3%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3868, Accuracy: 8827/10000 (88.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3910, Accuracy: 8839/10000 (88.4%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4570, Accuracy: 8694/10000 (86.9%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.3885, Accuracy: 8874/10000 (88.7%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4007, Accuracy: 8847/10000 (88.5%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3868, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3841, Accuracy: 8938/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.3589, Accuracy: 8920/10000 (89.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3927, Accuracy: 8848/10000 (88.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2743, Accuracy: 9175/10000 (91.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2703, Accuracy: 9203/10000 (92.0%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2744, Accuracy: 9206/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2762, Accuracy: 9189/10000 (91.9%), learning rate: 0.001

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2751, Accuracy: 9211/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2760, Accuracy: 9194/10000 (91.9%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2766, Accuracy: 9206/10000 (92.1%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2777, Accuracy: 9213/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2856, Accuracy: 9199/10000 (92.0%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2868, Accuracy: 9205/10000 (92.1%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2865, Accuracy: 9201/10000 (92.0%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2910, Accuracy: 9216/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2924, Accuracy: 9207/10000 (92.1%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2954, Accuracy: 9224/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2993, Accuracy: 9211/10000 (92.1%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2946, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2929, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2932, Accuracy: 9226/10000 (92.3%), learning rate: 0.0001
is best and save!

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2910, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001
is best and save!

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2942, Accuracy: 9221/10000 (92.2%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2926, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2948, Accuracy: 9221/10000 (92.2%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2939, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2936, Accuracy: 9214/10000 (92.1%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2940, Accuracy: 9232/10000 (92.3%), learning rate: 0.0001
is best and save!

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2935, Accuracy: 9219/10000 (92.2%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2930, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2946, Accuracy: 9222/10000 (92.2%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2944, Accuracy: 9222/10000 (92.2%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2936, Accuracy: 9238/10000 (92.4%), learning rate: 0.0001
is best and save!
Best accuracy: 0.9238 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 61) Prec1: 0.923800
layer index: 212 	 total channel: 64 	 remaining channel: 36
layer index:  53
num_parameters:  277892
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 64, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4664, Accuracy: 8643/10000 (86.4%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4061, Accuracy: 8783/10000 (87.8%), learning rate: 0.01
is best and save!

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4649, Accuracy: 8618/10000 (86.2%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4630, Accuracy: 8656/10000 (86.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.3984, Accuracy: 8808/10000 (88.1%), learning rate: 0.01
is best and save!

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3945, Accuracy: 8790/10000 (87.9%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4397, Accuracy: 8739/10000 (87.4%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.4995, Accuracy: 8575/10000 (85.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4646, Accuracy: 8708/10000 (87.1%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4610, Accuracy: 8673/10000 (86.7%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4239, Accuracy: 8776/10000 (87.8%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3874, Accuracy: 8849/10000 (88.5%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4270, Accuracy: 8732/10000 (87.3%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.4800, Accuracy: 8669/10000 (86.7%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4217, Accuracy: 8764/10000 (87.6%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.3705, Accuracy: 8922/10000 (89.2%), learning rate: 0.01
is best and save!

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4606, Accuracy: 8734/10000 (87.3%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4056, Accuracy: 8834/10000 (88.3%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4337, Accuracy: 8767/10000 (87.7%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4104, Accuracy: 8810/10000 (88.1%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.4047, Accuracy: 8809/10000 (88.1%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3799, Accuracy: 8887/10000 (88.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3856, Accuracy: 8867/10000 (88.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.3969, Accuracy: 8830/10000 (88.3%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4780, Accuracy: 8702/10000 (87.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.4748, Accuracy: 8702/10000 (87.0%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3821, Accuracy: 8870/10000 (88.7%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.4371, Accuracy: 8764/10000 (87.6%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4271, Accuracy: 8769/10000 (87.7%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4696, Accuracy: 8707/10000 (87.1%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2843, Accuracy: 9155/10000 (91.6%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2774, Accuracy: 9207/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2794, Accuracy: 9205/10000 (92.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2802, Accuracy: 9208/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2780, Accuracy: 9235/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2773, Accuracy: 9242/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2805, Accuracy: 9240/10000 (92.4%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2822, Accuracy: 9233/10000 (92.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2928, Accuracy: 9206/10000 (92.1%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2853, Accuracy: 9227/10000 (92.3%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2885, Accuracy: 9228/10000 (92.3%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2955, Accuracy: 9225/10000 (92.2%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2932, Accuracy: 9224/10000 (92.2%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2918, Accuracy: 9235/10000 (92.3%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2941, Accuracy: 9223/10000 (92.2%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2897, Accuracy: 9225/10000 (92.2%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2913, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2908, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2886, Accuracy: 9239/10000 (92.4%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2919, Accuracy: 9219/10000 (92.2%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2900, Accuracy: 9232/10000 (92.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2938, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2910, Accuracy: 9236/10000 (92.4%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2923, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2933, Accuracy: 9234/10000 (92.3%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9234/10000 (92.3%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2913, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2919, Accuracy: 9249/10000 (92.5%), learning rate: 0.0001
is best and save!

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2933, Accuracy: 9246/10000 (92.5%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9243/10000 (92.4%), learning rate: 0.0001
Best accuracy: 0.9249 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 59) Prec1: 0.924900
layer index: 217 	 total channel: 64 	 remaining channel: 36
layer index:  54
num_parameters:  252636
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 36, 64]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.4579, Accuracy: 8653/10000 (86.5%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.5302, Accuracy: 8536/10000 (85.4%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4616, Accuracy: 8695/10000 (86.9%), learning rate: 0.01
is best and save!

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4230, Accuracy: 8779/10000 (87.8%), learning rate: 0.01
is best and save!

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.5017, Accuracy: 8576/10000 (85.8%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3460, Accuracy: 8961/10000 (89.6%), learning rate: 0.01
is best and save!

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4388, Accuracy: 8706/10000 (87.1%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3715, Accuracy: 8877/10000 (88.8%), learning rate: 0.01

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4260, Accuracy: 8782/10000 (87.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4084, Accuracy: 8793/10000 (87.9%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4503, Accuracy: 8712/10000 (87.1%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.4989, Accuracy: 8559/10000 (85.6%), learning rate: 0.01

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.4720, Accuracy: 8659/10000 (86.6%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.5358, Accuracy: 8534/10000 (85.3%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4019, Accuracy: 8854/10000 (88.5%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4179, Accuracy: 8790/10000 (87.9%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4171, Accuracy: 8810/10000 (88.1%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4077, Accuracy: 8763/10000 (87.6%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.3454, Accuracy: 8972/10000 (89.7%), learning rate: 0.01
is best and save!

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4032, Accuracy: 8870/10000 (88.7%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3811, Accuracy: 8853/10000 (88.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.4243, Accuracy: 8794/10000 (87.9%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.4960, Accuracy: 8672/10000 (86.7%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.4945, Accuracy: 8649/10000 (86.5%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4194, Accuracy: 8803/10000 (88.0%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3602, Accuracy: 8939/10000 (89.4%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.4811, Accuracy: 8624/10000 (86.2%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3992, Accuracy: 8850/10000 (88.5%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4177, Accuracy: 8789/10000 (87.9%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.4271, Accuracy: 8757/10000 (87.6%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2712, Accuracy: 9179/10000 (91.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2701, Accuracy: 9211/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2727, Accuracy: 9211/10000 (92.1%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2729, Accuracy: 9217/10000 (92.2%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2731, Accuracy: 9227/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2724, Accuracy: 9216/10000 (92.2%), learning rate: 0.001

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2815, Accuracy: 9193/10000 (91.9%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2780, Accuracy: 9219/10000 (92.2%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2792, Accuracy: 9224/10000 (92.2%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2839, Accuracy: 9202/10000 (92.0%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2855, Accuracy: 9216/10000 (92.2%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2883, Accuracy: 9213/10000 (92.1%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2872, Accuracy: 9202/10000 (92.0%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2893, Accuracy: 9213/10000 (92.1%), learning rate: 0.001

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2966, Accuracy: 9208/10000 (92.1%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2909, Accuracy: 9218/10000 (92.2%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2917, Accuracy: 9215/10000 (92.2%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2909, Accuracy: 9218/10000 (92.2%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2878, Accuracy: 9216/10000 (92.2%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2892, Accuracy: 9200/10000 (92.0%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2889, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001
is best and save!

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2913, Accuracy: 9209/10000 (92.1%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2891, Accuracy: 9211/10000 (92.1%), learning rate: 0.0001

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2898, Accuracy: 9214/10000 (92.1%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2901, Accuracy: 9213/10000 (92.1%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2888, Accuracy: 9225/10000 (92.2%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2901, Accuracy: 9220/10000 (92.2%), learning rate: 0.0001

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2913, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2924, Accuracy: 9212/10000 (92.1%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2915, Accuracy: 9213/10000 (92.1%), learning rate: 0.0001
Best accuracy: 0.9231 

=> loaded checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar' (epoch 52) Prec1: 0.923100
layer index: 220 	 total channel: 64 	 remaining channel: 36
layer index:  55
num_parameters:  243228
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 36, 36]
=> save model at'/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1'
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(9, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): ResBasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (1): ResBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (2): ResBasicBlock(
        (conv1): Conv2d(32, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (3): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (4): ResBasicBlock(
        (conv1): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(34, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): LambdaLayer()
      )
      (5): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (6): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (7): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
      (8): ResBasicBlock(
        (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (fc): Linear(in_features=36, out_features=10, bias=True)
  )
)

epoch:  1 learning rate:  0.01
Test set: Average loss: 0.3857, Accuracy: 8879/10000 (88.8%), learning rate: 0.01
is best and save!

epoch:  2 learning rate:  0.01
Test set: Average loss: 0.4002, Accuracy: 8804/10000 (88.0%), learning rate: 0.01

epoch:  3 learning rate:  0.01
Test set: Average loss: 0.4596, Accuracy: 8707/10000 (87.1%), learning rate: 0.01

epoch:  4 learning rate:  0.01
Test set: Average loss: 0.4701, Accuracy: 8655/10000 (86.6%), learning rate: 0.01

epoch:  5 learning rate:  0.01
Test set: Average loss: 0.4293, Accuracy: 8741/10000 (87.4%), learning rate: 0.01

epoch:  6 learning rate:  0.01
Test set: Average loss: 0.3924, Accuracy: 8864/10000 (88.6%), learning rate: 0.01

epoch:  7 learning rate:  0.01
Test set: Average loss: 0.4033, Accuracy: 8796/10000 (88.0%), learning rate: 0.01

epoch:  8 learning rate:  0.01
Test set: Average loss: 0.3826, Accuracy: 8892/10000 (88.9%), learning rate: 0.01
is best and save!

epoch:  9 learning rate:  0.01
Test set: Average loss: 0.4216, Accuracy: 8775/10000 (87.8%), learning rate: 0.01

epoch:  10 learning rate:  0.01
Test set: Average loss: 0.4026, Accuracy: 8814/10000 (88.1%), learning rate: 0.01

epoch:  11 learning rate:  0.01
Test set: Average loss: 0.4469, Accuracy: 8715/10000 (87.2%), learning rate: 0.01

epoch:  12 learning rate:  0.01
Test set: Average loss: 0.3676, Accuracy: 8941/10000 (89.4%), learning rate: 0.01
is best and save!

epoch:  13 learning rate:  0.01
Test set: Average loss: 0.5285, Accuracy: 8474/10000 (84.7%), learning rate: 0.01

epoch:  14 learning rate:  0.01
Test set: Average loss: 0.5524, Accuracy: 8469/10000 (84.7%), learning rate: 0.01

epoch:  15 learning rate:  0.01
Test set: Average loss: 0.4525, Accuracy: 8699/10000 (87.0%), learning rate: 0.01

epoch:  16 learning rate:  0.01
Test set: Average loss: 0.4233, Accuracy: 8813/10000 (88.1%), learning rate: 0.01

epoch:  17 learning rate:  0.01
Test set: Average loss: 0.4373, Accuracy: 8782/10000 (87.8%), learning rate: 0.01

epoch:  18 learning rate:  0.01
Test set: Average loss: 0.4290, Accuracy: 8830/10000 (88.3%), learning rate: 0.01

epoch:  19 learning rate:  0.01
Test set: Average loss: 0.4016, Accuracy: 8849/10000 (88.5%), learning rate: 0.01

epoch:  20 learning rate:  0.01
Test set: Average loss: 0.4020, Accuracy: 8851/10000 (88.5%), learning rate: 0.01

epoch:  21 learning rate:  0.01
Test set: Average loss: 0.3867, Accuracy: 8851/10000 (88.5%), learning rate: 0.01

epoch:  22 learning rate:  0.01
Test set: Average loss: 0.3883, Accuracy: 8829/10000 (88.3%), learning rate: 0.01

epoch:  23 learning rate:  0.01
Test set: Average loss: 0.3750, Accuracy: 8931/10000 (89.3%), learning rate: 0.01

epoch:  24 learning rate:  0.01
Test set: Average loss: 0.5932, Accuracy: 8402/10000 (84.0%), learning rate: 0.01

epoch:  25 learning rate:  0.01
Test set: Average loss: 0.4280, Accuracy: 8764/10000 (87.6%), learning rate: 0.01

epoch:  26 learning rate:  0.01
Test set: Average loss: 0.3922, Accuracy: 8864/10000 (88.6%), learning rate: 0.01

epoch:  27 learning rate:  0.01
Test set: Average loss: 0.3729, Accuracy: 8880/10000 (88.8%), learning rate: 0.01

epoch:  28 learning rate:  0.01
Test set: Average loss: 0.3942, Accuracy: 8859/10000 (88.6%), learning rate: 0.01

epoch:  29 learning rate:  0.01
Test set: Average loss: 0.4042, Accuracy: 8823/10000 (88.2%), learning rate: 0.01

epoch:  30 learning rate:  0.01
Test set: Average loss: 0.3994, Accuracy: 8854/10000 (88.5%), learning rate: 0.001

epoch:  31 learning rate:  0.001
Test set: Average loss: 0.2723, Accuracy: 9180/10000 (91.8%), learning rate: 0.001
is best and save!

epoch:  32 learning rate:  0.001
Test set: Average loss: 0.2700, Accuracy: 9206/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  33 learning rate:  0.001
Test set: Average loss: 0.2738, Accuracy: 9199/10000 (92.0%), learning rate: 0.001

epoch:  34 learning rate:  0.001
Test set: Average loss: 0.2736, Accuracy: 9207/10000 (92.1%), learning rate: 0.001
is best and save!

epoch:  35 learning rate:  0.001
Test set: Average loss: 0.2747, Accuracy: 9228/10000 (92.3%), learning rate: 0.001
is best and save!

epoch:  36 learning rate:  0.001
Test set: Average loss: 0.2729, Accuracy: 9238/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  37 learning rate:  0.001
Test set: Average loss: 0.2763, Accuracy: 9234/10000 (92.3%), learning rate: 0.001

epoch:  38 learning rate:  0.001
Test set: Average loss: 0.2766, Accuracy: 9231/10000 (92.3%), learning rate: 0.001

epoch:  39 learning rate:  0.001
Test set: Average loss: 0.2841, Accuracy: 9228/10000 (92.3%), learning rate: 0.001

epoch:  40 learning rate:  0.001
Test set: Average loss: 0.2812, Accuracy: 9222/10000 (92.2%), learning rate: 0.001

epoch:  41 learning rate:  0.001
Test set: Average loss: 0.2861, Accuracy: 9213/10000 (92.1%), learning rate: 0.001

epoch:  42 learning rate:  0.001
Test set: Average loss: 0.2890, Accuracy: 9214/10000 (92.1%), learning rate: 0.001

epoch:  43 learning rate:  0.001
Test set: Average loss: 0.2887, Accuracy: 9230/10000 (92.3%), learning rate: 0.001

epoch:  44 learning rate:  0.001
Test set: Average loss: 0.2862, Accuracy: 9241/10000 (92.4%), learning rate: 0.001
is best and save!

epoch:  45 learning rate:  0.001
Test set: Average loss: 0.2850, Accuracy: 9224/10000 (92.2%), learning rate: 0.0001

epoch:  46 learning rate:  0.0001
Test set: Average loss: 0.2821, Accuracy: 9229/10000 (92.3%), learning rate: 0.0001

epoch:  47 learning rate:  0.0001
Test set: Average loss: 0.2835, Accuracy: 9240/10000 (92.4%), learning rate: 0.0001

epoch:  48 learning rate:  0.0001
Test set: Average loss: 0.2836, Accuracy: 9229/10000 (92.3%), learning rate: 0.0001

epoch:  49 learning rate:  0.0001
Test set: Average loss: 0.2825, Accuracy: 9233/10000 (92.3%), learning rate: 0.0001

epoch:  50 learning rate:  0.0001
Test set: Average loss: 0.2842, Accuracy: 9221/10000 (92.2%), learning rate: 0.0001

epoch:  51 learning rate:  0.0001
Test set: Average loss: 0.2836, Accuracy: 9229/10000 (92.3%), learning rate: 0.0001

epoch:  52 learning rate:  0.0001
Test set: Average loss: 0.2843, Accuracy: 9231/10000 (92.3%), learning rate: 0.0001

epoch:  53 learning rate:  0.0001
Test set: Average loss: 0.2833, Accuracy: 9243/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  54 learning rate:  0.0001
Test set: Average loss: 0.2834, Accuracy: 9225/10000 (92.2%), learning rate: 0.0001

epoch:  55 learning rate:  0.0001
Test set: Average loss: 0.2866, Accuracy: 9235/10000 (92.3%), learning rate: 0.0001

epoch:  56 learning rate:  0.0001
Test set: Average loss: 0.2837, Accuracy: 9242/10000 (92.4%), learning rate: 0.0001

epoch:  57 learning rate:  0.0001
Test set: Average loss: 0.2849, Accuracy: 9245/10000 (92.4%), learning rate: 0.0001
is best and save!

epoch:  58 learning rate:  0.0001
Test set: Average loss: 0.2857, Accuracy: 9237/10000 (92.4%), learning rate: 0.0001

epoch:  59 learning rate:  0.0001
Test set: Average loss: 0.2862, Accuracy: 9223/10000 (92.2%), learning rate: 0.0001

epoch:  60 learning rate:  0.0001
Test set: Average loss: 0.2879, Accuracy: 9226/10000 (92.3%), learning rate: 0.0001
Best accuracy: 0.9245 

=> test, loading student model checkpoint '/home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar'
[16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 18, 16, 32, 32, 32, 32, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 36, 36]
Test set: Average loss: 0.2849, Accuracy: 9245/10000 (92.4%)
test model accurancy:  0.9245 path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/model_best.pth.tar
###################  Finetune  ###################

Train Epoch: 0 [0/50000 (0.0%)]	Loss: 0.018165	

Train Epoch: 0 [19200/50000 (38.4%)]	Loss: 0.072717	

Train Epoch: 0 [38400/50000 (76.7%)]	Loss: 0.112810	
Test set: Average loss: 0.4586, Accuracy: 8725/10000 (87.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 1 [0/50000 (0.0%)]	Loss: 0.142875	

Train Epoch: 1 [19200/50000 (38.4%)]	Loss: 0.220554	

Train Epoch: 1 [38400/50000 (76.7%)]	Loss: 0.138785	
Test set: Average loss: 0.4492, Accuracy: 8754/10000 (87.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 2 [0/50000 (0.0%)]	Loss: 0.096997	

Train Epoch: 2 [19200/50000 (38.4%)]	Loss: 0.116389	

Train Epoch: 2 [38400/50000 (76.7%)]	Loss: 0.113977	
Test set: Average loss: 0.4181, Accuracy: 8791/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 3 [0/50000 (0.0%)]	Loss: 0.204150	

Train Epoch: 3 [19200/50000 (38.4%)]	Loss: 0.111657	

Train Epoch: 3 [38400/50000 (76.7%)]	Loss: 0.138572	
Test set: Average loss: 0.4146, Accuracy: 8808/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 4 [0/50000 (0.0%)]	Loss: 0.110344	

Train Epoch: 4 [19200/50000 (38.4%)]	Loss: 0.103466	

Train Epoch: 4 [38400/50000 (76.7%)]	Loss: 0.226466	
Test set: Average loss: 0.4038, Accuracy: 8814/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 5 [0/50000 (0.0%)]	Loss: 0.130032	

Train Epoch: 5 [19200/50000 (38.4%)]	Loss: 0.227264	

Train Epoch: 5 [38400/50000 (76.7%)]	Loss: 0.145983	
Test set: Average loss: 0.3936, Accuracy: 8810/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 6 [0/50000 (0.0%)]	Loss: 0.166491	

Train Epoch: 6 [19200/50000 (38.4%)]	Loss: 0.199643	

Train Epoch: 6 [38400/50000 (76.7%)]	Loss: 0.170934	
Test set: Average loss: 0.3945, Accuracy: 8878/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 7 [0/50000 (0.0%)]	Loss: 0.101426	

Train Epoch: 7 [19200/50000 (38.4%)]	Loss: 0.151551	

Train Epoch: 7 [38400/50000 (76.7%)]	Loss: 0.284115	
Test set: Average loss: 0.3788, Accuracy: 8852/10000 (88.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 8 [0/50000 (0.0%)]	Loss: 0.089262	

Train Epoch: 8 [19200/50000 (38.4%)]	Loss: 0.109452	

Train Epoch: 8 [38400/50000 (76.7%)]	Loss: 0.203788	
Test set: Average loss: 0.4125, Accuracy: 8768/10000 (87.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 9 [0/50000 (0.0%)]	Loss: 0.061011	

Train Epoch: 9 [19200/50000 (38.4%)]	Loss: 0.247383	

Train Epoch: 9 [38400/50000 (76.7%)]	Loss: 0.154180	
Test set: Average loss: 0.4261, Accuracy: 8808/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 10 [0/50000 (0.0%)]	Loss: 0.088133	

Train Epoch: 10 [19200/50000 (38.4%)]	Loss: 0.197009	

Train Epoch: 10 [38400/50000 (76.7%)]	Loss: 0.195697	
Test set: Average loss: 0.4324, Accuracy: 8762/10000 (87.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 11 [0/50000 (0.0%)]	Loss: 0.235366	

Train Epoch: 11 [19200/50000 (38.4%)]	Loss: 0.131002	

Train Epoch: 11 [38400/50000 (76.7%)]	Loss: 0.149807	
Test set: Average loss: 0.3864, Accuracy: 8857/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 12 [0/50000 (0.0%)]	Loss: 0.097041	

Train Epoch: 12 [19200/50000 (38.4%)]	Loss: 0.151071	

Train Epoch: 12 [38400/50000 (76.7%)]	Loss: 0.156506	
Test set: Average loss: 0.3552, Accuracy: 8957/10000 (89.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 13 [0/50000 (0.0%)]	Loss: 0.089603	

Train Epoch: 13 [19200/50000 (38.4%)]	Loss: 0.186141	

Train Epoch: 13 [38400/50000 (76.7%)]	Loss: 0.161907	
Test set: Average loss: 0.3647, Accuracy: 8904/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 14 [0/50000 (0.0%)]	Loss: 0.224343	

Train Epoch: 14 [19200/50000 (38.4%)]	Loss: 0.223834	

Train Epoch: 14 [38400/50000 (76.7%)]	Loss: 0.208293	
Test set: Average loss: 0.3595, Accuracy: 8942/10000 (89.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 15 [0/50000 (0.0%)]	Loss: 0.088062	

Train Epoch: 15 [19200/50000 (38.4%)]	Loss: 0.140685	

Train Epoch: 15 [38400/50000 (76.7%)]	Loss: 0.172048	
Test set: Average loss: 0.7020, Accuracy: 8111/10000 (81.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 16 [0/50000 (0.0%)]	Loss: 0.173134	

Train Epoch: 16 [19200/50000 (38.4%)]	Loss: 0.077554	

Train Epoch: 16 [38400/50000 (76.7%)]	Loss: 0.130011	
Test set: Average loss: 0.3793, Accuracy: 8867/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 17 [0/50000 (0.0%)]	Loss: 0.189511	

Train Epoch: 17 [19200/50000 (38.4%)]	Loss: 0.202751	

Train Epoch: 17 [38400/50000 (76.7%)]	Loss: 0.251780	
Test set: Average loss: 0.4021, Accuracy: 8833/10000 (88.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 18 [0/50000 (0.0%)]	Loss: 0.096708	

Train Epoch: 18 [19200/50000 (38.4%)]	Loss: 0.062907	

Train Epoch: 18 [38400/50000 (76.7%)]	Loss: 0.208674	
Test set: Average loss: 0.3759, Accuracy: 8900/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 19 [0/50000 (0.0%)]	Loss: 0.182898	

Train Epoch: 19 [19200/50000 (38.4%)]	Loss: 0.140417	

Train Epoch: 19 [38400/50000 (76.7%)]	Loss: 0.117863	
Test set: Average loss: 0.3864, Accuracy: 8862/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 20 [0/50000 (0.0%)]	Loss: 0.177772	

Train Epoch: 20 [19200/50000 (38.4%)]	Loss: 0.159802	

Train Epoch: 20 [38400/50000 (76.7%)]	Loss: 0.254457	
Test set: Average loss: 0.3961, Accuracy: 8871/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 21 [0/50000 (0.0%)]	Loss: 0.141254	

Train Epoch: 21 [19200/50000 (38.4%)]	Loss: 0.195498	

Train Epoch: 21 [38400/50000 (76.7%)]	Loss: 0.201277	
Test set: Average loss: 0.4624, Accuracy: 8734/10000 (87.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 22 [0/50000 (0.0%)]	Loss: 0.206181	

Train Epoch: 22 [19200/50000 (38.4%)]	Loss: 0.088184	

Train Epoch: 22 [38400/50000 (76.7%)]	Loss: 0.121163	
Test set: Average loss: 0.5791, Accuracy: 8414/10000 (84.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 23 [0/50000 (0.0%)]	Loss: 0.148711	

Train Epoch: 23 [19200/50000 (38.4%)]	Loss: 0.083923	

Train Epoch: 23 [38400/50000 (76.7%)]	Loss: 0.258579	
Test set: Average loss: 0.4659, Accuracy: 8692/10000 (86.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 24 [0/50000 (0.0%)]	Loss: 0.054934	

Train Epoch: 24 [19200/50000 (38.4%)]	Loss: 0.175179	

Train Epoch: 24 [38400/50000 (76.7%)]	Loss: 0.155940	
Test set: Average loss: 0.3597, Accuracy: 8947/10000 (89.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 25 [0/50000 (0.0%)]	Loss: 0.097608	

Train Epoch: 25 [19200/50000 (38.4%)]	Loss: 0.102517	

Train Epoch: 25 [38400/50000 (76.7%)]	Loss: 0.133297	
Test set: Average loss: 0.4383, Accuracy: 8730/10000 (87.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 26 [0/50000 (0.0%)]	Loss: 0.220262	

Train Epoch: 26 [19200/50000 (38.4%)]	Loss: 0.162158	

Train Epoch: 26 [38400/50000 (76.7%)]	Loss: 0.143794	
Test set: Average loss: 0.3784, Accuracy: 8885/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 27 [0/50000 (0.0%)]	Loss: 0.116933	

Train Epoch: 27 [19200/50000 (38.4%)]	Loss: 0.069177	

Train Epoch: 27 [38400/50000 (76.7%)]	Loss: 0.130612	
Test set: Average loss: 0.4201, Accuracy: 8779/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 28 [0/50000 (0.0%)]	Loss: 0.172594	

Train Epoch: 28 [19200/50000 (38.4%)]	Loss: 0.087383	

Train Epoch: 28 [38400/50000 (76.7%)]	Loss: 0.159286	
Test set: Average loss: 0.4276, Accuracy: 8818/10000 (88.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 29 [0/50000 (0.0%)]	Loss: 0.129579	

Train Epoch: 29 [19200/50000 (38.4%)]	Loss: 0.104147	

Train Epoch: 29 [38400/50000 (76.7%)]	Loss: 0.132180	
Test set: Average loss: 0.4097, Accuracy: 8803/10000 (88.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 30 [0/50000 (0.0%)]	Loss: 0.170422	

Train Epoch: 30 [19200/50000 (38.4%)]	Loss: 0.084688	

Train Epoch: 30 [38400/50000 (76.7%)]	Loss: 0.128668	
Test set: Average loss: 0.3927, Accuracy: 8856/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 31 [0/50000 (0.0%)]	Loss: 0.122022	

Train Epoch: 31 [19200/50000 (38.4%)]	Loss: 0.175434	

Train Epoch: 31 [38400/50000 (76.7%)]	Loss: 0.185542	
Test set: Average loss: 0.4021, Accuracy: 8850/10000 (88.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 32 [0/50000 (0.0%)]	Loss: 0.116172	

Train Epoch: 32 [19200/50000 (38.4%)]	Loss: 0.160752	

Train Epoch: 32 [38400/50000 (76.7%)]	Loss: 0.112814	
Test set: Average loss: 0.4551, Accuracy: 8710/10000 (87.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 33 [0/50000 (0.0%)]	Loss: 0.056078	

Train Epoch: 33 [19200/50000 (38.4%)]	Loss: 0.096335	

Train Epoch: 33 [38400/50000 (76.7%)]	Loss: 0.115726	
Test set: Average loss: 0.3818, Accuracy: 8905/10000 (89.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 34 [0/50000 (0.0%)]	Loss: 0.093482	

Train Epoch: 34 [19200/50000 (38.4%)]	Loss: 0.067088	

Train Epoch: 34 [38400/50000 (76.7%)]	Loss: 0.184421	
Test set: Average loss: 0.4426, Accuracy: 8764/10000 (87.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 35 [0/50000 (0.0%)]	Loss: 0.080824	

Train Epoch: 35 [19200/50000 (38.4%)]	Loss: 0.079831	

Train Epoch: 35 [38400/50000 (76.7%)]	Loss: 0.209177	
Test set: Average loss: 0.3742, Accuracy: 8882/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 36 [0/50000 (0.0%)]	Loss: 0.139429	

Train Epoch: 36 [19200/50000 (38.4%)]	Loss: 0.157516	

Train Epoch: 36 [38400/50000 (76.7%)]	Loss: 0.146953	
Test set: Average loss: 0.3856, Accuracy: 8902/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 37 [0/50000 (0.0%)]	Loss: 0.116885	

Train Epoch: 37 [19200/50000 (38.4%)]	Loss: 0.130928	

Train Epoch: 37 [38400/50000 (76.7%)]	Loss: 0.094989	
Test set: Average loss: 0.4543, Accuracy: 8736/10000 (87.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 38 [0/50000 (0.0%)]	Loss: 0.181864	

Train Epoch: 38 [19200/50000 (38.4%)]	Loss: 0.252540	

Train Epoch: 38 [38400/50000 (76.7%)]	Loss: 0.091963	
Test set: Average loss: 0.3970, Accuracy: 8861/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 39 [0/50000 (0.0%)]	Loss: 0.109079	

Train Epoch: 39 [19200/50000 (38.4%)]	Loss: 0.154773	

Train Epoch: 39 [38400/50000 (76.7%)]	Loss: 0.169830	
Test set: Average loss: 0.3688, Accuracy: 8882/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 40 [0/50000 (0.0%)]	Loss: 0.123989	

Train Epoch: 40 [19200/50000 (38.4%)]	Loss: 0.128880	

Train Epoch: 40 [38400/50000 (76.7%)]	Loss: 0.093683	
Test set: Average loss: 0.4376, Accuracy: 8721/10000 (87.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 41 [0/50000 (0.0%)]	Loss: 0.133778	

Train Epoch: 41 [19200/50000 (38.4%)]	Loss: 0.134807	

Train Epoch: 41 [38400/50000 (76.7%)]	Loss: 0.097477	
Test set: Average loss: 0.4769, Accuracy: 8620/10000 (86.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 42 [0/50000 (0.0%)]	Loss: 0.093488	

Train Epoch: 42 [19200/50000 (38.4%)]	Loss: 0.113964	

Train Epoch: 42 [38400/50000 (76.7%)]	Loss: 0.121381	
Test set: Average loss: 0.4196, Accuracy: 8810/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 43 [0/50000 (0.0%)]	Loss: 0.135407	

Train Epoch: 43 [19200/50000 (38.4%)]	Loss: 0.084752	

Train Epoch: 43 [38400/50000 (76.7%)]	Loss: 0.112753	
Test set: Average loss: 0.3994, Accuracy: 8805/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 44 [0/50000 (0.0%)]	Loss: 0.089915	

Train Epoch: 44 [19200/50000 (38.4%)]	Loss: 0.139680	

Train Epoch: 44 [38400/50000 (76.7%)]	Loss: 0.084365	
Test set: Average loss: 0.4727, Accuracy: 8656/10000 (86.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 45 [0/50000 (0.0%)]	Loss: 0.104477	

Train Epoch: 45 [19200/50000 (38.4%)]	Loss: 0.110764	

Train Epoch: 45 [38400/50000 (76.7%)]	Loss: 0.100190	
Test set: Average loss: 0.3682, Accuracy: 8894/10000 (88.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 46 [0/50000 (0.0%)]	Loss: 0.068702	

Train Epoch: 46 [19200/50000 (38.4%)]	Loss: 0.156757	

Train Epoch: 46 [38400/50000 (76.7%)]	Loss: 0.080363	
Test set: Average loss: 0.4852, Accuracy: 8655/10000 (86.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 47 [0/50000 (0.0%)]	Loss: 0.121965	

Train Epoch: 47 [19200/50000 (38.4%)]	Loss: 0.124945	

Train Epoch: 47 [38400/50000 (76.7%)]	Loss: 0.112540	
Test set: Average loss: 0.3715, Accuracy: 8881/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 48 [0/50000 (0.0%)]	Loss: 0.173795	

Train Epoch: 48 [19200/50000 (38.4%)]	Loss: 0.107754	

Train Epoch: 48 [38400/50000 (76.7%)]	Loss: 0.098591	
Test set: Average loss: 0.3778, Accuracy: 8908/10000 (89.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 49 [0/50000 (0.0%)]	Loss: 0.148695	

Train Epoch: 49 [19200/50000 (38.4%)]	Loss: 0.065185	

Train Epoch: 49 [38400/50000 (76.7%)]	Loss: 0.215847	
Test set: Average loss: 0.3783, Accuracy: 8889/10000 (88.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 50 [0/50000 (0.0%)]	Loss: 0.123554	

Train Epoch: 50 [19200/50000 (38.4%)]	Loss: 0.049708	

Train Epoch: 50 [38400/50000 (76.7%)]	Loss: 0.165712	
Test set: Average loss: 0.3852, Accuracy: 8903/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 51 [0/50000 (0.0%)]	Loss: 0.199316	

Train Epoch: 51 [19200/50000 (38.4%)]	Loss: 0.181661	

Train Epoch: 51 [38400/50000 (76.7%)]	Loss: 0.116722	
Test set: Average loss: 0.3824, Accuracy: 8844/10000 (88.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 52 [0/50000 (0.0%)]	Loss: 0.129642	

Train Epoch: 52 [19200/50000 (38.4%)]	Loss: 0.093781	

Train Epoch: 52 [38400/50000 (76.7%)]	Loss: 0.173022	
Test set: Average loss: 0.4256, Accuracy: 8789/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 53 [0/50000 (0.0%)]	Loss: 0.257043	

Train Epoch: 53 [19200/50000 (38.4%)]	Loss: 0.226440	

Train Epoch: 53 [38400/50000 (76.7%)]	Loss: 0.156314	
Test set: Average loss: 0.4126, Accuracy: 8823/10000 (88.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 54 [0/50000 (0.0%)]	Loss: 0.120055	

Train Epoch: 54 [19200/50000 (38.4%)]	Loss: 0.160555	

Train Epoch: 54 [38400/50000 (76.7%)]	Loss: 0.218710	
Test set: Average loss: 0.4327, Accuracy: 8808/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 55 [0/50000 (0.0%)]	Loss: 0.182760	

Train Epoch: 55 [19200/50000 (38.4%)]	Loss: 0.188396	

Train Epoch: 55 [38400/50000 (76.7%)]	Loss: 0.088171	
Test set: Average loss: 0.4433, Accuracy: 8719/10000 (87.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 56 [0/50000 (0.0%)]	Loss: 0.123619	

Train Epoch: 56 [19200/50000 (38.4%)]	Loss: 0.122911	

Train Epoch: 56 [38400/50000 (76.7%)]	Loss: 0.092134	
Test set: Average loss: 0.3815, Accuracy: 8868/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 57 [0/50000 (0.0%)]	Loss: 0.077842	

Train Epoch: 57 [19200/50000 (38.4%)]	Loss: 0.176883	

Train Epoch: 57 [38400/50000 (76.7%)]	Loss: 0.117580	
Test set: Average loss: 0.3985, Accuracy: 8857/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 58 [0/50000 (0.0%)]	Loss: 0.157229	

Train Epoch: 58 [19200/50000 (38.4%)]	Loss: 0.178625	

Train Epoch: 58 [38400/50000 (76.7%)]	Loss: 0.192580	
Test set: Average loss: 0.5086, Accuracy: 8582/10000 (85.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 59 [0/50000 (0.0%)]	Loss: 0.177548	

Train Epoch: 59 [19200/50000 (38.4%)]	Loss: 0.094367	

Train Epoch: 59 [38400/50000 (76.7%)]	Loss: 0.156010	
Test set: Average loss: 0.3865, Accuracy: 8889/10000 (88.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 60 [0/50000 (0.0%)]	Loss: 0.157141	

Train Epoch: 60 [19200/50000 (38.4%)]	Loss: 0.166519	

Train Epoch: 60 [38400/50000 (76.7%)]	Loss: 0.145017	
Test set: Average loss: 0.3943, Accuracy: 8827/10000 (88.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 61 [0/50000 (0.0%)]	Loss: 0.124486	

Train Epoch: 61 [19200/50000 (38.4%)]	Loss: 0.098633	

Train Epoch: 61 [38400/50000 (76.7%)]	Loss: 0.174278	
Test set: Average loss: 0.4511, Accuracy: 8713/10000 (87.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 62 [0/50000 (0.0%)]	Loss: 0.230450	

Train Epoch: 62 [19200/50000 (38.4%)]	Loss: 0.160445	

Train Epoch: 62 [38400/50000 (76.7%)]	Loss: 0.120636	
Test set: Average loss: 0.4552, Accuracy: 8680/10000 (86.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 63 [0/50000 (0.0%)]	Loss: 0.043920	

Train Epoch: 63 [19200/50000 (38.4%)]	Loss: 0.068759	

Train Epoch: 63 [38400/50000 (76.7%)]	Loss: 0.158061	
Test set: Average loss: 0.3776, Accuracy: 8877/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 64 [0/50000 (0.0%)]	Loss: 0.101778	

Train Epoch: 64 [19200/50000 (38.4%)]	Loss: 0.126899	

Train Epoch: 64 [38400/50000 (76.7%)]	Loss: 0.173457	
Test set: Average loss: 0.3741, Accuracy: 8930/10000 (89.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 65 [0/50000 (0.0%)]	Loss: 0.140851	

Train Epoch: 65 [19200/50000 (38.4%)]	Loss: 0.133397	

Train Epoch: 65 [38400/50000 (76.7%)]	Loss: 0.107892	
Test set: Average loss: 0.3703, Accuracy: 8931/10000 (89.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 66 [0/50000 (0.0%)]	Loss: 0.196928	

Train Epoch: 66 [19200/50000 (38.4%)]	Loss: 0.081209	

Train Epoch: 66 [38400/50000 (76.7%)]	Loss: 0.087025	
Test set: Average loss: 0.3809, Accuracy: 8898/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 67 [0/50000 (0.0%)]	Loss: 0.161899	

Train Epoch: 67 [19200/50000 (38.4%)]	Loss: 0.121050	

Train Epoch: 67 [38400/50000 (76.7%)]	Loss: 0.114652	
Test set: Average loss: 0.3558, Accuracy: 8915/10000 (89.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 68 [0/50000 (0.0%)]	Loss: 0.152221	

Train Epoch: 68 [19200/50000 (38.4%)]	Loss: 0.096239	

Train Epoch: 68 [38400/50000 (76.7%)]	Loss: 0.119698	
Test set: Average loss: 0.3924, Accuracy: 8844/10000 (88.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 69 [0/50000 (0.0%)]	Loss: 0.089809	

Train Epoch: 69 [19200/50000 (38.4%)]	Loss: 0.130412	

Train Epoch: 69 [38400/50000 (76.7%)]	Loss: 0.180016	
Test set: Average loss: 0.5001, Accuracy: 8568/10000 (85.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 70 [0/50000 (0.0%)]	Loss: 0.079592	

Train Epoch: 70 [19200/50000 (38.4%)]	Loss: 0.105827	

Train Epoch: 70 [38400/50000 (76.7%)]	Loss: 0.066336	
Test set: Average loss: 0.3899, Accuracy: 8859/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 71 [0/50000 (0.0%)]	Loss: 0.159834	

Train Epoch: 71 [19200/50000 (38.4%)]	Loss: 0.082311	

Train Epoch: 71 [38400/50000 (76.7%)]	Loss: 0.167796	
Test set: Average loss: 0.4132, Accuracy: 8811/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 72 [0/50000 (0.0%)]	Loss: 0.062638	

Train Epoch: 72 [19200/50000 (38.4%)]	Loss: 0.059906	

Train Epoch: 72 [38400/50000 (76.7%)]	Loss: 0.166472	
Test set: Average loss: 0.5606, Accuracy: 8444/10000 (84.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 73 [0/50000 (0.0%)]	Loss: 0.164996	

Train Epoch: 73 [19200/50000 (38.4%)]	Loss: 0.164046	

Train Epoch: 73 [38400/50000 (76.7%)]	Loss: 0.075657	
Test set: Average loss: 0.4166, Accuracy: 8785/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 74 [0/50000 (0.0%)]	Loss: 0.191578	

Train Epoch: 74 [19200/50000 (38.4%)]	Loss: 0.162957	

Train Epoch: 74 [38400/50000 (76.7%)]	Loss: 0.142492	
Test set: Average loss: 0.4016, Accuracy: 8871/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 75 [0/50000 (0.0%)]	Loss: 0.124923	

Train Epoch: 75 [19200/50000 (38.4%)]	Loss: 0.129983	

Train Epoch: 75 [38400/50000 (76.7%)]	Loss: 0.170497	
Test set: Average loss: 0.3953, Accuracy: 8866/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 76 [0/50000 (0.0%)]	Loss: 0.177597	

Train Epoch: 76 [19200/50000 (38.4%)]	Loss: 0.060790	

Train Epoch: 76 [38400/50000 (76.7%)]	Loss: 0.074570	
Test set: Average loss: 0.4434, Accuracy: 8742/10000 (87.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 77 [0/50000 (0.0%)]	Loss: 0.166971	

Train Epoch: 77 [19200/50000 (38.4%)]	Loss: 0.088653	

Train Epoch: 77 [38400/50000 (76.7%)]	Loss: 0.170470	
Test set: Average loss: 0.3942, Accuracy: 8860/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 78 [0/50000 (0.0%)]	Loss: 0.150333	

Train Epoch: 78 [19200/50000 (38.4%)]	Loss: 0.192417	

Train Epoch: 78 [38400/50000 (76.7%)]	Loss: 0.161686	
Test set: Average loss: 0.4028, Accuracy: 8823/10000 (88.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 79 [0/50000 (0.0%)]	Loss: 0.109602	

Train Epoch: 79 [19200/50000 (38.4%)]	Loss: 0.229939	

Train Epoch: 79 [38400/50000 (76.7%)]	Loss: 0.070134	
Test set: Average loss: 0.5823, Accuracy: 8428/10000 (84.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 80 [0/50000 (0.0%)]	Loss: 0.178611	

Train Epoch: 80 [19200/50000 (38.4%)]	Loss: 0.170989	

Train Epoch: 80 [38400/50000 (76.7%)]	Loss: 0.171120	
Test set: Average loss: 0.4795, Accuracy: 8676/10000 (86.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 81 [0/50000 (0.0%)]	Loss: 0.140883	

Train Epoch: 81 [19200/50000 (38.4%)]	Loss: 0.120724	

Train Epoch: 81 [38400/50000 (76.7%)]	Loss: 0.132996	
Test set: Average loss: 0.4137, Accuracy: 8859/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 82 [0/50000 (0.0%)]	Loss: 0.089915	

Train Epoch: 82 [19200/50000 (38.4%)]	Loss: 0.165406	

Train Epoch: 82 [38400/50000 (76.7%)]	Loss: 0.201573	
Test set: Average loss: 0.4207, Accuracy: 8782/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 83 [0/50000 (0.0%)]	Loss: 0.190540	

Train Epoch: 83 [19200/50000 (38.4%)]	Loss: 0.140718	

Train Epoch: 83 [38400/50000 (76.7%)]	Loss: 0.125675	
Test set: Average loss: 0.4235, Accuracy: 8774/10000 (87.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 84 [0/50000 (0.0%)]	Loss: 0.139666	

Train Epoch: 84 [19200/50000 (38.4%)]	Loss: 0.119393	

Train Epoch: 84 [38400/50000 (76.7%)]	Loss: 0.165508	
Test set: Average loss: 0.4115, Accuracy: 8854/10000 (88.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 85 [0/50000 (0.0%)]	Loss: 0.099853	

Train Epoch: 85 [19200/50000 (38.4%)]	Loss: 0.168402	

Train Epoch: 85 [38400/50000 (76.7%)]	Loss: 0.149117	
Test set: Average loss: 0.4002, Accuracy: 8875/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 86 [0/50000 (0.0%)]	Loss: 0.144431	

Train Epoch: 86 [19200/50000 (38.4%)]	Loss: 0.175453	

Train Epoch: 86 [38400/50000 (76.7%)]	Loss: 0.185699	
Test set: Average loss: 0.3797, Accuracy: 8888/10000 (88.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 87 [0/50000 (0.0%)]	Loss: 0.129512	

Train Epoch: 87 [19200/50000 (38.4%)]	Loss: 0.079614	

Train Epoch: 87 [38400/50000 (76.7%)]	Loss: 0.091633	
Test set: Average loss: 0.4035, Accuracy: 8828/10000 (88.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 88 [0/50000 (0.0%)]	Loss: 0.121995	

Train Epoch: 88 [19200/50000 (38.4%)]	Loss: 0.079124	

Train Epoch: 88 [38400/50000 (76.7%)]	Loss: 0.069369	
Test set: Average loss: 0.4839, Accuracy: 8688/10000 (86.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 89 [0/50000 (0.0%)]	Loss: 0.118710	

Train Epoch: 89 [19200/50000 (38.4%)]	Loss: 0.101260	

Train Epoch: 89 [38400/50000 (76.7%)]	Loss: 0.122774	
Test set: Average loss: 0.3629, Accuracy: 8882/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 90 [0/50000 (0.0%)]	Loss: 0.105699	

Train Epoch: 90 [19200/50000 (38.4%)]	Loss: 0.076944	

Train Epoch: 90 [38400/50000 (76.7%)]	Loss: 0.088989	
Test set: Average loss: 0.4001, Accuracy: 8844/10000 (88.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 91 [0/50000 (0.0%)]	Loss: 0.161108	

Train Epoch: 91 [19200/50000 (38.4%)]	Loss: 0.139789	

Train Epoch: 91 [38400/50000 (76.7%)]	Loss: 0.110479	
Test set: Average loss: 0.3804, Accuracy: 8862/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 92 [0/50000 (0.0%)]	Loss: 0.123647	

Train Epoch: 92 [19200/50000 (38.4%)]	Loss: 0.178239	

Train Epoch: 92 [38400/50000 (76.7%)]	Loss: 0.127043	
Test set: Average loss: 0.3730, Accuracy: 8932/10000 (89.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 93 [0/50000 (0.0%)]	Loss: 0.179452	

Train Epoch: 93 [19200/50000 (38.4%)]	Loss: 0.128732	

Train Epoch: 93 [38400/50000 (76.7%)]	Loss: 0.104168	
Test set: Average loss: 0.3556, Accuracy: 8927/10000 (89.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 94 [0/50000 (0.0%)]	Loss: 0.180151	

Train Epoch: 94 [19200/50000 (38.4%)]	Loss: 0.153602	

Train Epoch: 94 [38400/50000 (76.7%)]	Loss: 0.128937	
Test set: Average loss: 0.5232, Accuracy: 8564/10000 (85.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 95 [0/50000 (0.0%)]	Loss: 0.160574	

Train Epoch: 95 [19200/50000 (38.4%)]	Loss: 0.140134	

Train Epoch: 95 [38400/50000 (76.7%)]	Loss: 0.123458	
Test set: Average loss: 0.4475, Accuracy: 8671/10000 (86.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 96 [0/50000 (0.0%)]	Loss: 0.157743	

Train Epoch: 96 [19200/50000 (38.4%)]	Loss: 0.161716	

Train Epoch: 96 [38400/50000 (76.7%)]	Loss: 0.112877	
Test set: Average loss: 0.4598, Accuracy: 8742/10000 (87.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 97 [0/50000 (0.0%)]	Loss: 0.160307	

Train Epoch: 97 [19200/50000 (38.4%)]	Loss: 0.125429	

Train Epoch: 97 [38400/50000 (76.7%)]	Loss: 0.187214	
Test set: Average loss: 0.4367, Accuracy: 8779/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 98 [0/50000 (0.0%)]	Loss: 0.173092	

Train Epoch: 98 [19200/50000 (38.4%)]	Loss: 0.096629	

Train Epoch: 98 [38400/50000 (76.7%)]	Loss: 0.148336	
Test set: Average loss: 0.3453, Accuracy: 9008/10000 (90.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 99 [0/50000 (0.0%)]	Loss: 0.117635	

Train Epoch: 99 [19200/50000 (38.4%)]	Loss: 0.131631	

Train Epoch: 99 [38400/50000 (76.7%)]	Loss: 0.106843	
Test set: Average loss: 0.4329, Accuracy: 8795/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 100 [0/50000 (0.0%)]	Loss: 0.087193	

Train Epoch: 100 [19200/50000 (38.4%)]	Loss: 0.084548	

Train Epoch: 100 [38400/50000 (76.7%)]	Loss: 0.086627	
Test set: Average loss: 0.3628, Accuracy: 8937/10000 (89.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 101 [0/50000 (0.0%)]	Loss: 0.142537	

Train Epoch: 101 [19200/50000 (38.4%)]	Loss: 0.127802	

Train Epoch: 101 [38400/50000 (76.7%)]	Loss: 0.114334	
Test set: Average loss: 0.4170, Accuracy: 8812/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 102 [0/50000 (0.0%)]	Loss: 0.069965	

Train Epoch: 102 [19200/50000 (38.4%)]	Loss: 0.090344	

Train Epoch: 102 [38400/50000 (76.7%)]	Loss: 0.104114	
Test set: Average loss: 0.3506, Accuracy: 8949/10000 (89.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 103 [0/50000 (0.0%)]	Loss: 0.088169	

Train Epoch: 103 [19200/50000 (38.4%)]	Loss: 0.100695	

Train Epoch: 103 [38400/50000 (76.7%)]	Loss: 0.172895	
Test set: Average loss: 0.4202, Accuracy: 8786/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 104 [0/50000 (0.0%)]	Loss: 0.137493	

Train Epoch: 104 [19200/50000 (38.4%)]	Loss: 0.100594	

Train Epoch: 104 [38400/50000 (76.7%)]	Loss: 0.158452	
Test set: Average loss: 0.4290, Accuracy: 8795/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 105 [0/50000 (0.0%)]	Loss: 0.139780	

Train Epoch: 105 [19200/50000 (38.4%)]	Loss: 0.097595	

Train Epoch: 105 [38400/50000 (76.7%)]	Loss: 0.193924	
Test set: Average loss: 0.3954, Accuracy: 8867/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 106 [0/50000 (0.0%)]	Loss: 0.137126	

Train Epoch: 106 [19200/50000 (38.4%)]	Loss: 0.147776	

Train Epoch: 106 [38400/50000 (76.7%)]	Loss: 0.095564	
Test set: Average loss: 0.4337, Accuracy: 8764/10000 (87.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 107 [0/50000 (0.0%)]	Loss: 0.148254	

Train Epoch: 107 [19200/50000 (38.4%)]	Loss: 0.143531	

Train Epoch: 107 [38400/50000 (76.7%)]	Loss: 0.116800	
Test set: Average loss: 0.3946, Accuracy: 8866/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 108 [0/50000 (0.0%)]	Loss: 0.073862	

Train Epoch: 108 [19200/50000 (38.4%)]	Loss: 0.266648	

Train Epoch: 108 [38400/50000 (76.7%)]	Loss: 0.290745	
Test set: Average loss: 0.3641, Accuracy: 8912/10000 (89.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 109 [0/50000 (0.0%)]	Loss: 0.141347	

Train Epoch: 109 [19200/50000 (38.4%)]	Loss: 0.193934	

Train Epoch: 109 [38400/50000 (76.7%)]	Loss: 0.114052	
Test set: Average loss: 0.4960, Accuracy: 8653/10000 (86.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 110 [0/50000 (0.0%)]	Loss: 0.183346	

Train Epoch: 110 [19200/50000 (38.4%)]	Loss: 0.119618	

Train Epoch: 110 [38400/50000 (76.7%)]	Loss: 0.102445	
Test set: Average loss: 0.4103, Accuracy: 8832/10000 (88.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 111 [0/50000 (0.0%)]	Loss: 0.114236	

Train Epoch: 111 [19200/50000 (38.4%)]	Loss: 0.158437	

Train Epoch: 111 [38400/50000 (76.7%)]	Loss: 0.141546	
Test set: Average loss: 0.3942, Accuracy: 8898/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 112 [0/50000 (0.0%)]	Loss: 0.191686	

Train Epoch: 112 [19200/50000 (38.4%)]	Loss: 0.064944	

Train Epoch: 112 [38400/50000 (76.7%)]	Loss: 0.126879	
Test set: Average loss: 0.4008, Accuracy: 8843/10000 (88.4%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 113 [0/50000 (0.0%)]	Loss: 0.161646	

Train Epoch: 113 [19200/50000 (38.4%)]	Loss: 0.175089	

Train Epoch: 113 [38400/50000 (76.7%)]	Loss: 0.121160	
Test set: Average loss: 0.4163, Accuracy: 8799/10000 (88.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 114 [0/50000 (0.0%)]	Loss: 0.133660	

Train Epoch: 114 [19200/50000 (38.4%)]	Loss: 0.131748	

Train Epoch: 114 [38400/50000 (76.7%)]	Loss: 0.107950	
Test set: Average loss: 0.4519, Accuracy: 8752/10000 (87.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 115 [0/50000 (0.0%)]	Loss: 0.235721	

Train Epoch: 115 [19200/50000 (38.4%)]	Loss: 0.145720	

Train Epoch: 115 [38400/50000 (76.7%)]	Loss: 0.060962	
Test set: Average loss: 0.4021, Accuracy: 8813/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 116 [0/50000 (0.0%)]	Loss: 0.213795	

Train Epoch: 116 [19200/50000 (38.4%)]	Loss: 0.093904	

Train Epoch: 116 [38400/50000 (76.7%)]	Loss: 0.101383	
Test set: Average loss: 0.4222, Accuracy: 8768/10000 (87.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 117 [0/50000 (0.0%)]	Loss: 0.105350	

Train Epoch: 117 [19200/50000 (38.4%)]	Loss: 0.202565	

Train Epoch: 117 [38400/50000 (76.7%)]	Loss: 0.148886	
Test set: Average loss: 0.4735, Accuracy: 8687/10000 (86.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 118 [0/50000 (0.0%)]	Loss: 0.123571	

Train Epoch: 118 [19200/50000 (38.4%)]	Loss: 0.086806	

Train Epoch: 118 [38400/50000 (76.7%)]	Loss: 0.102602	
Test set: Average loss: 0.4250, Accuracy: 8749/10000 (87.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 119 [0/50000 (0.0%)]	Loss: 0.205149	

Train Epoch: 119 [19200/50000 (38.4%)]	Loss: 0.105661	

Train Epoch: 119 [38400/50000 (76.7%)]	Loss: 0.216724	
Test set: Average loss: 0.3435, Accuracy: 8966/10000 (89.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 120 [0/50000 (0.0%)]	Loss: 0.119949	

Train Epoch: 120 [19200/50000 (38.4%)]	Loss: 0.205048	

Train Epoch: 120 [38400/50000 (76.7%)]	Loss: 0.213408	
Test set: Average loss: 0.4176, Accuracy: 8812/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 121 [0/50000 (0.0%)]	Loss: 0.184166	

Train Epoch: 121 [19200/50000 (38.4%)]	Loss: 0.149902	

Train Epoch: 121 [38400/50000 (76.7%)]	Loss: 0.112699	
Test set: Average loss: 0.3737, Accuracy: 8888/10000 (88.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 122 [0/50000 (0.0%)]	Loss: 0.154225	

Train Epoch: 122 [19200/50000 (38.4%)]	Loss: 0.155892	

Train Epoch: 122 [38400/50000 (76.7%)]	Loss: 0.200255	
Test set: Average loss: 0.4461, Accuracy: 8698/10000 (87.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 123 [0/50000 (0.0%)]	Loss: 0.077441	

Train Epoch: 123 [19200/50000 (38.4%)]	Loss: 0.126814	

Train Epoch: 123 [38400/50000 (76.7%)]	Loss: 0.127411	
Test set: Average loss: 0.3927, Accuracy: 8857/10000 (88.6%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 124 [0/50000 (0.0%)]	Loss: 0.094977	

Train Epoch: 124 [19200/50000 (38.4%)]	Loss: 0.132207	

Train Epoch: 124 [38400/50000 (76.7%)]	Loss: 0.176401	
Test set: Average loss: 0.3748, Accuracy: 8899/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 125 [0/50000 (0.0%)]	Loss: 0.098013	

Train Epoch: 125 [19200/50000 (38.4%)]	Loss: 0.215694	

Train Epoch: 125 [38400/50000 (76.7%)]	Loss: 0.325778	
Test set: Average loss: 0.3700, Accuracy: 8897/10000 (89.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 126 [0/50000 (0.0%)]	Loss: 0.102120	

Train Epoch: 126 [19200/50000 (38.4%)]	Loss: 0.097401	

Train Epoch: 126 [38400/50000 (76.7%)]	Loss: 0.101048	
Test set: Average loss: 0.3802, Accuracy: 8880/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 127 [0/50000 (0.0%)]	Loss: 0.084666	

Train Epoch: 127 [19200/50000 (38.4%)]	Loss: 0.132649	

Train Epoch: 127 [38400/50000 (76.7%)]	Loss: 0.208948	
Test set: Average loss: 0.5207, Accuracy: 8632/10000 (86.3%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 128 [0/50000 (0.0%)]	Loss: 0.102356	

Train Epoch: 128 [19200/50000 (38.4%)]	Loss: 0.040457	

Train Epoch: 128 [38400/50000 (76.7%)]	Loss: 0.198890	
Test set: Average loss: 0.4523, Accuracy: 8788/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 129 [0/50000 (0.0%)]	Loss: 0.078200	

Train Epoch: 129 [19200/50000 (38.4%)]	Loss: 0.117812	

Train Epoch: 129 [38400/50000 (76.7%)]	Loss: 0.179283	
Test set: Average loss: 0.4127, Accuracy: 8822/10000 (88.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 130 [0/50000 (0.0%)]	Loss: 0.146824	

Train Epoch: 130 [19200/50000 (38.4%)]	Loss: 0.116822	

Train Epoch: 130 [38400/50000 (76.7%)]	Loss: 0.159992	
Test set: Average loss: 0.4417, Accuracy: 8781/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 131 [0/50000 (0.0%)]	Loss: 0.135139	

Train Epoch: 131 [19200/50000 (38.4%)]	Loss: 0.157956	

Train Epoch: 131 [38400/50000 (76.7%)]	Loss: 0.260227	
Test set: Average loss: 0.5075, Accuracy: 8668/10000 (86.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 132 [0/50000 (0.0%)]	Loss: 0.152327	

Train Epoch: 132 [19200/50000 (38.4%)]	Loss: 0.124785	

Train Epoch: 132 [38400/50000 (76.7%)]	Loss: 0.178189	
Test set: Average loss: 0.3802, Accuracy: 8879/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 133 [0/50000 (0.0%)]	Loss: 0.159934	

Train Epoch: 133 [19200/50000 (38.4%)]	Loss: 0.142602	

Train Epoch: 133 [38400/50000 (76.7%)]	Loss: 0.139420	
Test set: Average loss: 0.4095, Accuracy: 8784/10000 (87.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 134 [0/50000 (0.0%)]	Loss: 0.116396	

Train Epoch: 134 [19200/50000 (38.4%)]	Loss: 0.154712	

Train Epoch: 134 [38400/50000 (76.7%)]	Loss: 0.139344	
Test set: Average loss: 0.4890, Accuracy: 8683/10000 (86.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 135 [0/50000 (0.0%)]	Loss: 0.085449	

Train Epoch: 135 [19200/50000 (38.4%)]	Loss: 0.130121	

Train Epoch: 135 [38400/50000 (76.7%)]	Loss: 0.145666	
Test set: Average loss: 0.4717, Accuracy: 8699/10000 (87.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 136 [0/50000 (0.0%)]	Loss: 0.244952	

Train Epoch: 136 [19200/50000 (38.4%)]	Loss: 0.083955	

Train Epoch: 136 [38400/50000 (76.7%)]	Loss: 0.132830	
Test set: Average loss: 0.3819, Accuracy: 8910/10000 (89.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 137 [0/50000 (0.0%)]	Loss: 0.072803	

Train Epoch: 137 [19200/50000 (38.4%)]	Loss: 0.086088	

Train Epoch: 137 [38400/50000 (76.7%)]	Loss: 0.112985	
Test set: Average loss: 0.4068, Accuracy: 8848/10000 (88.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 138 [0/50000 (0.0%)]	Loss: 0.195227	

Train Epoch: 138 [19200/50000 (38.4%)]	Loss: 0.134361	

Train Epoch: 138 [38400/50000 (76.7%)]	Loss: 0.227146	
Test set: Average loss: 0.4790, Accuracy: 8695/10000 (86.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 139 [0/50000 (0.0%)]	Loss: 0.105507	

Train Epoch: 139 [19200/50000 (38.4%)]	Loss: 0.162315	

Train Epoch: 139 [38400/50000 (76.7%)]	Loss: 0.112676	
Test set: Average loss: 0.4676, Accuracy: 8688/10000 (86.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 140 [0/50000 (0.0%)]	Loss: 0.132602	

Train Epoch: 140 [19200/50000 (38.4%)]	Loss: 0.079714	

Train Epoch: 140 [38400/50000 (76.7%)]	Loss: 0.069272	
Test set: Average loss: 0.3844, Accuracy: 8877/10000 (88.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 141 [0/50000 (0.0%)]	Loss: 0.120484	

Train Epoch: 141 [19200/50000 (38.4%)]	Loss: 0.208258	

Train Epoch: 141 [38400/50000 (76.7%)]	Loss: 0.187721	
Test set: Average loss: 0.4181, Accuracy: 8787/10000 (87.9%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 142 [0/50000 (0.0%)]	Loss: 0.130039	

Train Epoch: 142 [19200/50000 (38.4%)]	Loss: 0.054877	

Train Epoch: 142 [38400/50000 (76.7%)]	Loss: 0.138512	
Test set: Average loss: 0.3460, Accuracy: 8980/10000 (89.8%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 143 [0/50000 (0.0%)]	Loss: 0.172135	

Train Epoch: 143 [19200/50000 (38.4%)]	Loss: 0.055540	

Train Epoch: 143 [38400/50000 (76.7%)]	Loss: 0.190529	
Test set: Average loss: 0.3684, Accuracy: 8908/10000 (89.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 144 [0/50000 (0.0%)]	Loss: 0.176021	

Train Epoch: 144 [19200/50000 (38.4%)]	Loss: 0.072313	

Train Epoch: 144 [38400/50000 (76.7%)]	Loss: 0.132268	
Test set: Average loss: 0.4115, Accuracy: 8796/10000 (88.0%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 145 [0/50000 (0.0%)]	Loss: 0.087462	

Train Epoch: 145 [19200/50000 (38.4%)]	Loss: 0.113112	

Train Epoch: 145 [38400/50000 (76.7%)]	Loss: 0.084966	
Test set: Average loss: 0.4015, Accuracy: 8807/10000 (88.1%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 146 [0/50000 (0.0%)]	Loss: 0.118584	

Train Epoch: 146 [19200/50000 (38.4%)]	Loss: 0.104266	

Train Epoch: 146 [38400/50000 (76.7%)]	Loss: 0.165881	
Test set: Average loss: 0.3933, Accuracy: 8870/10000 (88.7%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 147 [0/50000 (0.0%)]	Loss: 0.095930	

Train Epoch: 147 [19200/50000 (38.4%)]	Loss: 0.136307	

Train Epoch: 147 [38400/50000 (76.7%)]	Loss: 0.218672	
Test set: Average loss: 0.3569, Accuracy: 8950/10000 (89.5%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 148 [0/50000 (0.0%)]	Loss: 0.131449	

Train Epoch: 148 [19200/50000 (38.4%)]	Loss: 0.078437	

Train Epoch: 148 [38400/50000 (76.7%)]	Loss: 0.129287	
Test set: Average loss: 0.4197, Accuracy: 8817/10000 (88.2%), learning rate: 0.01
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 149 [0/50000 (0.0%)]	Loss: 0.159448	

Train Epoch: 149 [19200/50000 (38.4%)]	Loss: 0.150854	

Train Epoch: 149 [38400/50000 (76.7%)]	Loss: 0.250539	
Test set: Average loss: 0.5086, Accuracy: 8538/10000 (85.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 150 [0/50000 (0.0%)]	Loss: 0.162873	

Train Epoch: 150 [19200/50000 (38.4%)]	Loss: 0.091102	

Train Epoch: 150 [38400/50000 (76.7%)]	Loss: 0.064710	
Test set: Average loss: 0.2826, Accuracy: 9155/10000 (91.6%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 151 [0/50000 (0.0%)]	Loss: 0.090382	

Train Epoch: 151 [19200/50000 (38.4%)]	Loss: 0.074089	

Train Epoch: 151 [38400/50000 (76.7%)]	Loss: 0.065116	
Test set: Average loss: 0.2768, Accuracy: 9177/10000 (91.8%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 152 [0/50000 (0.0%)]	Loss: 0.032349	

Train Epoch: 152 [19200/50000 (38.4%)]	Loss: 0.064178	

Train Epoch: 152 [38400/50000 (76.7%)]	Loss: 0.044574	
Test set: Average loss: 0.2748, Accuracy: 9200/10000 (92.0%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 153 [0/50000 (0.0%)]	Loss: 0.034652	

Train Epoch: 153 [19200/50000 (38.4%)]	Loss: 0.036842	

Train Epoch: 153 [38400/50000 (76.7%)]	Loss: 0.024275	
Test set: Average loss: 0.2758, Accuracy: 9196/10000 (92.0%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 154 [0/50000 (0.0%)]	Loss: 0.038066	

Train Epoch: 154 [19200/50000 (38.4%)]	Loss: 0.050481	

Train Epoch: 154 [38400/50000 (76.7%)]	Loss: 0.052490	
Test set: Average loss: 0.2804, Accuracy: 9192/10000 (91.9%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 155 [0/50000 (0.0%)]	Loss: 0.033401	

Train Epoch: 155 [19200/50000 (38.4%)]	Loss: 0.010072	

Train Epoch: 155 [38400/50000 (76.7%)]	Loss: 0.034460	
Test set: Average loss: 0.2783, Accuracy: 9206/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 156 [0/50000 (0.0%)]	Loss: 0.014917	

Train Epoch: 156 [19200/50000 (38.4%)]	Loss: 0.024515	

Train Epoch: 156 [38400/50000 (76.7%)]	Loss: 0.038707	
Test set: Average loss: 0.2832, Accuracy: 9209/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 157 [0/50000 (0.0%)]	Loss: 0.011321	

Train Epoch: 157 [19200/50000 (38.4%)]	Loss: 0.028091	

Train Epoch: 157 [38400/50000 (76.7%)]	Loss: 0.009224	
Test set: Average loss: 0.2819, Accuracy: 9222/10000 (92.2%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 158 [0/50000 (0.0%)]	Loss: 0.046555	

Train Epoch: 158 [19200/50000 (38.4%)]	Loss: 0.061290	

Train Epoch: 158 [38400/50000 (76.7%)]	Loss: 0.046420	
Test set: Average loss: 0.2842, Accuracy: 9218/10000 (92.2%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 159 [0/50000 (0.0%)]	Loss: 0.011223	

Train Epoch: 159 [19200/50000 (38.4%)]	Loss: 0.028980	

Train Epoch: 159 [38400/50000 (76.7%)]	Loss: 0.038131	
Test set: Average loss: 0.2873, Accuracy: 9228/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 160 [0/50000 (0.0%)]	Loss: 0.014006	

Train Epoch: 160 [19200/50000 (38.4%)]	Loss: 0.078077	

Train Epoch: 160 [38400/50000 (76.7%)]	Loss: 0.017677	
Test set: Average loss: 0.2895, Accuracy: 9207/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 161 [0/50000 (0.0%)]	Loss: 0.060485	

Train Epoch: 161 [19200/50000 (38.4%)]	Loss: 0.049373	

Train Epoch: 161 [38400/50000 (76.7%)]	Loss: 0.010136	
Test set: Average loss: 0.2915, Accuracy: 9222/10000 (92.2%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 162 [0/50000 (0.0%)]	Loss: 0.031012	

Train Epoch: 162 [19200/50000 (38.4%)]	Loss: 0.018483	

Train Epoch: 162 [38400/50000 (76.7%)]	Loss: 0.020747	
Test set: Average loss: 0.2924, Accuracy: 9208/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 163 [0/50000 (0.0%)]	Loss: 0.028438	

Train Epoch: 163 [19200/50000 (38.4%)]	Loss: 0.044643	

Train Epoch: 163 [38400/50000 (76.7%)]	Loss: 0.031423	
Test set: Average loss: 0.2959, Accuracy: 9205/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 164 [0/50000 (0.0%)]	Loss: 0.007972	

Train Epoch: 164 [19200/50000 (38.4%)]	Loss: 0.006429	

Train Epoch: 164 [38400/50000 (76.7%)]	Loss: 0.039926	
Test set: Average loss: 0.2941, Accuracy: 9214/10000 (92.1%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 165 [0/50000 (0.0%)]	Loss: 0.027550	

Train Epoch: 165 [19200/50000 (38.4%)]	Loss: 0.010002	

Train Epoch: 165 [38400/50000 (76.7%)]	Loss: 0.029400	
Test set: Average loss: 0.2934, Accuracy: 9219/10000 (92.2%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 166 [0/50000 (0.0%)]	Loss: 0.011734	

Train Epoch: 166 [19200/50000 (38.4%)]	Loss: 0.057289	

Train Epoch: 166 [38400/50000 (76.7%)]	Loss: 0.019650	
Test set: Average loss: 0.2922, Accuracy: 9237/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 167 [0/50000 (0.0%)]	Loss: 0.016368	

Train Epoch: 167 [19200/50000 (38.4%)]	Loss: 0.029794	

Train Epoch: 167 [38400/50000 (76.7%)]	Loss: 0.010348	
Test set: Average loss: 0.2970, Accuracy: 9239/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 168 [0/50000 (0.0%)]	Loss: 0.044012	

Train Epoch: 168 [19200/50000 (38.4%)]	Loss: 0.046463	

Train Epoch: 168 [38400/50000 (76.7%)]	Loss: 0.034159	
Test set: Average loss: 0.2953, Accuracy: 9230/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 169 [0/50000 (0.0%)]	Loss: 0.024025	

Train Epoch: 169 [19200/50000 (38.4%)]	Loss: 0.004820	

Train Epoch: 169 [38400/50000 (76.7%)]	Loss: 0.014405	
Test set: Average loss: 0.2983, Accuracy: 9239/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 170 [0/50000 (0.0%)]	Loss: 0.027510	

Train Epoch: 170 [19200/50000 (38.4%)]	Loss: 0.007953	

Train Epoch: 170 [38400/50000 (76.7%)]	Loss: 0.018072	
Test set: Average loss: 0.2992, Accuracy: 9230/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 171 [0/50000 (0.0%)]	Loss: 0.008841	

Train Epoch: 171 [19200/50000 (38.4%)]	Loss: 0.024521	

Train Epoch: 171 [38400/50000 (76.7%)]	Loss: 0.015121	
Test set: Average loss: 0.2980, Accuracy: 9238/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 172 [0/50000 (0.0%)]	Loss: 0.066537	

Train Epoch: 172 [19200/50000 (38.4%)]	Loss: 0.005735	

Train Epoch: 172 [38400/50000 (76.7%)]	Loss: 0.029687	
Test set: Average loss: 0.2981, Accuracy: 9235/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 173 [0/50000 (0.0%)]	Loss: 0.015102	

Train Epoch: 173 [19200/50000 (38.4%)]	Loss: 0.006201	

Train Epoch: 173 [38400/50000 (76.7%)]	Loss: 0.007749	
Test set: Average loss: 0.2962, Accuracy: 9250/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 174 [0/50000 (0.0%)]	Loss: 0.007163	

Train Epoch: 174 [19200/50000 (38.4%)]	Loss: 0.010320	

Train Epoch: 174 [38400/50000 (76.7%)]	Loss: 0.027782	
Test set: Average loss: 0.3019, Accuracy: 9244/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 175 [0/50000 (0.0%)]	Loss: 0.014536	

Train Epoch: 175 [19200/50000 (38.4%)]	Loss: 0.020208	

Train Epoch: 175 [38400/50000 (76.7%)]	Loss: 0.026274	
Test set: Average loss: 0.3020, Accuracy: 9230/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 176 [0/50000 (0.0%)]	Loss: 0.017736	

Train Epoch: 176 [19200/50000 (38.4%)]	Loss: 0.043653	

Train Epoch: 176 [38400/50000 (76.7%)]	Loss: 0.020640	
Test set: Average loss: 0.3039, Accuracy: 9226/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 177 [0/50000 (0.0%)]	Loss: 0.015557	

Train Epoch: 177 [19200/50000 (38.4%)]	Loss: 0.010428	

Train Epoch: 177 [38400/50000 (76.7%)]	Loss: 0.041846	
Test set: Average loss: 0.3047, Accuracy: 9234/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 178 [0/50000 (0.0%)]	Loss: 0.006686	

Train Epoch: 178 [19200/50000 (38.4%)]	Loss: 0.014245	

Train Epoch: 178 [38400/50000 (76.7%)]	Loss: 0.010471	
Test set: Average loss: 0.3014, Accuracy: 9249/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 179 [0/50000 (0.0%)]	Loss: 0.007876	

Train Epoch: 179 [19200/50000 (38.4%)]	Loss: 0.011280	

Train Epoch: 179 [38400/50000 (76.7%)]	Loss: 0.013227	
Test set: Average loss: 0.3050, Accuracy: 9237/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 180 [0/50000 (0.0%)]	Loss: 0.013127	

Train Epoch: 180 [19200/50000 (38.4%)]	Loss: 0.016476	

Train Epoch: 180 [38400/50000 (76.7%)]	Loss: 0.026911	
Test set: Average loss: 0.3034, Accuracy: 9247/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 181 [0/50000 (0.0%)]	Loss: 0.014731	

Train Epoch: 181 [19200/50000 (38.4%)]	Loss: 0.010886	

Train Epoch: 181 [38400/50000 (76.7%)]	Loss: 0.009440	
Test set: Average loss: 0.3045, Accuracy: 9240/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 182 [0/50000 (0.0%)]	Loss: 0.049348	

Train Epoch: 182 [19200/50000 (38.4%)]	Loss: 0.013475	

Train Epoch: 182 [38400/50000 (76.7%)]	Loss: 0.004216	
Test set: Average loss: 0.3077, Accuracy: 9241/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 183 [0/50000 (0.0%)]	Loss: 0.005912	

Train Epoch: 183 [19200/50000 (38.4%)]	Loss: 0.019085	

Train Epoch: 183 [38400/50000 (76.7%)]	Loss: 0.014600	
Test set: Average loss: 0.3128, Accuracy: 9238/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 184 [0/50000 (0.0%)]	Loss: 0.012014	

Train Epoch: 184 [19200/50000 (38.4%)]	Loss: 0.013709	

Train Epoch: 184 [38400/50000 (76.7%)]	Loss: 0.010388	
Test set: Average loss: 0.3105, Accuracy: 9241/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 185 [0/50000 (0.0%)]	Loss: 0.011017	

Train Epoch: 185 [19200/50000 (38.4%)]	Loss: 0.032368	

Train Epoch: 185 [38400/50000 (76.7%)]	Loss: 0.012888	
Test set: Average loss: 0.3046, Accuracy: 9253/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 186 [0/50000 (0.0%)]	Loss: 0.009490	

Train Epoch: 186 [19200/50000 (38.4%)]	Loss: 0.007314	

Train Epoch: 186 [38400/50000 (76.7%)]	Loss: 0.006111	
Test set: Average loss: 0.3119, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 187 [0/50000 (0.0%)]	Loss: 0.010781	

Train Epoch: 187 [19200/50000 (38.4%)]	Loss: 0.016723	

Train Epoch: 187 [38400/50000 (76.7%)]	Loss: 0.011785	
Test set: Average loss: 0.3118, Accuracy: 9238/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 188 [0/50000 (0.0%)]	Loss: 0.028172	

Train Epoch: 188 [19200/50000 (38.4%)]	Loss: 0.006573	

Train Epoch: 188 [38400/50000 (76.7%)]	Loss: 0.009548	
Test set: Average loss: 0.3143, Accuracy: 9242/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 189 [0/50000 (0.0%)]	Loss: 0.009783	

Train Epoch: 189 [19200/50000 (38.4%)]	Loss: 0.008863	

Train Epoch: 189 [38400/50000 (76.7%)]	Loss: 0.017908	
Test set: Average loss: 0.3171, Accuracy: 9242/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 190 [0/50000 (0.0%)]	Loss: 0.032282	

Train Epoch: 190 [19200/50000 (38.4%)]	Loss: 0.003657	

Train Epoch: 190 [38400/50000 (76.7%)]	Loss: 0.005594	
Test set: Average loss: 0.3161, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 191 [0/50000 (0.0%)]	Loss: 0.031083	

Train Epoch: 191 [19200/50000 (38.4%)]	Loss: 0.002832	

Train Epoch: 191 [38400/50000 (76.7%)]	Loss: 0.006173	
Test set: Average loss: 0.3146, Accuracy: 9250/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 192 [0/50000 (0.0%)]	Loss: 0.009390	

Train Epoch: 192 [19200/50000 (38.4%)]	Loss: 0.003657	

Train Epoch: 192 [38400/50000 (76.7%)]	Loss: 0.009420	
Test set: Average loss: 0.3140, Accuracy: 9235/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 193 [0/50000 (0.0%)]	Loss: 0.006376	

Train Epoch: 193 [19200/50000 (38.4%)]	Loss: 0.005281	

Train Epoch: 193 [38400/50000 (76.7%)]	Loss: 0.017814	
Test set: Average loss: 0.3194, Accuracy: 9234/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 194 [0/50000 (0.0%)]	Loss: 0.014478	

Train Epoch: 194 [19200/50000 (38.4%)]	Loss: 0.030454	

Train Epoch: 194 [38400/50000 (76.7%)]	Loss: 0.010468	
Test set: Average loss: 0.3155, Accuracy: 9250/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 195 [0/50000 (0.0%)]	Loss: 0.016376	

Train Epoch: 195 [19200/50000 (38.4%)]	Loss: 0.015800	

Train Epoch: 195 [38400/50000 (76.7%)]	Loss: 0.004954	
Test set: Average loss: 0.3114, Accuracy: 9259/10000 (92.6%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 196 [0/50000 (0.0%)]	Loss: 0.038032	

Train Epoch: 196 [19200/50000 (38.4%)]	Loss: 0.016176	

Train Epoch: 196 [38400/50000 (76.7%)]	Loss: 0.010967	
Test set: Average loss: 0.3165, Accuracy: 9245/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 197 [0/50000 (0.0%)]	Loss: 0.006829	

Train Epoch: 197 [19200/50000 (38.4%)]	Loss: 0.014183	

Train Epoch: 197 [38400/50000 (76.7%)]	Loss: 0.005231	
Test set: Average loss: 0.3165, Accuracy: 9240/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 198 [0/50000 (0.0%)]	Loss: 0.002970	

Train Epoch: 198 [19200/50000 (38.4%)]	Loss: 0.010685	

Train Epoch: 198 [38400/50000 (76.7%)]	Loss: 0.007268	
Test set: Average loss: 0.3183, Accuracy: 9243/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 199 [0/50000 (0.0%)]	Loss: 0.007655	

Train Epoch: 199 [19200/50000 (38.4%)]	Loss: 0.008244	

Train Epoch: 199 [38400/50000 (76.7%)]	Loss: 0.006229	
Test set: Average loss: 0.3174, Accuracy: 9236/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 200 [0/50000 (0.0%)]	Loss: 0.008705	

Train Epoch: 200 [19200/50000 (38.4%)]	Loss: 0.007296	

Train Epoch: 200 [38400/50000 (76.7%)]	Loss: 0.018995	
Test set: Average loss: 0.3131, Accuracy: 9252/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 201 [0/50000 (0.0%)]	Loss: 0.017419	

Train Epoch: 201 [19200/50000 (38.4%)]	Loss: 0.016283	

Train Epoch: 201 [38400/50000 (76.7%)]	Loss: 0.005910	
Test set: Average loss: 0.3155, Accuracy: 9253/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 202 [0/50000 (0.0%)]	Loss: 0.010489	

Train Epoch: 202 [19200/50000 (38.4%)]	Loss: 0.014251	

Train Epoch: 202 [38400/50000 (76.7%)]	Loss: 0.005086	
Test set: Average loss: 0.3189, Accuracy: 9254/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 203 [0/50000 (0.0%)]	Loss: 0.020116	

Train Epoch: 203 [19200/50000 (38.4%)]	Loss: 0.013636	

Train Epoch: 203 [38400/50000 (76.7%)]	Loss: 0.011603	
Test set: Average loss: 0.3225, Accuracy: 9237/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 204 [0/50000 (0.0%)]	Loss: 0.008327	

Train Epoch: 204 [19200/50000 (38.4%)]	Loss: 0.025039	

Train Epoch: 204 [38400/50000 (76.7%)]	Loss: 0.008915	
Test set: Average loss: 0.3169, Accuracy: 9245/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 205 [0/50000 (0.0%)]	Loss: 0.006969	

Train Epoch: 205 [19200/50000 (38.4%)]	Loss: 0.013179	

Train Epoch: 205 [38400/50000 (76.7%)]	Loss: 0.009010	
Test set: Average loss: 0.3209, Accuracy: 9228/10000 (92.3%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 206 [0/50000 (0.0%)]	Loss: 0.008593	

Train Epoch: 206 [19200/50000 (38.4%)]	Loss: 0.002460	

Train Epoch: 206 [38400/50000 (76.7%)]	Loss: 0.016357	
Test set: Average loss: 0.3240, Accuracy: 9252/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 207 [0/50000 (0.0%)]	Loss: 0.008200	

Train Epoch: 207 [19200/50000 (38.4%)]	Loss: 0.013236	

Train Epoch: 207 [38400/50000 (76.7%)]	Loss: 0.013917	
Test set: Average loss: 0.3201, Accuracy: 9269/10000 (92.7%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 208 [0/50000 (0.0%)]	Loss: 0.019055	

Train Epoch: 208 [19200/50000 (38.4%)]	Loss: 0.003845	

Train Epoch: 208 [38400/50000 (76.7%)]	Loss: 0.006899	
Test set: Average loss: 0.3229, Accuracy: 9266/10000 (92.7%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 209 [0/50000 (0.0%)]	Loss: 0.007924	

Train Epoch: 209 [19200/50000 (38.4%)]	Loss: 0.028001	

Train Epoch: 209 [38400/50000 (76.7%)]	Loss: 0.006902	
Test set: Average loss: 0.3293, Accuracy: 9241/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 210 [0/50000 (0.0%)]	Loss: 0.008166	

Train Epoch: 210 [19200/50000 (38.4%)]	Loss: 0.003615	

Train Epoch: 210 [38400/50000 (76.7%)]	Loss: 0.002643	
Test set: Average loss: 0.3257, Accuracy: 9261/10000 (92.6%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 211 [0/50000 (0.0%)]	Loss: 0.006647	

Train Epoch: 211 [19200/50000 (38.4%)]	Loss: 0.004032	

Train Epoch: 211 [38400/50000 (76.7%)]	Loss: 0.010736	
Test set: Average loss: 0.3297, Accuracy: 9251/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 212 [0/50000 (0.0%)]	Loss: 0.004570	

Train Epoch: 212 [19200/50000 (38.4%)]	Loss: 0.009748	

Train Epoch: 212 [38400/50000 (76.7%)]	Loss: 0.014287	
Test set: Average loss: 0.3260, Accuracy: 9245/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 213 [0/50000 (0.0%)]	Loss: 0.004045	

Train Epoch: 213 [19200/50000 (38.4%)]	Loss: 0.010448	

Train Epoch: 213 [38400/50000 (76.7%)]	Loss: 0.004763	
Test set: Average loss: 0.3217, Accuracy: 9248/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 214 [0/50000 (0.0%)]	Loss: 0.002928	

Train Epoch: 214 [19200/50000 (38.4%)]	Loss: 0.010786	

Train Epoch: 214 [38400/50000 (76.7%)]	Loss: 0.048934	
Test set: Average loss: 0.3157, Accuracy: 9253/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 215 [0/50000 (0.0%)]	Loss: 0.003476	

Train Epoch: 215 [19200/50000 (38.4%)]	Loss: 0.004077	

Train Epoch: 215 [38400/50000 (76.7%)]	Loss: 0.014545	
Test set: Average loss: 0.3199, Accuracy: 9272/10000 (92.7%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 216 [0/50000 (0.0%)]	Loss: 0.010357	

Train Epoch: 216 [19200/50000 (38.4%)]	Loss: 0.017237	

Train Epoch: 216 [38400/50000 (76.7%)]	Loss: 0.009772	
Test set: Average loss: 0.3261, Accuracy: 9259/10000 (92.6%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 217 [0/50000 (0.0%)]	Loss: 0.007930	

Train Epoch: 217 [19200/50000 (38.4%)]	Loss: 0.003289	

Train Epoch: 217 [38400/50000 (76.7%)]	Loss: 0.010338	
Test set: Average loss: 0.3286, Accuracy: 9264/10000 (92.6%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 218 [0/50000 (0.0%)]	Loss: 0.006990	

Train Epoch: 218 [19200/50000 (38.4%)]	Loss: 0.005629	

Train Epoch: 218 [38400/50000 (76.7%)]	Loss: 0.006962	
Test set: Average loss: 0.3243, Accuracy: 9240/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 219 [0/50000 (0.0%)]	Loss: 0.007836	

Train Epoch: 219 [19200/50000 (38.4%)]	Loss: 0.004586	

Train Epoch: 219 [38400/50000 (76.7%)]	Loss: 0.005017	
Test set: Average loss: 0.3258, Accuracy: 9245/10000 (92.4%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 220 [0/50000 (0.0%)]	Loss: 0.009263	

Train Epoch: 220 [19200/50000 (38.4%)]	Loss: 0.005897	

Train Epoch: 220 [38400/50000 (76.7%)]	Loss: 0.007283	
Test set: Average loss: 0.3260, Accuracy: 9269/10000 (92.7%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 221 [0/50000 (0.0%)]	Loss: 0.004834	

Train Epoch: 221 [19200/50000 (38.4%)]	Loss: 0.026831	

Train Epoch: 221 [38400/50000 (76.7%)]	Loss: 0.003637	
Test set: Average loss: 0.3249, Accuracy: 9254/10000 (92.5%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 222 [0/50000 (0.0%)]	Loss: 0.005931	

Train Epoch: 222 [19200/50000 (38.4%)]	Loss: 0.021544	

Train Epoch: 222 [38400/50000 (76.7%)]	Loss: 0.009823	
Test set: Average loss: 0.3238, Accuracy: 9281/10000 (92.8%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 223 [0/50000 (0.0%)]	Loss: 0.002652	

Train Epoch: 223 [19200/50000 (38.4%)]	Loss: 0.002046	

Train Epoch: 223 [38400/50000 (76.7%)]	Loss: 0.013805	
Test set: Average loss: 0.3234, Accuracy: 9267/10000 (92.7%), learning rate: 0.001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 224 [0/50000 (0.0%)]	Loss: 0.021495	

Train Epoch: 224 [19200/50000 (38.4%)]	Loss: 0.005398	

Train Epoch: 224 [38400/50000 (76.7%)]	Loss: 0.004976	
Test set: Average loss: 0.3238, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 225 [0/50000 (0.0%)]	Loss: 0.017460	

Train Epoch: 225 [19200/50000 (38.4%)]	Loss: 0.011453	

Train Epoch: 225 [38400/50000 (76.7%)]	Loss: 0.022864	
Test set: Average loss: 0.3248, Accuracy: 9260/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 226 [0/50000 (0.0%)]	Loss: 0.011089	

Train Epoch: 226 [19200/50000 (38.4%)]	Loss: 0.004042	

Train Epoch: 226 [38400/50000 (76.7%)]	Loss: 0.013049	
Test set: Average loss: 0.3230, Accuracy: 9253/10000 (92.5%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 227 [0/50000 (0.0%)]	Loss: 0.002588	

Train Epoch: 227 [19200/50000 (38.4%)]	Loss: 0.004543	

Train Epoch: 227 [38400/50000 (76.7%)]	Loss: 0.017546	
Test set: Average loss: 0.3221, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 228 [0/50000 (0.0%)]	Loss: 0.007659	

Train Epoch: 228 [19200/50000 (38.4%)]	Loss: 0.003118	

Train Epoch: 228 [38400/50000 (76.7%)]	Loss: 0.010901	
Test set: Average loss: 0.3216, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 229 [0/50000 (0.0%)]	Loss: 0.020243	

Train Epoch: 229 [19200/50000 (38.4%)]	Loss: 0.002624	

Train Epoch: 229 [38400/50000 (76.7%)]	Loss: 0.007609	
Test set: Average loss: 0.3211, Accuracy: 9268/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 230 [0/50000 (0.0%)]	Loss: 0.011185	

Train Epoch: 230 [19200/50000 (38.4%)]	Loss: 0.007364	

Train Epoch: 230 [38400/50000 (76.7%)]	Loss: 0.007405	
Test set: Average loss: 0.3216, Accuracy: 9259/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 231 [0/50000 (0.0%)]	Loss: 0.002692	

Train Epoch: 231 [19200/50000 (38.4%)]	Loss: 0.028278	

Train Epoch: 231 [38400/50000 (76.7%)]	Loss: 0.016478	
Test set: Average loss: 0.3197, Accuracy: 9271/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 232 [0/50000 (0.0%)]	Loss: 0.008769	

Train Epoch: 232 [19200/50000 (38.4%)]	Loss: 0.006554	

Train Epoch: 232 [38400/50000 (76.7%)]	Loss: 0.011212	
Test set: Average loss: 0.3191, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 233 [0/50000 (0.0%)]	Loss: 0.007028	

Train Epoch: 233 [19200/50000 (38.4%)]	Loss: 0.010031	

Train Epoch: 233 [38400/50000 (76.7%)]	Loss: 0.023240	
Test set: Average loss: 0.3217, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 234 [0/50000 (0.0%)]	Loss: 0.005367	

Train Epoch: 234 [19200/50000 (38.4%)]	Loss: 0.016253	

Train Epoch: 234 [38400/50000 (76.7%)]	Loss: 0.014823	
Test set: Average loss: 0.3167, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 235 [0/50000 (0.0%)]	Loss: 0.001741	

Train Epoch: 235 [19200/50000 (38.4%)]	Loss: 0.009091	

Train Epoch: 235 [38400/50000 (76.7%)]	Loss: 0.004922	
Test set: Average loss: 0.3181, Accuracy: 9280/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 236 [0/50000 (0.0%)]	Loss: 0.033894	

Train Epoch: 236 [19200/50000 (38.4%)]	Loss: 0.015111	

Train Epoch: 236 [38400/50000 (76.7%)]	Loss: 0.008322	
Test set: Average loss: 0.3225, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 237 [0/50000 (0.0%)]	Loss: 0.006110	

Train Epoch: 237 [19200/50000 (38.4%)]	Loss: 0.010331	

Train Epoch: 237 [38400/50000 (76.7%)]	Loss: 0.005985	
Test set: Average loss: 0.3213, Accuracy: 9266/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 238 [0/50000 (0.0%)]	Loss: 0.003291	

Train Epoch: 238 [19200/50000 (38.4%)]	Loss: 0.006024	

Train Epoch: 238 [38400/50000 (76.7%)]	Loss: 0.018703	
Test set: Average loss: 0.3202, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 239 [0/50000 (0.0%)]	Loss: 0.003186	

Train Epoch: 239 [19200/50000 (38.4%)]	Loss: 0.004486	

Train Epoch: 239 [38400/50000 (76.7%)]	Loss: 0.007493	
Test set: Average loss: 0.3192, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 240 [0/50000 (0.0%)]	Loss: 0.006613	

Train Epoch: 240 [19200/50000 (38.4%)]	Loss: 0.002595	

Train Epoch: 240 [38400/50000 (76.7%)]	Loss: 0.004922	
Test set: Average loss: 0.3194, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 241 [0/50000 (0.0%)]	Loss: 0.030666	

Train Epoch: 241 [19200/50000 (38.4%)]	Loss: 0.003232	

Train Epoch: 241 [38400/50000 (76.7%)]	Loss: 0.004059	
Test set: Average loss: 0.3192, Accuracy: 9267/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 242 [0/50000 (0.0%)]	Loss: 0.003668	

Train Epoch: 242 [19200/50000 (38.4%)]	Loss: 0.003399	

Train Epoch: 242 [38400/50000 (76.7%)]	Loss: 0.004429	
Test set: Average loss: 0.3163, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 243 [0/50000 (0.0%)]	Loss: 0.008132	

Train Epoch: 243 [19200/50000 (38.4%)]	Loss: 0.016095	

Train Epoch: 243 [38400/50000 (76.7%)]	Loss: 0.002010	
Test set: Average loss: 0.3204, Accuracy: 9283/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 244 [0/50000 (0.0%)]	Loss: 0.003428	

Train Epoch: 244 [19200/50000 (38.4%)]	Loss: 0.002191	

Train Epoch: 244 [38400/50000 (76.7%)]	Loss: 0.015680	
Test set: Average loss: 0.3187, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 245 [0/50000 (0.0%)]	Loss: 0.003029	

Train Epoch: 245 [19200/50000 (38.4%)]	Loss: 0.005664	

Train Epoch: 245 [38400/50000 (76.7%)]	Loss: 0.005922	
Test set: Average loss: 0.3202, Accuracy: 9263/10000 (92.6%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 246 [0/50000 (0.0%)]	Loss: 0.004954	

Train Epoch: 246 [19200/50000 (38.4%)]	Loss: 0.014580	

Train Epoch: 246 [38400/50000 (76.7%)]	Loss: 0.003463	
Test set: Average loss: 0.3205, Accuracy: 9269/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 247 [0/50000 (0.0%)]	Loss: 0.013772	

Train Epoch: 247 [19200/50000 (38.4%)]	Loss: 0.004369	

Train Epoch: 247 [38400/50000 (76.7%)]	Loss: 0.013199	
Test set: Average loss: 0.3210, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 248 [0/50000 (0.0%)]	Loss: 0.004994	

Train Epoch: 248 [19200/50000 (38.4%)]	Loss: 0.011650	

Train Epoch: 248 [38400/50000 (76.7%)]	Loss: 0.003411	
Test set: Average loss: 0.3197, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 249 [0/50000 (0.0%)]	Loss: 0.007014	

Train Epoch: 249 [19200/50000 (38.4%)]	Loss: 0.007604	

Train Epoch: 249 [38400/50000 (76.7%)]	Loss: 0.020105	
Test set: Average loss: 0.3206, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 250 [0/50000 (0.0%)]	Loss: 0.012945	

Train Epoch: 250 [19200/50000 (38.4%)]	Loss: 0.011344	

Train Epoch: 250 [38400/50000 (76.7%)]	Loss: 0.009790	
Test set: Average loss: 0.3200, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 251 [0/50000 (0.0%)]	Loss: 0.005878	

Train Epoch: 251 [19200/50000 (38.4%)]	Loss: 0.007523	

Train Epoch: 251 [38400/50000 (76.7%)]	Loss: 0.015309	
Test set: Average loss: 0.3200, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 252 [0/50000 (0.0%)]	Loss: 0.004066	

Train Epoch: 252 [19200/50000 (38.4%)]	Loss: 0.026420	

Train Epoch: 252 [38400/50000 (76.7%)]	Loss: 0.003697	
Test set: Average loss: 0.3198, Accuracy: 9296/10000 (93.0%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
is best and save!

Train Epoch: 253 [0/50000 (0.0%)]	Loss: 0.005243	

Train Epoch: 253 [19200/50000 (38.4%)]	Loss: 0.016932	

Train Epoch: 253 [38400/50000 (76.7%)]	Loss: 0.006896	
Test set: Average loss: 0.3221, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 254 [0/50000 (0.0%)]	Loss: 0.011851	

Train Epoch: 254 [19200/50000 (38.4%)]	Loss: 0.026797	

Train Epoch: 254 [38400/50000 (76.7%)]	Loss: 0.006954	
Test set: Average loss: 0.3188, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 255 [0/50000 (0.0%)]	Loss: 0.007236	

Train Epoch: 255 [19200/50000 (38.4%)]	Loss: 0.027910	

Train Epoch: 255 [38400/50000 (76.7%)]	Loss: 0.008791	
Test set: Average loss: 0.3204, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 256 [0/50000 (0.0%)]	Loss: 0.008482	

Train Epoch: 256 [19200/50000 (38.4%)]	Loss: 0.018540	

Train Epoch: 256 [38400/50000 (76.7%)]	Loss: 0.014638	
Test set: Average loss: 0.3212, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 257 [0/50000 (0.0%)]	Loss: 0.004302	

Train Epoch: 257 [19200/50000 (38.4%)]	Loss: 0.023369	

Train Epoch: 257 [38400/50000 (76.7%)]	Loss: 0.032989	
Test set: Average loss: 0.3229, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 258 [0/50000 (0.0%)]	Loss: 0.002089	

Train Epoch: 258 [19200/50000 (38.4%)]	Loss: 0.003090	

Train Epoch: 258 [38400/50000 (76.7%)]	Loss: 0.018968	
Test set: Average loss: 0.3217, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 259 [0/50000 (0.0%)]	Loss: 0.018412	

Train Epoch: 259 [19200/50000 (38.4%)]	Loss: 0.011926	

Train Epoch: 259 [38400/50000 (76.7%)]	Loss: 0.008315	
Test set: Average loss: 0.3194, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 260 [0/50000 (0.0%)]	Loss: 0.009351	

Train Epoch: 260 [19200/50000 (38.4%)]	Loss: 0.002654	

Train Epoch: 260 [38400/50000 (76.7%)]	Loss: 0.007325	
Test set: Average loss: 0.3217, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 261 [0/50000 (0.0%)]	Loss: 0.006928	

Train Epoch: 261 [19200/50000 (38.4%)]	Loss: 0.003144	

Train Epoch: 261 [38400/50000 (76.7%)]	Loss: 0.003363	
Test set: Average loss: 0.3216, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 262 [0/50000 (0.0%)]	Loss: 0.009386	

Train Epoch: 262 [19200/50000 (38.4%)]	Loss: 0.004092	

Train Epoch: 262 [38400/50000 (76.7%)]	Loss: 0.004031	
Test set: Average loss: 0.3211, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 263 [0/50000 (0.0%)]	Loss: 0.010054	

Train Epoch: 263 [19200/50000 (38.4%)]	Loss: 0.007293	

Train Epoch: 263 [38400/50000 (76.7%)]	Loss: 0.012789	
Test set: Average loss: 0.3213, Accuracy: 9278/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 264 [0/50000 (0.0%)]	Loss: 0.004951	

Train Epoch: 264 [19200/50000 (38.4%)]	Loss: 0.001965	

Train Epoch: 264 [38400/50000 (76.7%)]	Loss: 0.011752	
Test set: Average loss: 0.3204, Accuracy: 9285/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 265 [0/50000 (0.0%)]	Loss: 0.005480	

Train Epoch: 265 [19200/50000 (38.4%)]	Loss: 0.003357	

Train Epoch: 265 [38400/50000 (76.7%)]	Loss: 0.005679	
Test set: Average loss: 0.3232, Accuracy: 9267/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 266 [0/50000 (0.0%)]	Loss: 0.002352	

Train Epoch: 266 [19200/50000 (38.4%)]	Loss: 0.011410	

Train Epoch: 266 [38400/50000 (76.7%)]	Loss: 0.021478	
Test set: Average loss: 0.3216, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 267 [0/50000 (0.0%)]	Loss: 0.008526	

Train Epoch: 267 [19200/50000 (38.4%)]	Loss: 0.003806	

Train Epoch: 267 [38400/50000 (76.7%)]	Loss: 0.019431	
Test set: Average loss: 0.3246, Accuracy: 9270/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 268 [0/50000 (0.0%)]	Loss: 0.007481	

Train Epoch: 268 [19200/50000 (38.4%)]	Loss: 0.006828	

Train Epoch: 268 [38400/50000 (76.7%)]	Loss: 0.036721	
Test set: Average loss: 0.3215, Accuracy: 9283/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 269 [0/50000 (0.0%)]	Loss: 0.004203	

Train Epoch: 269 [19200/50000 (38.4%)]	Loss: 0.007680	

Train Epoch: 269 [38400/50000 (76.7%)]	Loss: 0.006103	
Test set: Average loss: 0.3228, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 270 [0/50000 (0.0%)]	Loss: 0.010464	

Train Epoch: 270 [19200/50000 (38.4%)]	Loss: 0.008182	

Train Epoch: 270 [38400/50000 (76.7%)]	Loss: 0.003447	
Test set: Average loss: 0.3217, Accuracy: 9288/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 271 [0/50000 (0.0%)]	Loss: 0.002396	

Train Epoch: 271 [19200/50000 (38.4%)]	Loss: 0.005033	

Train Epoch: 271 [38400/50000 (76.7%)]	Loss: 0.005580	
Test set: Average loss: 0.3197, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 272 [0/50000 (0.0%)]	Loss: 0.004012	

Train Epoch: 272 [19200/50000 (38.4%)]	Loss: 0.037037	

Train Epoch: 272 [38400/50000 (76.7%)]	Loss: 0.003295	
Test set: Average loss: 0.3246, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 273 [0/50000 (0.0%)]	Loss: 0.006925	

Train Epoch: 273 [19200/50000 (38.4%)]	Loss: 0.017194	

Train Epoch: 273 [38400/50000 (76.7%)]	Loss: 0.002400	
Test set: Average loss: 0.3226, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 274 [0/50000 (0.0%)]	Loss: 0.003913	

Train Epoch: 274 [19200/50000 (38.4%)]	Loss: 0.008841	

Train Epoch: 274 [38400/50000 (76.7%)]	Loss: 0.003801	
Test set: Average loss: 0.3206, Accuracy: 9291/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 275 [0/50000 (0.0%)]	Loss: 0.005395	

Train Epoch: 275 [19200/50000 (38.4%)]	Loss: 0.029921	

Train Epoch: 275 [38400/50000 (76.7%)]	Loss: 0.003321	
Test set: Average loss: 0.3232, Accuracy: 9293/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 276 [0/50000 (0.0%)]	Loss: 0.004428	

Train Epoch: 276 [19200/50000 (38.4%)]	Loss: 0.008493	

Train Epoch: 276 [38400/50000 (76.7%)]	Loss: 0.011147	
Test set: Average loss: 0.3243, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 277 [0/50000 (0.0%)]	Loss: 0.003687	

Train Epoch: 277 [19200/50000 (38.4%)]	Loss: 0.003690	

Train Epoch: 277 [38400/50000 (76.7%)]	Loss: 0.006459	
Test set: Average loss: 0.3202, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 278 [0/50000 (0.0%)]	Loss: 0.003749	

Train Epoch: 278 [19200/50000 (38.4%)]	Loss: 0.001767	

Train Epoch: 278 [38400/50000 (76.7%)]	Loss: 0.026210	
Test set: Average loss: 0.3217, Accuracy: 9272/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 279 [0/50000 (0.0%)]	Loss: 0.004463	

Train Epoch: 279 [19200/50000 (38.4%)]	Loss: 0.005114	

Train Epoch: 279 [38400/50000 (76.7%)]	Loss: 0.003593	
Test set: Average loss: 0.3228, Accuracy: 9285/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 280 [0/50000 (0.0%)]	Loss: 0.008458	

Train Epoch: 280 [19200/50000 (38.4%)]	Loss: 0.003048	

Train Epoch: 280 [38400/50000 (76.7%)]	Loss: 0.008356	
Test set: Average loss: 0.3217, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 281 [0/50000 (0.0%)]	Loss: 0.002978	

Train Epoch: 281 [19200/50000 (38.4%)]	Loss: 0.023578	

Train Epoch: 281 [38400/50000 (76.7%)]	Loss: 0.003107	
Test set: Average loss: 0.3227, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 282 [0/50000 (0.0%)]	Loss: 0.003374	

Train Epoch: 282 [19200/50000 (38.4%)]	Loss: 0.003800	

Train Epoch: 282 [38400/50000 (76.7%)]	Loss: 0.008390	
Test set: Average loss: 0.3212, Accuracy: 9277/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 283 [0/50000 (0.0%)]	Loss: 0.008530	

Train Epoch: 283 [19200/50000 (38.4%)]	Loss: 0.011255	

Train Epoch: 283 [38400/50000 (76.7%)]	Loss: 0.004517	
Test set: Average loss: 0.3219, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 284 [0/50000 (0.0%)]	Loss: 0.012002	

Train Epoch: 284 [19200/50000 (38.4%)]	Loss: 0.005190	

Train Epoch: 284 [38400/50000 (76.7%)]	Loss: 0.009222	
Test set: Average loss: 0.3205, Accuracy: 9286/10000 (92.9%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 285 [0/50000 (0.0%)]	Loss: 0.007550	

Train Epoch: 285 [19200/50000 (38.4%)]	Loss: 0.007752	

Train Epoch: 285 [38400/50000 (76.7%)]	Loss: 0.006388	
Test set: Average loss: 0.3200, Accuracy: 9275/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 286 [0/50000 (0.0%)]	Loss: 0.012209	

Train Epoch: 286 [19200/50000 (38.4%)]	Loss: 0.023731	

Train Epoch: 286 [38400/50000 (76.7%)]	Loss: 0.003022	
Test set: Average loss: 0.3232, Accuracy: 9280/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 287 [0/50000 (0.0%)]	Loss: 0.002127	

Train Epoch: 287 [19200/50000 (38.4%)]	Loss: 0.022662	

Train Epoch: 287 [38400/50000 (76.7%)]	Loss: 0.002093	
Test set: Average loss: 0.3199, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 288 [0/50000 (0.0%)]	Loss: 0.004068	

Train Epoch: 288 [19200/50000 (38.4%)]	Loss: 0.004127	

Train Epoch: 288 [38400/50000 (76.7%)]	Loss: 0.002196	
Test set: Average loss: 0.3229, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 289 [0/50000 (0.0%)]	Loss: 0.003392	

Train Epoch: 289 [19200/50000 (38.4%)]	Loss: 0.010698	

Train Epoch: 289 [38400/50000 (76.7%)]	Loss: 0.004996	
Test set: Average loss: 0.3227, Accuracy: 9284/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 290 [0/50000 (0.0%)]	Loss: 0.005430	

Train Epoch: 290 [19200/50000 (38.4%)]	Loss: 0.007691	

Train Epoch: 290 [38400/50000 (76.7%)]	Loss: 0.002935	
Test set: Average loss: 0.3237, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 291 [0/50000 (0.0%)]	Loss: 0.002578	

Train Epoch: 291 [19200/50000 (38.4%)]	Loss: 0.002995	

Train Epoch: 291 [38400/50000 (76.7%)]	Loss: 0.010040	
Test set: Average loss: 0.3251, Accuracy: 9279/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 292 [0/50000 (0.0%)]	Loss: 0.005786	

Train Epoch: 292 [19200/50000 (38.4%)]	Loss: 0.007239	

Train Epoch: 292 [38400/50000 (76.7%)]	Loss: 0.007349	
Test set: Average loss: 0.3227, Accuracy: 9274/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 293 [0/50000 (0.0%)]	Loss: 0.003763	

Train Epoch: 293 [19200/50000 (38.4%)]	Loss: 0.005161	

Train Epoch: 293 [38400/50000 (76.7%)]	Loss: 0.008940	
Test set: Average loss: 0.3232, Accuracy: 9280/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 294 [0/50000 (0.0%)]	Loss: 0.002862	

Train Epoch: 294 [19200/50000 (38.4%)]	Loss: 0.003488	

Train Epoch: 294 [38400/50000 (76.7%)]	Loss: 0.007447	
Test set: Average loss: 0.3208, Accuracy: 9280/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 295 [0/50000 (0.0%)]	Loss: 0.002848	

Train Epoch: 295 [19200/50000 (38.4%)]	Loss: 0.009343	

Train Epoch: 295 [38400/50000 (76.7%)]	Loss: 0.007322	
Test set: Average loss: 0.3251, Accuracy: 9273/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 296 [0/50000 (0.0%)]	Loss: 0.018075	

Train Epoch: 296 [19200/50000 (38.4%)]	Loss: 0.002097	

Train Epoch: 296 [38400/50000 (76.7%)]	Loss: 0.003227	
Test set: Average loss: 0.3245, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 297 [0/50000 (0.0%)]	Loss: 0.032356	

Train Epoch: 297 [19200/50000 (38.4%)]	Loss: 0.006503	

Train Epoch: 297 [38400/50000 (76.7%)]	Loss: 0.014237	
Test set: Average loss: 0.3280, Accuracy: 9281/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 298 [0/50000 (0.0%)]	Loss: 0.003371	

Train Epoch: 298 [19200/50000 (38.4%)]	Loss: 0.005636	

Train Epoch: 298 [38400/50000 (76.7%)]	Loss: 0.002488	
Test set: Average loss: 0.3251, Accuracy: 9273/10000 (92.7%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned

Train Epoch: 299 [0/50000 (0.0%)]	Loss: 0.006289	

Train Epoch: 299 [19200/50000 (38.4%)]	Loss: 0.013373	

Train Epoch: 299 [38400/50000 (76.7%)]	Loss: 0.013252	
Test set: Average loss: 0.3247, Accuracy: 9282/10000 (92.8%), learning rate: 0.0001
=> Save path: /home2/pengyifan/pyf/freq-lite/logs/resnet56/0.24/1/finetuned
